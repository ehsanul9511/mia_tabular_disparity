{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import data_utils\n",
    "import model_utils\n",
    "from attack_utils import get_CSMIA_case_by_case_results, CSMIA_attack, LOMIA_attack, get_LOMIA_results\n",
    "from data_utils import oneHotCatVars, filter_random_data_by_conf_score\n",
    "from vulnerability_score_utils import get_vulnerability_score, draw_hist_plot\n",
    "from experiment_utils import MIAExperiment\n",
    "from disparity_inference_utils import get_confidence_array, draw_confidence_array_scatter, get_indices_by_group_condition, get_corr_btn_sens_and_out_per_subgroup, get_slopes, get_angular_difference, calculate_stds, get_mutual_info_btn_sens_and_out_per_subgroup\n",
    "from targeted_inference import get_angular_difference_range_for_subgroup,single_attribute_based_targeted_imputation, nested_attribute_based_targeted_imputation, single_attribute_based_targeted_ai, nested_attribute_based_targeted_ai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network._base import ACTIVATIONS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "import pickle\n",
    "# import utils\n",
    "import copy\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Setting the font family, size, and weight globally\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['font.weight'] = 'light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/51 [00:01<01:05,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 248 125 1.984\n",
      "after scaling: 125 125 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 7/51 [00:09<00:57,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 233 219 1.0639269406392695\n",
      "after scaling: 219 219 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 12/51 [00:15<00:51,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 221 164 1.3475609756097562\n",
      "after scaling: 164 164 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 19/51 [00:24<00:42,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 204 163 1.2515337423312884\n",
      "after scaling: 163 163 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 26/51 [00:34<00:32,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 187 126 1.4841269841269842\n",
      "after scaling: 126 126 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 34/51 [00:44<00:22,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 167 120 1.3916666666666666\n",
      "after scaling: 120 120 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 41/51 [00:53<00:12,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 150 76 1.9736842105263157\n",
      "after scaling: 76 76 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 45/51 [00:58<00:07,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 361 325 1.1107692307692307\n",
      "after scaling: 325 325 1.0\n",
      "before scaling: 140 101 1.386138613861386\n",
      "after scaling: 101 101 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [01:05<00:01,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 373 326 1.1441717791411044\n",
      "after scaling: 326 326 1.0\n",
      "before scaling: 128 87 1.471264367816092\n",
      "after scaling: 87 87 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [01:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 500, 500, 252, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 469, 500, 500, 500, 500, 500, 500, 500, 500, 500, 371, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 399, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 336, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 359, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 253, 500, 500, 500, 500, 500, 500, 450, 360, 500, 500, 500, 500, 500, 500, 500, 500, 436, 339]\n"
     ]
    }
   ],
   "source": [
    "experiments = {}\n",
    "for i in range(1):\n",
    "    experiment = MIAExperiment(sampling_condition_dict = \n",
    "        {\n",
    "                'subgroup_col_name': 'ST',\n",
    "                'n': 1000,\n",
    "        }, random_state = i,\n",
    "        shortname = f\"Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5_random_state_{i}\"\n",
    "    )\n",
    "    experiments[experiment.shortname] = experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for experiment: Census19_subgroup_col_name_ST_n_1000_rs0\n"
     ]
    }
   ],
   "source": [
    "save_model = False\n",
    "\n",
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    \n",
    "    print(f\"Training classifier for experiment: {experiment}\")\n",
    "    try:\n",
    "        experiment.clf = model_utils.load_model(f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model_.pkl')\n",
    "        print(f\"Loaded classifier for experiment from file: {experiment}\")\n",
    "    except:\n",
    "        # clf = model_utils.get_model(max_iter=500, hidden_layer_sizes=(256, 256))\n",
    "        experiment.clf = model_utils.get_model(max_iter=500)\n",
    "        experiment.clf.fit(experiment.X_train, experiment.y_tr_onehot)\n",
    "\n",
    "        if save_model:\n",
    "            model_utils.save_model(experiment.clf, f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    experiment.confidence_array = get_confidence_array(experiment, experiment.X_train, experiment.y_te, experiment.clf)\n",
    "    sens_pred, case_indices = CSMIA_attack(experiment.clf, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)\n",
    "    case_2_indices = case_indices[2]\n",
    "    experiment.case_2_indices = case_2_indices\n",
    "    experiment.confidence_array_case_2 = experiment.confidence_array[case_2_indices, :]\n",
    "    experiment.X_case_2 = experiment.X_train.loc[case_2_indices].copy().reset_index(drop=True)\n",
    "    experiment.y_case_2 = experiment.y_tr.ravel()[case_2_indices]\n",
    "    experiment.sens_pred = sens_pred\n",
    "    experiment.sens_pred_LOMIA = LOMIA_attack(experiment, experiment.clf, experiment.X_train, experiment.y_tr, experiment.ds.ds.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_attrib_cols = ['ST', 'SCHL', 'RAC1P', 'SEX']\n",
    "single_kappas = [1, 0.75, 0.5, 0.375, 0.25, 0.1, 0.05]\n",
    "nested_kappas = [0.5, 0.375, 0.25, 0.1, 0.05, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5_random_state_0\n",
      "\n",
      "{'ST': [49, 50, 47]}\n",
      "          attack_accuracy\n",
      "0.056033            71.57\n",
      "0.096418            69.59\n",
      "0.250121            67.27\n",
      "0.367438            65.87\n",
      "0.505472            64.55\n",
      "0.745174            62.14\n",
      "1.000000            60.24\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(single_attribute_based_targeted_ai(experiment, experiment.sens_pred, subgroup_col_name='ST', kappas=single_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5_random_state_0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ehsanulkabir/Downloads/tabular/disparity_inference_utils.py:181: RankWarning: Polyfit may be poorly conditioned\n",
      "  return [np.polyfit(confidence_array[indices_by_y_values[y_value], 1], confidence_array[indices_by_y_values[y_value], 0], 1)[0] for y_value in y_values]\n",
      "/Users/ehsanulkabir/Downloads/tabular/disparity_inference_utils.py:181: RankWarning: Polyfit may be poorly conditioned\n",
      "  return [np.polyfit(confidence_array[indices_by_y_values[y_value], 1], confidence_array[indices_by_y_values[y_value], 0], 1)[0] for y_value in y_values]\n",
      "/Users/ehsanulkabir/Downloads/tabular/disparity_inference_utils.py:181: RankWarning: Polyfit may be poorly conditioned\n",
      "  return [np.polyfit(confidence_array[indices_by_y_values[y_value], 1], confidence_array[indices_by_y_values[y_value], 0], 1)[0] for y_value in y_values]\n",
      "/Users/ehsanulkabir/Downloads/tabular/disparity_inference_utils.py:181: RankWarning: Polyfit may be poorly conditioned\n",
      "  return [np.polyfit(confidence_array[indices_by_y_values[y_value], 1], confidence_array[indices_by_y_values[y_value], 0], 1)[0] for y_value in y_values]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          i  attack_accuracy\n",
      "1.000000  0            60.24\n",
      "0.485280  1            64.89\n",
      "0.365257  2            64.39\n",
      "0.360391  3            64.28\n",
      "0.219409  4            68.31\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(nested_attribute_based_targeted_ai(experiment, experiment.sens_pred, subgroup_cols=['ST', 'SCHL', 'RAC1P', 'SEX'], kappas=nested_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    aux_indices_same_distrib = experiment.X_train.sample(n=5000, random_state=experiment.random_state).index\n",
    "    experiment.X_aux_same_distrib, experiment.y_aux_same_distrib = experiment.X_train.loc[aux_indices_same_distrib].copy().reset_index(drop=True), experiment.y_tr[aux_indices_same_distrib]\n",
    "\n",
    "    aux_indices_diff_distrib = experiment.X_test.sample(n=5000, random_state=experiment.random_state).index\n",
    "    experiment.X_aux_diff_distrib, experiment.y_aux_diff_distrib = experiment.X_test.loc[aux_indices_diff_distrib].copy().reset_index(drop=True), experiment.y_te[aux_indices_diff_distrib]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          imputation_attack_accuracy\n",
      "0.056033                       71.42\n",
      "0.096418                       68.80\n",
      "0.246264                       66.24\n",
      "0.382259                       65.83\n",
      "0.498425                       66.23\n",
      "0.740732                       65.16\n",
      "1.000000                       64.50\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(single_attribute_based_targeted_imputation(experiment, experiment.X_train, experiment.y_tr, experiment.X_aux_same_distrib, experiment.y_aux_same_distrib, subgroup_col_name='ST', kappas=single_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          imputation_attack_accuracy\n",
      "0.055407                       58.97\n",
      "0.105989                       60.64\n",
      "0.241984                       60.90\n",
      "0.376888                       60.59\n",
      "0.495194                       60.94\n",
      "0.757693                       60.88\n",
      "1.000000                       61.01\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(single_attribute_based_targeted_imputation(experiment, experiment.X_train, experiment.y_tr, experiment.X_aux_diff_distrib, experiment.y_aux_diff_distrib, subgroup_col_name='ST', kappas=single_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5_random_state_0\n",
      "\n",
      "          i  attack_accuracy\n",
      "1.000000  0            64.50\n",
      "0.498425  1            66.23\n",
      "0.318694  2            66.59\n",
      "0.039456  3            65.71\n",
      "0.383592  4            66.17\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(nested_attribute_based_targeted_imputation(experiment, experiment.X_train, experiment.y_tr, experiment.X_aux_same_distrib, experiment.y_aux_same_distrib, subgroup_cols=['ST', 'SCHL', 'RAC1P', 'SEX'], kappas=nested_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5_random_state_0\n",
      "\n",
      "          i  attack_accuracy\n",
      "1.000000  0            61.01\n",
      "0.495194  1            60.94\n",
      "0.367135  2            61.48\n",
      "0.044443  3            60.47\n",
      "0.381411  4            61.19\n"
     ]
    }
   ],
   "source": [
    "for experiment_key in experiments:\n",
    "    experiment = experiments[experiment_key]\n",
    "    print(f'\\n{experiment_key}\\n')\n",
    "    print(nested_attribute_based_targeted_imputation(experiment, experiment.X_train, experiment.y_tr, experiment.X_aux_diff_distrib, experiment.y_aux_diff_distrib, subgroup_cols=['ST', 'SCHL', 'RAC1P', 'SEX'], kappas=nested_kappas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = experiment.ds.ds.df.copy()\n",
    "\n",
    "sens_col_name = f'{experiment.sensitive_column}_{experiment.sensitive_positive}'\n",
    "married_indices = aux_df[aux_df[sens_col_name]==False][aux_df[\"is_train\"]==1][[sens_col_name]].index\n",
    "single_indices = aux_df[aux_df[sens_col_name]==True][aux_df[\"is_train\"]==1][[sens_col_name]].index\n",
    "\n",
    "pcnt_single = 0.15\n",
    "total_count = 5000\n",
    "married_sample_indices = aux_df.loc[married_indices].sample(n=total_count-int(total_count * pcnt_single), replace=False).index\n",
    "single_sample_indices = aux_df.loc[single_indices].sample(n=int(total_count * pcnt_single), replace=False).index\n",
    "\n",
    "all_sample_indices = married_sample_indices.append(single_sample_indices)\n",
    "aux_df_distrib_drift = aux_df.loc[all_sample_indices].copy().reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
