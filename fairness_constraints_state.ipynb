{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import data_utils\n",
    "import model_utils\n",
    "from attack_utils import get_CSMIA_case_by_case_results, CSMIA_attack, LOMIA_attack\n",
    "from data_utils import oneHotCatVars, filter_random_data_by_conf_score\n",
    "from vulnerability_score_utils import get_vulnerability_score, draw_hist_plot\n",
    "from experiment_utils import MIAExperiment\n",
    "from disparity_inference_utils import get_confidence_array, draw_confidence_array_scatter, get_indices_by_group_condition, get_corr_btn_sens_and_out_per_subgroup, get_slopes, get_angular_difference, calculate_stds, get_mutual_info_btn_sens_and_out_per_subgroup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neural_network._base import ACTIVATIONS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.inspection import permutation_importance\n",
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tabulate\n",
    "import pickle\n",
    "# import utils\n",
    "import copy\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Setting the font family, size, and weight globally\n",
    "mpl.rcParams['font.family'] = 'DejaVu Sans'\n",
    "mpl.rcParams['font.size'] = 8\n",
    "mpl.rcParams['font.weight'] = 'light'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/51 [00:00<00:46,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 505 392 1.288265306122449\n",
      "after scaling: 392 392 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/51 [00:01<00:43,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 496 125 3.968\n",
      "after scaling: 125 125 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/51 [00:03<00:40,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 486 253 1.9209486166007905\n",
      "after scaling: 253 253 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/51 [00:06<00:36,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 466 219 2.127853881278539\n",
      "after scaling: 219 219 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 12/51 [00:10<00:32,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 447 336 1.3303571428571428\n",
      "after scaling: 336 336 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 13/51 [00:11<00:31,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 442 164 2.6951219512195124\n",
      "after scaling: 164 164 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 16/51 [00:13<00:29,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 427 314 1.3598726114649682\n",
      "after scaling: 314 314 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 17/51 [00:14<00:28,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 422 300 1.4066666666666667\n",
      "after scaling: 300 300 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 19/51 [00:16<00:26,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 594 566 1.0494699646643109\n",
      "after scaling: 566 566 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 20/51 [00:16<00:25,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 407 163 2.496932515337423\n",
      "after scaling: 163 163 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 25/51 [00:20<00:21,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 383 274 1.397810218978102\n",
      "after scaling: 274 274 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 26/51 [00:21<00:20,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 628 469 1.3390191897654584\n",
      "after scaling: 469 469 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 27/51 [00:22<00:19,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 373 126 2.9603174603174605\n",
      "after scaling: 126 126 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 28/51 [00:23<00:18,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 368 187 1.967914438502674\n",
      "after scaling: 187 187 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 30/51 [00:25<00:17,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 358 294 1.217687074829932\n",
      "after scaling: 294 294 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 32/51 [00:26<00:15,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 349 316 1.1044303797468353\n",
      "after scaling: 316 316 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 34/51 [00:28<00:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 667 493 1.3529411764705883\n",
      "after scaling: 492 493 0.9979716024340771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 35/51 [00:29<00:13,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 334 120 2.783333333333333\n",
      "after scaling: 120 120 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 40/51 [00:33<00:09,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 309 260 1.1884615384615385\n",
      "after scaling: 260 260 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 41/51 [00:34<00:08,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 701 357 1.9635854341736694\n",
      "after scaling: 357 357 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 42/51 [00:35<00:07,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 300 76 3.9473684210526314\n",
      "after scaling: 76 76 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 45/51 [00:37<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 721 325 2.2184615384615385\n",
      "after scaling: 325 325 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 46/51 [00:38<00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 280 101 2.772277227722772\n",
      "after scaling: 101 101 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 48/51 [00:40<00:02,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 736 697 1.0559540889526542\n",
      "after scaling: 697 697 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 49/51 [00:40<00:01,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 265 171 1.5497076023391814\n",
      "after scaling: 171 171 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 50/51 [00:41<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 746 326 2.2883435582822087\n",
      "after scaling: 326 326 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:42<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling: 255 87 2.9310344827586206\n",
      "after scaling: 87 87 1.0\n",
      "[1000, 1000, 776, 252, 1000, 1000, 1000, 520, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 469, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 751, 1000, 371, 1000, 1000, 1000, 1000, 1000, 735, 1000, 710, 1000, 1000, 1000, 1000, 952, 400, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 715, 1000, 1000, 746, 337, 1000, 508, 1000, 1000, 1000, 821, 1000, 1000, 1000, 905, 1000, 1000, 1000, 1000, 738, 359, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 841, 1000, 1000, 509, 253, 1000, 1000, 1000, 1000, 1000, 1000, 450, 360, 1000, 1000, 1000, 1000, 947, 645, 1000, 1000, 436, 341]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = MIAExperiment(sampling_condition_dict_list = \n",
    "    {\n",
    "            'subgroup_col_name': 'ST',\n",
    "    }, shortname = f\"Corr_btn_sens_and_output_for_ST_ranging_from_0_to_-0.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for experiment: Census19_subgroup_col_name_ST\n",
      "Loaded classifier for experiment from file: Census19_subgroup_col_name_ST\n"
     ]
    }
   ],
   "source": [
    "save_model = True\n",
    "\n",
    "print(f\"Training classifier for experiment: {experiment}\")\n",
    "try:\n",
    "    experiment.clf = model_utils.load_model(f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model.pkl')\n",
    "    print(f\"Loaded classifier for experiment from file: {experiment}\")\n",
    "except:\n",
    "    # clf = model_utils.get_model(max_iter=500, hidden_layer_sizes=(256, 256))\n",
    "    experiment.clf = model_utils.get_model(max_iter=500)\n",
    "    experiment.clf.fit(experiment.X_train, experiment.y_tr_onehot)\n",
    "\n",
    "    if save_model:\n",
    "        model_utils.save_model(experiment.clf, f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier for experiment: Census19_subgroup_col_name_ST\n",
      "Loaded classifier for experiment from file: Census19_subgroup_col_name_ST\n"
     ]
    }
   ],
   "source": [
    "save_model=True\n",
    "print(f\"Training classifier for experiment: {experiment}\")\n",
    "try:\n",
    "    experiment.clf = model_utils.load_model(f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model_only_on_test_dummy.pkl')\n",
    "    print(f\"Loaded classifier for experiment from file: {experiment}\")\n",
    "except:\n",
    "    # clf = model_utils.get_model(max_iter=500, hidden_layer_sizes=(256, 256))\n",
    "    base_model = model_utils.get_model(max_iter=500)\n",
    "    experiment.clf = copy.deepcopy(base_model)\n",
    "    experiment.clf.fit(experiment.X_test, experiment.y_te_onehot)\n",
    "\n",
    "    if save_model:\n",
    "        model_utils.save_model(experiment.clf, f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_target_model_only_on_test_dummy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class MLPClassifierFC(MLPClassifier):\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "            Fit the model to the given data.\n",
    "        \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            # resample data according to sample weights\n",
    "            n_samples = X.shape[0]\n",
    "            sample_weight = np.asarray(sample_weight)/np.sum(sample_weight)\n",
    "            sample_idxs = np.random.choice(n_samples, n_samples, p=sample_weight)\n",
    "            X = X.iloc[sample_idxs]\n",
    "            y = y[sample_idxs]\n",
    "            \n",
    "        return super().fit(X, y)\n",
    "\n",
    "clf2 = MLPClassifierFC(max_iter=500)\n",
    "clf2.coefs_ = experiment.clf.coefs_\n",
    "clf2.intercepts_ = experiment.clf.intercepts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity, EqualizedOdds, ErrorRate\n",
    "\n",
    "subgroup_col_name = 'ST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_oh_cols = [f'ST_{i}' for i in range(51)]\n",
    "subgroup_vals_tr = experiment.X_train[subgroup_oh_cols].to_numpy().argmax(axis=1)\n",
    "subgroup_vals_tr = experiment.X_train[f'ST_0'].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgroup_vals_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.51777835\n",
      "Iteration 2, loss = 0.46912649\n",
      "Iteration 3, loss = 0.46075360\n",
      "Iteration 4, loss = 0.45556304\n",
      "Iteration 5, loss = 0.45216112\n",
      "Iteration 6, loss = 0.44827510\n",
      "Iteration 7, loss = 0.44533778\n",
      "Iteration 8, loss = 0.44203308\n",
      "Iteration 9, loss = 0.43929611\n",
      "Iteration 10, loss = 0.43665200\n",
      "Iteration 11, loss = 0.43431058\n",
      "Iteration 12, loss = 0.43138366\n",
      "Iteration 13, loss = 0.42942338\n",
      "Iteration 14, loss = 0.42701461\n",
      "Iteration 15, loss = 0.42588661\n",
      "Iteration 16, loss = 0.42364137\n",
      "Iteration 17, loss = 0.42146638\n",
      "Iteration 18, loss = 0.42008516\n",
      "Iteration 19, loss = 0.41843438\n",
      "Iteration 20, loss = 0.41707000\n",
      "Iteration 21, loss = 0.41563445\n",
      "Iteration 22, loss = 0.41398079\n",
      "Iteration 23, loss = 0.41290925\n",
      "Iteration 24, loss = 0.41139400\n",
      "Iteration 25, loss = 0.41042029\n",
      "Iteration 26, loss = 0.40962343\n",
      "Iteration 27, loss = 0.40849378\n",
      "Iteration 28, loss = 0.40795414\n",
      "Iteration 29, loss = 0.40606990\n",
      "Iteration 30, loss = 0.40500490\n",
      "Iteration 31, loss = 0.40375501\n",
      "Iteration 32, loss = 0.40334471\n",
      "Iteration 33, loss = 0.40215809\n",
      "Iteration 34, loss = 0.40125305\n",
      "Iteration 35, loss = 0.40091296\n",
      "Iteration 36, loss = 0.39964427\n",
      "Iteration 37, loss = 0.39862742\n",
      "Iteration 38, loss = 0.39812623\n",
      "Iteration 39, loss = 0.39652608\n",
      "Iteration 40, loss = 0.39577407\n",
      "Iteration 41, loss = 0.39487700\n",
      "Iteration 42, loss = 0.39424904\n",
      "Iteration 43, loss = 0.39286978\n",
      "Iteration 44, loss = 0.39164371\n",
      "Iteration 45, loss = 0.39110034\n",
      "Iteration 46, loss = 0.39080239\n",
      "Iteration 47, loss = 0.38954191\n",
      "Iteration 48, loss = 0.38888511\n",
      "Iteration 49, loss = 0.38772497\n",
      "Iteration 50, loss = 0.38751770\n",
      "Iteration 51, loss = 0.38637145\n",
      "Iteration 52, loss = 0.38644729\n",
      "Iteration 53, loss = 0.38613556\n",
      "Iteration 54, loss = 0.38537604\n",
      "Iteration 55, loss = 0.38341642\n",
      "Iteration 56, loss = 0.38364498\n",
      "Iteration 57, loss = 0.38256247\n",
      "Iteration 58, loss = 0.38201053\n",
      "Iteration 59, loss = 0.38125539\n",
      "Iteration 60, loss = 0.38060983\n",
      "Iteration 61, loss = 0.38008294\n",
      "Iteration 62, loss = 0.37981462\n",
      "Iteration 63, loss = 0.37863678\n",
      "Iteration 64, loss = 0.37772579\n",
      "Iteration 65, loss = 0.37742618\n",
      "Iteration 66, loss = 0.37679215\n",
      "Iteration 67, loss = 0.37629261\n",
      "Iteration 68, loss = 0.37643500\n",
      "Iteration 69, loss = 0.37542774\n",
      "Iteration 70, loss = 0.37466551\n",
      "Iteration 71, loss = 0.37463839\n",
      "Iteration 72, loss = 0.37371215\n",
      "Iteration 73, loss = 0.37324050\n",
      "Iteration 74, loss = 0.37320045\n",
      "Iteration 75, loss = 0.37200846\n",
      "Iteration 76, loss = 0.37094477\n",
      "Iteration 77, loss = 0.37171027\n",
      "Iteration 78, loss = 0.37125803\n",
      "Iteration 79, loss = 0.37035232\n",
      "Iteration 80, loss = 0.36996534\n",
      "Iteration 81, loss = 0.37106215\n",
      "Iteration 82, loss = 0.36934286\n",
      "Iteration 83, loss = 0.36822400\n",
      "Iteration 84, loss = 0.36962971\n",
      "Iteration 85, loss = 0.36765188\n",
      "Iteration 86, loss = 0.36758892\n",
      "Iteration 87, loss = 0.36715079\n",
      "Iteration 88, loss = 0.36698724\n",
      "Iteration 89, loss = 0.36614029\n",
      "Iteration 90, loss = 0.36611035\n",
      "Iteration 91, loss = 0.36569217\n",
      "Iteration 92, loss = 0.36463674\n",
      "Iteration 93, loss = 0.36471043\n",
      "Iteration 94, loss = 0.36439317\n",
      "Iteration 95, loss = 0.36374438\n",
      "Iteration 96, loss = 0.36339019\n",
      "Iteration 97, loss = 0.36314764\n",
      "Iteration 98, loss = 0.36360136\n",
      "Iteration 99, loss = 0.36178959\n",
      "Iteration 100, loss = 0.36146364\n",
      "Iteration 101, loss = 0.36111383\n",
      "Iteration 102, loss = 0.36082756\n",
      "Iteration 103, loss = 0.36071944\n",
      "Iteration 104, loss = 0.36026864\n",
      "Iteration 105, loss = 0.35981486\n",
      "Iteration 106, loss = 0.35937545\n",
      "Iteration 107, loss = 0.36013393\n",
      "Iteration 108, loss = 0.35851633\n",
      "Iteration 109, loss = 0.35926923\n",
      "Iteration 110, loss = 0.35805641\n",
      "Iteration 111, loss = 0.35805557\n",
      "Iteration 112, loss = 0.35803926\n",
      "Iteration 113, loss = 0.35736194\n",
      "Iteration 114, loss = 0.35736135\n",
      "Iteration 115, loss = 0.35745430\n",
      "Iteration 116, loss = 0.35699889\n",
      "Iteration 117, loss = 0.35543376\n",
      "Iteration 118, loss = 0.35538187\n",
      "Iteration 119, loss = 0.35534144\n",
      "Iteration 120, loss = 0.35571710\n",
      "Iteration 121, loss = 0.35497882\n",
      "Iteration 122, loss = 0.35568891\n",
      "Iteration 123, loss = 0.35461294\n",
      "Iteration 124, loss = 0.35428244\n",
      "Iteration 125, loss = 0.35347208\n",
      "Iteration 126, loss = 0.35363859\n",
      "Iteration 127, loss = 0.35331105\n",
      "Iteration 128, loss = 0.35269311\n",
      "Iteration 129, loss = 0.35281104\n",
      "Iteration 130, loss = 0.35285834\n",
      "Iteration 131, loss = 0.35218793\n",
      "Iteration 132, loss = 0.35166401\n",
      "Iteration 133, loss = 0.35126790\n",
      "Iteration 134, loss = 0.35126598\n",
      "Iteration 135, loss = 0.35178396\n",
      "Iteration 136, loss = 0.35035351\n",
      "Iteration 137, loss = 0.35088262\n",
      "Iteration 138, loss = 0.34983879\n",
      "Iteration 139, loss = 0.35009535\n",
      "Iteration 140, loss = 0.34991288\n",
      "Iteration 141, loss = 0.34952822\n",
      "Iteration 142, loss = 0.35012785\n",
      "Iteration 143, loss = 0.34938095\n",
      "Iteration 144, loss = 0.34841859\n",
      "Iteration 145, loss = 0.34816224\n",
      "Iteration 146, loss = 0.34791629\n",
      "Iteration 147, loss = 0.34787223\n",
      "Iteration 148, loss = 0.34781504\n",
      "Iteration 149, loss = 0.34773126\n",
      "Iteration 150, loss = 0.34762156\n",
      "Iteration 151, loss = 0.34734470\n",
      "Iteration 152, loss = 0.34691099\n",
      "Iteration 153, loss = 0.34659659\n",
      "Iteration 154, loss = 0.34654633\n",
      "Iteration 155, loss = 0.34560300\n",
      "Iteration 156, loss = 0.34661781\n",
      "Iteration 157, loss = 0.34599858\n",
      "Iteration 158, loss = 0.34578095\n",
      "Iteration 159, loss = 0.34567987\n",
      "Iteration 160, loss = 0.34683197\n",
      "Iteration 161, loss = 0.34424137\n",
      "Iteration 162, loss = 0.34481969\n",
      "Iteration 163, loss = 0.34487382\n",
      "Iteration 164, loss = 0.34389075\n",
      "Iteration 165, loss = 0.34410583\n",
      "Iteration 166, loss = 0.34376880\n",
      "Iteration 167, loss = 0.34427832\n",
      "Iteration 168, loss = 0.34303152\n",
      "Iteration 169, loss = 0.34438200\n",
      "Iteration 170, loss = 0.34350716\n",
      "Iteration 171, loss = 0.34298922\n",
      "Iteration 172, loss = 0.34322847\n",
      "Iteration 173, loss = 0.34187765\n",
      "Iteration 174, loss = 0.34182670\n",
      "Iteration 175, loss = 0.34192209\n",
      "Iteration 176, loss = 0.34264942\n",
      "Iteration 177, loss = 0.34264328\n",
      "Iteration 178, loss = 0.34088228\n",
      "Iteration 179, loss = 0.34131136\n",
      "Iteration 180, loss = 0.34191398\n",
      "Iteration 181, loss = 0.34157132\n",
      "Iteration 182, loss = 0.34183428\n",
      "Iteration 183, loss = 0.34041433\n",
      "Iteration 184, loss = 0.34060009\n",
      "Iteration 185, loss = 0.34087005\n",
      "Iteration 186, loss = 0.33945584\n",
      "Iteration 187, loss = 0.34119185\n",
      "Iteration 188, loss = 0.33957321\n",
      "Iteration 189, loss = 0.33849799\n",
      "Iteration 190, loss = 0.33885633\n",
      "Iteration 191, loss = 0.33938139\n",
      "Iteration 192, loss = 0.33961260\n",
      "Iteration 193, loss = 0.33845285\n",
      "Iteration 194, loss = 0.33899548\n",
      "Iteration 195, loss = 0.33770530\n",
      "Iteration 196, loss = 0.33779471\n",
      "Iteration 197, loss = 0.33851978\n",
      "Iteration 198, loss = 0.33756790\n",
      "Iteration 199, loss = 0.33821730\n",
      "Iteration 200, loss = 0.33789168\n",
      "Iteration 201, loss = 0.33724930\n",
      "Iteration 202, loss = 0.33848785\n",
      "Iteration 203, loss = 0.33712292\n",
      "Iteration 204, loss = 0.33725769\n",
      "Iteration 205, loss = 0.33735274\n",
      "Iteration 206, loss = 0.33657471\n",
      "Iteration 207, loss = 0.33645448\n",
      "Iteration 208, loss = 0.33661678\n",
      "Iteration 209, loss = 0.33621715\n",
      "Iteration 210, loss = 0.33652055\n",
      "Iteration 211, loss = 0.33654260\n",
      "Iteration 212, loss = 0.33525646\n",
      "Iteration 213, loss = 0.33515937\n",
      "Iteration 214, loss = 0.33540176\n",
      "Iteration 215, loss = 0.33486119\n",
      "Iteration 216, loss = 0.33534873\n",
      "Iteration 217, loss = 0.33525021\n",
      "Iteration 218, loss = 0.33663160\n",
      "Iteration 219, loss = 0.33477510\n",
      "Iteration 220, loss = 0.33365980\n",
      "Iteration 221, loss = 0.33461023\n",
      "Iteration 222, loss = 0.33540394\n",
      "Iteration 223, loss = 0.33358058\n",
      "Iteration 224, loss = 0.33401895\n",
      "Iteration 225, loss = 0.33397385\n",
      "Iteration 226, loss = 0.33371787\n",
      "Iteration 227, loss = 0.33417059\n",
      "Iteration 228, loss = 0.33416991\n",
      "Iteration 229, loss = 0.33276839\n",
      "Iteration 230, loss = 0.33241626\n",
      "Iteration 231, loss = 0.33400430\n",
      "Iteration 232, loss = 0.33302698\n",
      "Iteration 233, loss = 0.33234779\n",
      "Iteration 234, loss = 0.33360717\n",
      "Iteration 235, loss = 0.33264169\n",
      "Iteration 236, loss = 0.33242093\n",
      "Iteration 237, loss = 0.33408631\n",
      "Iteration 238, loss = 0.33330510\n",
      "Iteration 239, loss = 0.33151967\n",
      "Iteration 240, loss = 0.33144307\n",
      "Iteration 241, loss = 0.33124495\n",
      "Iteration 242, loss = 0.33113782\n",
      "Iteration 243, loss = 0.33190238\n",
      "Iteration 244, loss = 0.33148632\n",
      "Iteration 245, loss = 0.33127564\n",
      "Iteration 246, loss = 0.33094249\n",
      "Iteration 247, loss = 0.33151544\n",
      "Iteration 248, loss = 0.33079812\n",
      "Iteration 249, loss = 0.33123115\n",
      "Iteration 250, loss = 0.33016833\n",
      "Iteration 251, loss = 0.33091340\n",
      "Iteration 252, loss = 0.33025684\n",
      "Iteration 253, loss = 0.32984804\n",
      "Iteration 254, loss = 0.32939384\n",
      "Iteration 255, loss = 0.33089414\n",
      "Iteration 256, loss = 0.32956994\n",
      "Iteration 257, loss = 0.32997805\n",
      "Iteration 258, loss = 0.32930503\n",
      "Iteration 259, loss = 0.32959456\n",
      "Iteration 260, loss = 0.32965393\n",
      "Iteration 261, loss = 0.32889585\n",
      "Iteration 262, loss = 0.32887679\n",
      "Iteration 263, loss = 0.32896487\n",
      "Iteration 264, loss = 0.32902956\n",
      "Iteration 265, loss = 0.32994285\n",
      "Iteration 266, loss = 0.32823442\n",
      "Iteration 267, loss = 0.32821355\n",
      "Iteration 268, loss = 0.32759478\n",
      "Iteration 269, loss = 0.32917392\n",
      "Iteration 270, loss = 0.32830255\n",
      "Iteration 271, loss = 0.32790940\n",
      "Iteration 272, loss = 0.32838366\n",
      "Iteration 273, loss = 0.32923589\n",
      "Iteration 274, loss = 0.32711236\n",
      "Iteration 275, loss = 0.32774097\n",
      "Iteration 276, loss = 0.32752960\n",
      "Iteration 277, loss = 0.32826768\n",
      "Iteration 278, loss = 0.32780543\n",
      "Iteration 279, loss = 0.32752843\n",
      "Iteration 280, loss = 0.32646635\n",
      "Iteration 281, loss = 0.32644724\n",
      "Iteration 282, loss = 0.32717137\n",
      "Iteration 283, loss = 0.32638266\n",
      "Iteration 284, loss = 0.32663608\n",
      "Iteration 285, loss = 0.32658322\n",
      "Iteration 286, loss = 0.32731951\n",
      "Iteration 287, loss = 0.32646550\n",
      "Iteration 288, loss = 0.32664108\n",
      "Iteration 289, loss = 0.32648802\n",
      "Iteration 290, loss = 0.32573758\n",
      "Iteration 291, loss = 0.32589902\n",
      "Iteration 292, loss = 0.32622569\n",
      "Iteration 293, loss = 0.32537635\n",
      "Iteration 294, loss = 0.32654218\n",
      "Iteration 295, loss = 0.32601687\n",
      "Iteration 296, loss = 0.32655705\n",
      "Iteration 297, loss = 0.32556739\n",
      "Iteration 298, loss = 0.32598449\n",
      "Iteration 299, loss = 0.32595107\n",
      "Iteration 300, loss = 0.32482437\n",
      "Iteration 301, loss = 0.32382006\n",
      "Iteration 302, loss = 0.32494093\n",
      "Iteration 303, loss = 0.32431139\n",
      "Iteration 304, loss = 0.32489347\n",
      "Iteration 305, loss = 0.32394032\n",
      "Iteration 306, loss = 0.32500919\n",
      "Iteration 307, loss = 0.32289850\n",
      "Iteration 308, loss = 0.32399967\n",
      "Iteration 309, loss = 0.32467157\n",
      "Iteration 310, loss = 0.32452872\n",
      "Iteration 311, loss = 0.32412134\n",
      "Iteration 312, loss = 0.32401685\n",
      "Iteration 313, loss = 0.32441435\n",
      "Iteration 314, loss = 0.32328561\n",
      "Iteration 315, loss = 0.32346700\n",
      "Iteration 316, loss = 0.32441226\n",
      "Iteration 317, loss = 0.32408902\n",
      "Iteration 318, loss = 0.32351633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51721489\n",
      "Iteration 2, loss = 0.47367440\n",
      "Iteration 3, loss = 0.46380117\n",
      "Iteration 4, loss = 0.45811932\n",
      "Iteration 5, loss = 0.45392870\n",
      "Iteration 6, loss = 0.44999717\n",
      "Iteration 7, loss = 0.44612469\n",
      "Iteration 8, loss = 0.44295844\n",
      "Iteration 9, loss = 0.43942155\n",
      "Iteration 10, loss = 0.43675782\n",
      "Iteration 11, loss = 0.43403815\n",
      "Iteration 12, loss = 0.43180152\n",
      "Iteration 13, loss = 0.43033433\n",
      "Iteration 14, loss = 0.42733213\n",
      "Iteration 15, loss = 0.42531860\n",
      "Iteration 16, loss = 0.42373075\n",
      "Iteration 17, loss = 0.42179200\n",
      "Iteration 18, loss = 0.42037421\n",
      "Iteration 19, loss = 0.41877704\n",
      "Iteration 20, loss = 0.41721781\n",
      "Iteration 21, loss = 0.41635827\n",
      "Iteration 22, loss = 0.41458063\n",
      "Iteration 23, loss = 0.41315401\n",
      "Iteration 24, loss = 0.41174226\n",
      "Iteration 25, loss = 0.41050851\n",
      "Iteration 26, loss = 0.40981596\n",
      "Iteration 27, loss = 0.40832366\n",
      "Iteration 28, loss = 0.40733212\n",
      "Iteration 29, loss = 0.40622714\n",
      "Iteration 30, loss = 0.40492119\n",
      "Iteration 31, loss = 0.40399014\n",
      "Iteration 32, loss = 0.40345015\n",
      "Iteration 33, loss = 0.40258356\n",
      "Iteration 34, loss = 0.40088354\n",
      "Iteration 35, loss = 0.39995843\n",
      "Iteration 36, loss = 0.39940730\n",
      "Iteration 37, loss = 0.39822887\n",
      "Iteration 38, loss = 0.39753583\n",
      "Iteration 39, loss = 0.39707633\n",
      "Iteration 40, loss = 0.39694824\n",
      "Iteration 41, loss = 0.39557809\n",
      "Iteration 42, loss = 0.39405173\n",
      "Iteration 43, loss = 0.39376007\n",
      "Iteration 44, loss = 0.39256109\n",
      "Iteration 45, loss = 0.39208271\n",
      "Iteration 46, loss = 0.39114148\n",
      "Iteration 47, loss = 0.39006410\n",
      "Iteration 48, loss = 0.39023887\n",
      "Iteration 49, loss = 0.38870252\n",
      "Iteration 50, loss = 0.38871365\n",
      "Iteration 51, loss = 0.38725238\n",
      "Iteration 52, loss = 0.38641662\n",
      "Iteration 53, loss = 0.38624674\n",
      "Iteration 54, loss = 0.38557249\n",
      "Iteration 55, loss = 0.38479654\n",
      "Iteration 56, loss = 0.38452626\n",
      "Iteration 57, loss = 0.38402767\n",
      "Iteration 58, loss = 0.38291310\n",
      "Iteration 59, loss = 0.38208716\n",
      "Iteration 60, loss = 0.38237839\n",
      "Iteration 61, loss = 0.38123023\n",
      "Iteration 62, loss = 0.37994604\n",
      "Iteration 63, loss = 0.37990088\n",
      "Iteration 64, loss = 0.38056740\n",
      "Iteration 65, loss = 0.37844208\n",
      "Iteration 66, loss = 0.37803600\n",
      "Iteration 67, loss = 0.37704199\n",
      "Iteration 68, loss = 0.37710341\n",
      "Iteration 69, loss = 0.37636387\n",
      "Iteration 70, loss = 0.37625481\n",
      "Iteration 71, loss = 0.37499501\n",
      "Iteration 72, loss = 0.37520641\n",
      "Iteration 73, loss = 0.37451113\n",
      "Iteration 74, loss = 0.37394905\n",
      "Iteration 75, loss = 0.37388026\n",
      "Iteration 76, loss = 0.37282598\n",
      "Iteration 77, loss = 0.37251305\n",
      "Iteration 78, loss = 0.37180972\n",
      "Iteration 79, loss = 0.37092675\n",
      "Iteration 80, loss = 0.37106559\n",
      "Iteration 81, loss = 0.36999753\n",
      "Iteration 82, loss = 0.37040216\n",
      "Iteration 83, loss = 0.37025557\n",
      "Iteration 84, loss = 0.36864835\n",
      "Iteration 85, loss = 0.36868286\n",
      "Iteration 86, loss = 0.36803448\n",
      "Iteration 87, loss = 0.36733124\n",
      "Iteration 88, loss = 0.36734218\n",
      "Iteration 89, loss = 0.36662088\n",
      "Iteration 90, loss = 0.36714804\n",
      "Iteration 91, loss = 0.36675585\n",
      "Iteration 92, loss = 0.36584936\n",
      "Iteration 93, loss = 0.36580057\n",
      "Iteration 94, loss = 0.36470837\n",
      "Iteration 95, loss = 0.36479367\n",
      "Iteration 96, loss = 0.36495001\n",
      "Iteration 97, loss = 0.36439005\n",
      "Iteration 98, loss = 0.36299786\n",
      "Iteration 99, loss = 0.36340006\n",
      "Iteration 100, loss = 0.36261116\n",
      "Iteration 101, loss = 0.36207225\n",
      "Iteration 102, loss = 0.36222069\n",
      "Iteration 103, loss = 0.36177373\n",
      "Iteration 104, loss = 0.36057327\n",
      "Iteration 105, loss = 0.36041308\n",
      "Iteration 106, loss = 0.36037267\n",
      "Iteration 107, loss = 0.36070563\n",
      "Iteration 108, loss = 0.35983506\n",
      "Iteration 109, loss = 0.35953342\n",
      "Iteration 110, loss = 0.35907941\n",
      "Iteration 111, loss = 0.35935235\n",
      "Iteration 112, loss = 0.35843772\n",
      "Iteration 113, loss = 0.35866925\n",
      "Iteration 114, loss = 0.35701306\n",
      "Iteration 115, loss = 0.35688256\n",
      "Iteration 116, loss = 0.35747868\n",
      "Iteration 117, loss = 0.35633401\n",
      "Iteration 118, loss = 0.35687192\n",
      "Iteration 119, loss = 0.35663630\n",
      "Iteration 120, loss = 0.35567598\n",
      "Iteration 121, loss = 0.35519831\n",
      "Iteration 122, loss = 0.35504001\n",
      "Iteration 123, loss = 0.35498923\n",
      "Iteration 124, loss = 0.35462650\n",
      "Iteration 125, loss = 0.35470082\n",
      "Iteration 126, loss = 0.35420502\n",
      "Iteration 127, loss = 0.35380655\n",
      "Iteration 128, loss = 0.35290167\n",
      "Iteration 129, loss = 0.35431090\n",
      "Iteration 130, loss = 0.35398207\n",
      "Iteration 131, loss = 0.35272023\n",
      "Iteration 132, loss = 0.35263660\n",
      "Iteration 133, loss = 0.35239911\n",
      "Iteration 134, loss = 0.35198938\n",
      "Iteration 135, loss = 0.35271004\n",
      "Iteration 136, loss = 0.35146488\n",
      "Iteration 137, loss = 0.35136004\n",
      "Iteration 138, loss = 0.35156731\n",
      "Iteration 139, loss = 0.35001227\n",
      "Iteration 140, loss = 0.35123947\n",
      "Iteration 141, loss = 0.34983145\n",
      "Iteration 142, loss = 0.35189047\n",
      "Iteration 143, loss = 0.35030186\n",
      "Iteration 144, loss = 0.35024703\n",
      "Iteration 145, loss = 0.34962361\n",
      "Iteration 146, loss = 0.34932763\n",
      "Iteration 147, loss = 0.34886827\n",
      "Iteration 148, loss = 0.34916154\n",
      "Iteration 149, loss = 0.34853197\n",
      "Iteration 150, loss = 0.34785314\n",
      "Iteration 151, loss = 0.34839813\n",
      "Iteration 152, loss = 0.34807382\n",
      "Iteration 153, loss = 0.34779509\n",
      "Iteration 154, loss = 0.34759612\n",
      "Iteration 155, loss = 0.34725447\n",
      "Iteration 156, loss = 0.34792619\n",
      "Iteration 157, loss = 0.34695474\n",
      "Iteration 158, loss = 0.34755858\n",
      "Iteration 159, loss = 0.34648899\n",
      "Iteration 160, loss = 0.34581396\n",
      "Iteration 161, loss = 0.34625745\n",
      "Iteration 162, loss = 0.34561961\n",
      "Iteration 163, loss = 0.34517455\n",
      "Iteration 164, loss = 0.34529356\n",
      "Iteration 165, loss = 0.34488068\n",
      "Iteration 166, loss = 0.34516096\n",
      "Iteration 167, loss = 0.34467200\n",
      "Iteration 168, loss = 0.34467243\n",
      "Iteration 169, loss = 0.34436032\n",
      "Iteration 170, loss = 0.34492368\n",
      "Iteration 171, loss = 0.34437494\n",
      "Iteration 172, loss = 0.34443808\n",
      "Iteration 173, loss = 0.34402205\n",
      "Iteration 174, loss = 0.34329093\n",
      "Iteration 175, loss = 0.34373768\n",
      "Iteration 176, loss = 0.34418272\n",
      "Iteration 177, loss = 0.34256818\n",
      "Iteration 178, loss = 0.34297044\n",
      "Iteration 179, loss = 0.34247859\n",
      "Iteration 180, loss = 0.34268671\n",
      "Iteration 181, loss = 0.34285919\n",
      "Iteration 182, loss = 0.34180114\n",
      "Iteration 183, loss = 0.34220966\n",
      "Iteration 184, loss = 0.34160273\n",
      "Iteration 185, loss = 0.34171684\n",
      "Iteration 186, loss = 0.34190448\n",
      "Iteration 187, loss = 0.34080807\n",
      "Iteration 188, loss = 0.34142059\n",
      "Iteration 189, loss = 0.34009780\n",
      "Iteration 190, loss = 0.34021552\n",
      "Iteration 191, loss = 0.34009667\n",
      "Iteration 192, loss = 0.34052795\n",
      "Iteration 193, loss = 0.34003074\n",
      "Iteration 194, loss = 0.33961381\n",
      "Iteration 195, loss = 0.34024211\n",
      "Iteration 196, loss = 0.33934887\n",
      "Iteration 197, loss = 0.34008399\n",
      "Iteration 198, loss = 0.34039833\n",
      "Iteration 199, loss = 0.33863321\n",
      "Iteration 200, loss = 0.33927444\n",
      "Iteration 201, loss = 0.34009746\n",
      "Iteration 202, loss = 0.33867999\n",
      "Iteration 203, loss = 0.33791392\n",
      "Iteration 204, loss = 0.33805482\n",
      "Iteration 205, loss = 0.33768012\n",
      "Iteration 206, loss = 0.33801259\n",
      "Iteration 207, loss = 0.33785859\n",
      "Iteration 208, loss = 0.33906988\n",
      "Iteration 209, loss = 0.33747416\n",
      "Iteration 210, loss = 0.33734296\n",
      "Iteration 211, loss = 0.33664665\n",
      "Iteration 212, loss = 0.33737223\n",
      "Iteration 213, loss = 0.33706709\n",
      "Iteration 214, loss = 0.33842000\n",
      "Iteration 215, loss = 0.33669688\n",
      "Iteration 216, loss = 0.33814227\n",
      "Iteration 217, loss = 0.33548153\n",
      "Iteration 218, loss = 0.33747166\n",
      "Iteration 219, loss = 0.33516280\n",
      "Iteration 220, loss = 0.33576783\n",
      "Iteration 221, loss = 0.33597640\n",
      "Iteration 222, loss = 0.33603947\n",
      "Iteration 223, loss = 0.33633947\n",
      "Iteration 224, loss = 0.33589346\n",
      "Iteration 225, loss = 0.33604222\n",
      "Iteration 226, loss = 0.33495032\n",
      "Iteration 227, loss = 0.33478820\n",
      "Iteration 228, loss = 0.33617229\n",
      "Iteration 229, loss = 0.33494008\n",
      "Iteration 230, loss = 0.33494986\n",
      "Iteration 231, loss = 0.33379979\n",
      "Iteration 232, loss = 0.33379479\n",
      "Iteration 233, loss = 0.33430698\n",
      "Iteration 234, loss = 0.33411586\n",
      "Iteration 235, loss = 0.33447370\n",
      "Iteration 236, loss = 0.33485419\n",
      "Iteration 237, loss = 0.33424405\n",
      "Iteration 238, loss = 0.33401161\n",
      "Iteration 239, loss = 0.33383972\n",
      "Iteration 240, loss = 0.33271877\n",
      "Iteration 241, loss = 0.33319400\n",
      "Iteration 242, loss = 0.33295044\n",
      "Iteration 243, loss = 0.33341015\n",
      "Iteration 244, loss = 0.33260688\n",
      "Iteration 245, loss = 0.33306962\n",
      "Iteration 246, loss = 0.33252020\n",
      "Iteration 247, loss = 0.33245488\n",
      "Iteration 248, loss = 0.33348042\n",
      "Iteration 249, loss = 0.33256071\n",
      "Iteration 250, loss = 0.33228038\n",
      "Iteration 251, loss = 0.33262727\n",
      "Iteration 252, loss = 0.33336768\n",
      "Iteration 253, loss = 0.33246073\n",
      "Iteration 254, loss = 0.33165541\n",
      "Iteration 255, loss = 0.33167324\n",
      "Iteration 256, loss = 0.33250678\n",
      "Iteration 257, loss = 0.33159813\n",
      "Iteration 258, loss = 0.33076761\n",
      "Iteration 259, loss = 0.33148901\n",
      "Iteration 260, loss = 0.33150732\n",
      "Iteration 261, loss = 0.33094208\n",
      "Iteration 262, loss = 0.33153066\n",
      "Iteration 263, loss = 0.33118270\n",
      "Iteration 264, loss = 0.33231032\n",
      "Iteration 265, loss = 0.33043178\n",
      "Iteration 266, loss = 0.33022496\n",
      "Iteration 267, loss = 0.33041679\n",
      "Iteration 268, loss = 0.33101945\n",
      "Iteration 269, loss = 0.33040855\n",
      "Iteration 270, loss = 0.32979188\n",
      "Iteration 271, loss = 0.33046148\n",
      "Iteration 272, loss = 0.33066028\n",
      "Iteration 273, loss = 0.32983473\n",
      "Iteration 274, loss = 0.33018712\n",
      "Iteration 275, loss = 0.32958708\n",
      "Iteration 276, loss = 0.32971516\n",
      "Iteration 277, loss = 0.32918008\n",
      "Iteration 278, loss = 0.32824463\n",
      "Iteration 279, loss = 0.33007624\n",
      "Iteration 280, loss = 0.32991821\n",
      "Iteration 281, loss = 0.32932100\n",
      "Iteration 282, loss = 0.33015587\n",
      "Iteration 283, loss = 0.33010144\n",
      "Iteration 284, loss = 0.32738882\n",
      "Iteration 285, loss = 0.32944382\n",
      "Iteration 286, loss = 0.32849704\n",
      "Iteration 287, loss = 0.32847789\n",
      "Iteration 288, loss = 0.32805018\n",
      "Iteration 289, loss = 0.32775288\n",
      "Iteration 290, loss = 0.32877115\n",
      "Iteration 291, loss = 0.32875434\n",
      "Iteration 292, loss = 0.32707742\n",
      "Iteration 293, loss = 0.32866019\n",
      "Iteration 294, loss = 0.32845567\n",
      "Iteration 295, loss = 0.32733289\n",
      "Iteration 296, loss = 0.32729889\n",
      "Iteration 297, loss = 0.32752097\n",
      "Iteration 298, loss = 0.32739560\n",
      "Iteration 299, loss = 0.32716000\n",
      "Iteration 300, loss = 0.32707832\n",
      "Iteration 301, loss = 0.32718445\n",
      "Iteration 302, loss = 0.32702215\n",
      "Iteration 303, loss = 0.32609373\n",
      "Iteration 304, loss = 0.32656485\n",
      "Iteration 305, loss = 0.32674515\n",
      "Iteration 306, loss = 0.32626807\n",
      "Iteration 307, loss = 0.32728834\n",
      "Iteration 308, loss = 0.32699981\n",
      "Iteration 309, loss = 0.32562103\n",
      "Iteration 310, loss = 0.32758222\n",
      "Iteration 311, loss = 0.32706409\n",
      "Iteration 312, loss = 0.32605650\n",
      "Iteration 313, loss = 0.32604246\n",
      "Iteration 314, loss = 0.32551781\n",
      "Iteration 315, loss = 0.32609916\n",
      "Iteration 316, loss = 0.32583961\n",
      "Iteration 317, loss = 0.32555724\n",
      "Iteration 318, loss = 0.32575047\n",
      "Iteration 319, loss = 0.32604487\n",
      "Iteration 320, loss = 0.32549222\n",
      "Iteration 321, loss = 0.32457643\n",
      "Iteration 322, loss = 0.32444576\n",
      "Iteration 323, loss = 0.32446137\n",
      "Iteration 324, loss = 0.32540050\n",
      "Iteration 325, loss = 0.32461352\n",
      "Iteration 326, loss = 0.32476563\n",
      "Iteration 327, loss = 0.32431126\n",
      "Iteration 328, loss = 0.32505859\n",
      "Iteration 329, loss = 0.32444117\n",
      "Iteration 330, loss = 0.32431730\n",
      "Iteration 331, loss = 0.32400783\n",
      "Iteration 332, loss = 0.32399747\n",
      "Iteration 333, loss = 0.32454968\n",
      "Iteration 334, loss = 0.32495614\n",
      "Iteration 335, loss = 0.32495221\n",
      "Iteration 336, loss = 0.32339066\n",
      "Iteration 337, loss = 0.32390550\n",
      "Iteration 338, loss = 0.32467060\n",
      "Iteration 339, loss = 0.32337715\n",
      "Iteration 340, loss = 0.32344994\n",
      "Iteration 341, loss = 0.32373466\n",
      "Iteration 342, loss = 0.32369884\n",
      "Iteration 343, loss = 0.32399875\n",
      "Iteration 344, loss = 0.32363517\n",
      "Iteration 345, loss = 0.32315452\n",
      "Iteration 346, loss = 0.32342115\n",
      "Iteration 347, loss = 0.32410046\n",
      "Iteration 348, loss = 0.32312440\n",
      "Iteration 349, loss = 0.32616577\n",
      "Iteration 350, loss = 0.32338207\n",
      "Iteration 351, loss = 0.32151340\n",
      "Iteration 352, loss = 0.32307830\n",
      "Iteration 353, loss = 0.32312894\n",
      "Iteration 354, loss = 0.32217786\n",
      "Iteration 355, loss = 0.32254961\n",
      "Iteration 356, loss = 0.32336993\n",
      "Iteration 357, loss = 0.32347883\n",
      "Iteration 358, loss = 0.32198511\n",
      "Iteration 359, loss = 0.32204805\n",
      "Iteration 360, loss = 0.32174872\n",
      "Iteration 361, loss = 0.32193338\n",
      "Iteration 362, loss = 0.32170155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50545712\n",
      "Iteration 2, loss = 0.46224298\n",
      "Iteration 3, loss = 0.45458111\n",
      "Iteration 4, loss = 0.44858376\n",
      "Iteration 5, loss = 0.44473412\n",
      "Iteration 6, loss = 0.44122085\n",
      "Iteration 7, loss = 0.43842858\n",
      "Iteration 8, loss = 0.43514615\n",
      "Iteration 9, loss = 0.43263476\n",
      "Iteration 10, loss = 0.43008307\n",
      "Iteration 11, loss = 0.42769113\n",
      "Iteration 12, loss = 0.42548721\n",
      "Iteration 13, loss = 0.42308749\n",
      "Iteration 14, loss = 0.42104722\n",
      "Iteration 15, loss = 0.41943573\n",
      "Iteration 16, loss = 0.41731916\n",
      "Iteration 17, loss = 0.41575866\n",
      "Iteration 18, loss = 0.41420211\n",
      "Iteration 19, loss = 0.41280668\n",
      "Iteration 20, loss = 0.41056112\n",
      "Iteration 21, loss = 0.40940478\n",
      "Iteration 22, loss = 0.40793900\n",
      "Iteration 23, loss = 0.40707030\n",
      "Iteration 24, loss = 0.40558291\n",
      "Iteration 25, loss = 0.40419967\n",
      "Iteration 26, loss = 0.40307482\n",
      "Iteration 27, loss = 0.40221120\n",
      "Iteration 28, loss = 0.40071611\n",
      "Iteration 29, loss = 0.39923967\n",
      "Iteration 30, loss = 0.39850169\n",
      "Iteration 31, loss = 0.39751355\n",
      "Iteration 32, loss = 0.39633072\n",
      "Iteration 33, loss = 0.39565842\n",
      "Iteration 34, loss = 0.39484884\n",
      "Iteration 35, loss = 0.39361457\n",
      "Iteration 36, loss = 0.39283131\n",
      "Iteration 37, loss = 0.39153093\n",
      "Iteration 38, loss = 0.39023770\n",
      "Iteration 39, loss = 0.38967449\n",
      "Iteration 40, loss = 0.38927479\n",
      "Iteration 41, loss = 0.38842751\n",
      "Iteration 42, loss = 0.38718879\n",
      "Iteration 43, loss = 0.38648953\n",
      "Iteration 44, loss = 0.38580973\n",
      "Iteration 45, loss = 0.38484141\n",
      "Iteration 46, loss = 0.38380830\n",
      "Iteration 47, loss = 0.38366019\n",
      "Iteration 48, loss = 0.38258219\n",
      "Iteration 49, loss = 0.38186565\n",
      "Iteration 50, loss = 0.38127220\n",
      "Iteration 51, loss = 0.38078223\n",
      "Iteration 52, loss = 0.37975601\n",
      "Iteration 53, loss = 0.37956082\n",
      "Iteration 54, loss = 0.37828226\n",
      "Iteration 55, loss = 0.37796586\n",
      "Iteration 56, loss = 0.37764507\n",
      "Iteration 57, loss = 0.37743891\n",
      "Iteration 58, loss = 0.37638698\n",
      "Iteration 59, loss = 0.37523346\n",
      "Iteration 60, loss = 0.37505948\n",
      "Iteration 61, loss = 0.37460156\n",
      "Iteration 62, loss = 0.37372264\n",
      "Iteration 63, loss = 0.37287895\n",
      "Iteration 64, loss = 0.37289459\n",
      "Iteration 65, loss = 0.37147321\n",
      "Iteration 66, loss = 0.37050024\n",
      "Iteration 67, loss = 0.37047701\n",
      "Iteration 68, loss = 0.37059467\n",
      "Iteration 69, loss = 0.37015860\n",
      "Iteration 70, loss = 0.36949627\n",
      "Iteration 71, loss = 0.36865653\n",
      "Iteration 72, loss = 0.36802178\n",
      "Iteration 73, loss = 0.36748738\n",
      "Iteration 74, loss = 0.36608927\n",
      "Iteration 75, loss = 0.36631936\n",
      "Iteration 76, loss = 0.36633039\n",
      "Iteration 77, loss = 0.36549241\n",
      "Iteration 78, loss = 0.36457967\n",
      "Iteration 79, loss = 0.36504128\n",
      "Iteration 80, loss = 0.36472384\n",
      "Iteration 81, loss = 0.36339884\n",
      "Iteration 82, loss = 0.36415508\n",
      "Iteration 83, loss = 0.36353539\n",
      "Iteration 84, loss = 0.36225504\n",
      "Iteration 85, loss = 0.36230937\n",
      "Iteration 86, loss = 0.36154788\n",
      "Iteration 87, loss = 0.36086581\n",
      "Iteration 88, loss = 0.36086717\n",
      "Iteration 89, loss = 0.36015405\n",
      "Iteration 90, loss = 0.36019040\n",
      "Iteration 91, loss = 0.35976844\n",
      "Iteration 92, loss = 0.35955093\n",
      "Iteration 93, loss = 0.35891448\n",
      "Iteration 94, loss = 0.35829505\n",
      "Iteration 95, loss = 0.35801971\n",
      "Iteration 96, loss = 0.35728351\n",
      "Iteration 97, loss = 0.35751329\n",
      "Iteration 98, loss = 0.35698044\n",
      "Iteration 99, loss = 0.35659336\n",
      "Iteration 100, loss = 0.35747254\n",
      "Iteration 101, loss = 0.35667732\n",
      "Iteration 102, loss = 0.35588129\n",
      "Iteration 103, loss = 0.35498514\n",
      "Iteration 104, loss = 0.35425976\n",
      "Iteration 105, loss = 0.35567571\n",
      "Iteration 106, loss = 0.35343750\n",
      "Iteration 107, loss = 0.35409470\n",
      "Iteration 108, loss = 0.35369246\n",
      "Iteration 109, loss = 0.35306786\n",
      "Iteration 110, loss = 0.35260813\n",
      "Iteration 111, loss = 0.35232639\n",
      "Iteration 112, loss = 0.35230895\n",
      "Iteration 113, loss = 0.35117498\n",
      "Iteration 114, loss = 0.35172475\n",
      "Iteration 115, loss = 0.35191444\n",
      "Iteration 116, loss = 0.35094759\n",
      "Iteration 117, loss = 0.35131194\n",
      "Iteration 118, loss = 0.35029848\n",
      "Iteration 119, loss = 0.35013278\n",
      "Iteration 120, loss = 0.34977087\n",
      "Iteration 121, loss = 0.34986526\n",
      "Iteration 122, loss = 0.34911198\n",
      "Iteration 123, loss = 0.34792060\n",
      "Iteration 124, loss = 0.34867568\n",
      "Iteration 125, loss = 0.34889076\n",
      "Iteration 126, loss = 0.34826288\n",
      "Iteration 127, loss = 0.34733812\n",
      "Iteration 128, loss = 0.34744448\n",
      "Iteration 129, loss = 0.34744212\n",
      "Iteration 130, loss = 0.34676548\n",
      "Iteration 131, loss = 0.34714973\n",
      "Iteration 132, loss = 0.34695030\n",
      "Iteration 133, loss = 0.34650072\n",
      "Iteration 134, loss = 0.34603643\n",
      "Iteration 135, loss = 0.34557241\n",
      "Iteration 136, loss = 0.34494652\n",
      "Iteration 137, loss = 0.34581478\n",
      "Iteration 138, loss = 0.34384705\n",
      "Iteration 139, loss = 0.34470599\n",
      "Iteration 140, loss = 0.34490860\n",
      "Iteration 141, loss = 0.34453743\n",
      "Iteration 142, loss = 0.34414669\n",
      "Iteration 143, loss = 0.34313176\n",
      "Iteration 144, loss = 0.34360678\n",
      "Iteration 145, loss = 0.34286312\n",
      "Iteration 146, loss = 0.34259047\n",
      "Iteration 147, loss = 0.34286183\n",
      "Iteration 148, loss = 0.34204760\n",
      "Iteration 149, loss = 0.34304720\n",
      "Iteration 150, loss = 0.34186109\n",
      "Iteration 151, loss = 0.34296252\n",
      "Iteration 152, loss = 0.34205746\n",
      "Iteration 153, loss = 0.34159235\n",
      "Iteration 154, loss = 0.34091355\n",
      "Iteration 155, loss = 0.34187683\n",
      "Iteration 156, loss = 0.34015651\n",
      "Iteration 157, loss = 0.34044390\n",
      "Iteration 158, loss = 0.34103943\n",
      "Iteration 159, loss = 0.34068523\n",
      "Iteration 160, loss = 0.34001589\n",
      "Iteration 161, loss = 0.33915194\n",
      "Iteration 162, loss = 0.33892538\n",
      "Iteration 163, loss = 0.34020240\n",
      "Iteration 164, loss = 0.33826362\n",
      "Iteration 165, loss = 0.33824583\n",
      "Iteration 166, loss = 0.33976842\n",
      "Iteration 167, loss = 0.33826599\n",
      "Iteration 168, loss = 0.33840418\n",
      "Iteration 169, loss = 0.33888942\n",
      "Iteration 170, loss = 0.33804261\n",
      "Iteration 171, loss = 0.33795956\n",
      "Iteration 172, loss = 0.33641342\n",
      "Iteration 173, loss = 0.33743233\n",
      "Iteration 174, loss = 0.33695160\n",
      "Iteration 175, loss = 0.33697000\n",
      "Iteration 176, loss = 0.33695427\n",
      "Iteration 177, loss = 0.33664441\n",
      "Iteration 178, loss = 0.33679052\n",
      "Iteration 179, loss = 0.33574805\n",
      "Iteration 180, loss = 0.33706301\n",
      "Iteration 181, loss = 0.33550662\n",
      "Iteration 182, loss = 0.33470482\n",
      "Iteration 183, loss = 0.33567047\n",
      "Iteration 184, loss = 0.33659135\n",
      "Iteration 185, loss = 0.33447219\n",
      "Iteration 186, loss = 0.33555398\n",
      "Iteration 187, loss = 0.33399132\n",
      "Iteration 188, loss = 0.33505506\n",
      "Iteration 189, loss = 0.33434177\n",
      "Iteration 190, loss = 0.33454764\n",
      "Iteration 191, loss = 0.33450209\n",
      "Iteration 192, loss = 0.33395151\n",
      "Iteration 193, loss = 0.33354120\n",
      "Iteration 194, loss = 0.33300436\n",
      "Iteration 195, loss = 0.33387843\n",
      "Iteration 196, loss = 0.33362181\n",
      "Iteration 197, loss = 0.33363069\n",
      "Iteration 198, loss = 0.33315691\n",
      "Iteration 199, loss = 0.33322627\n",
      "Iteration 200, loss = 0.33281229\n",
      "Iteration 201, loss = 0.33274254\n",
      "Iteration 202, loss = 0.33227691\n",
      "Iteration 203, loss = 0.33155479\n",
      "Iteration 204, loss = 0.33211112\n",
      "Iteration 205, loss = 0.33094734\n",
      "Iteration 206, loss = 0.33388119\n",
      "Iteration 207, loss = 0.33037984\n",
      "Iteration 208, loss = 0.33167309\n",
      "Iteration 209, loss = 0.33049340\n",
      "Iteration 210, loss = 0.33276294\n",
      "Iteration 211, loss = 0.33077022\n",
      "Iteration 212, loss = 0.33055156\n",
      "Iteration 213, loss = 0.33001745\n",
      "Iteration 214, loss = 0.33105896\n",
      "Iteration 215, loss = 0.33087112\n",
      "Iteration 216, loss = 0.33141512\n",
      "Iteration 217, loss = 0.33042327\n",
      "Iteration 218, loss = 0.33032742\n",
      "Iteration 219, loss = 0.32969374\n",
      "Iteration 220, loss = 0.32965999\n",
      "Iteration 221, loss = 0.33080965\n",
      "Iteration 222, loss = 0.32937423\n",
      "Iteration 223, loss = 0.32891248\n",
      "Iteration 224, loss = 0.32912728\n",
      "Iteration 225, loss = 0.33049053\n",
      "Iteration 226, loss = 0.32844120\n",
      "Iteration 227, loss = 0.32803144\n",
      "Iteration 228, loss = 0.32823049\n",
      "Iteration 229, loss = 0.32951648\n",
      "Iteration 230, loss = 0.32843213\n",
      "Iteration 231, loss = 0.32806582\n",
      "Iteration 232, loss = 0.32793744\n",
      "Iteration 233, loss = 0.32786597\n",
      "Iteration 234, loss = 0.32781450\n",
      "Iteration 235, loss = 0.32776265\n",
      "Iteration 236, loss = 0.32820442\n",
      "Iteration 237, loss = 0.32739725\n",
      "Iteration 238, loss = 0.32869443\n",
      "Iteration 239, loss = 0.32628379\n",
      "Iteration 240, loss = 0.32653014\n",
      "Iteration 241, loss = 0.32734727\n",
      "Iteration 242, loss = 0.32720236\n",
      "Iteration 243, loss = 0.32682436\n",
      "Iteration 244, loss = 0.32704597\n",
      "Iteration 245, loss = 0.32625809\n",
      "Iteration 246, loss = 0.32675642\n",
      "Iteration 247, loss = 0.32558523\n",
      "Iteration 248, loss = 0.32631830\n",
      "Iteration 249, loss = 0.32644772\n",
      "Iteration 250, loss = 0.32517032\n",
      "Iteration 251, loss = 0.32526226\n",
      "Iteration 252, loss = 0.32548442\n",
      "Iteration 253, loss = 0.32637925\n",
      "Iteration 254, loss = 0.32437670\n",
      "Iteration 255, loss = 0.32497564\n",
      "Iteration 256, loss = 0.32604800\n",
      "Iteration 257, loss = 0.32495555\n",
      "Iteration 258, loss = 0.32576299\n",
      "Iteration 259, loss = 0.32487911\n",
      "Iteration 260, loss = 0.32507660\n",
      "Iteration 261, loss = 0.32434068\n",
      "Iteration 262, loss = 0.32403194\n",
      "Iteration 263, loss = 0.32395663\n",
      "Iteration 264, loss = 0.32474142\n",
      "Iteration 265, loss = 0.32399008\n",
      "Iteration 266, loss = 0.32393804\n",
      "Iteration 267, loss = 0.32273582\n",
      "Iteration 268, loss = 0.32409852\n",
      "Iteration 269, loss = 0.32391414\n",
      "Iteration 270, loss = 0.32280823\n",
      "Iteration 271, loss = 0.32238920\n",
      "Iteration 272, loss = 0.32293992\n",
      "Iteration 273, loss = 0.32394495\n",
      "Iteration 274, loss = 0.32387494\n",
      "Iteration 275, loss = 0.32245882\n",
      "Iteration 276, loss = 0.32278929\n",
      "Iteration 277, loss = 0.32266761\n",
      "Iteration 278, loss = 0.32284782\n",
      "Iteration 279, loss = 0.32234070\n",
      "Iteration 280, loss = 0.32277085\n",
      "Iteration 281, loss = 0.32184963\n",
      "Iteration 282, loss = 0.32312553\n",
      "Iteration 283, loss = 0.32200806\n",
      "Iteration 284, loss = 0.32193246\n",
      "Iteration 285, loss = 0.32206985\n",
      "Iteration 286, loss = 0.32212954\n",
      "Iteration 287, loss = 0.32159314\n",
      "Iteration 288, loss = 0.32281045\n",
      "Iteration 289, loss = 0.32218558\n",
      "Iteration 290, loss = 0.32125698\n",
      "Iteration 291, loss = 0.32040872\n",
      "Iteration 292, loss = 0.32043229\n",
      "Iteration 293, loss = 0.32142229\n",
      "Iteration 294, loss = 0.32133244\n",
      "Iteration 295, loss = 0.32205240\n",
      "Iteration 296, loss = 0.32046587\n",
      "Iteration 297, loss = 0.32154512\n",
      "Iteration 298, loss = 0.32169953\n",
      "Iteration 299, loss = 0.31983923\n",
      "Iteration 300, loss = 0.31976819\n",
      "Iteration 301, loss = 0.32097300\n",
      "Iteration 302, loss = 0.32105917\n",
      "Iteration 303, loss = 0.32091033\n",
      "Iteration 304, loss = 0.31895095\n",
      "Iteration 305, loss = 0.31996479\n",
      "Iteration 306, loss = 0.32008409\n",
      "Iteration 307, loss = 0.31986908\n",
      "Iteration 308, loss = 0.31929055\n",
      "Iteration 309, loss = 0.31916078\n",
      "Iteration 310, loss = 0.31971495\n",
      "Iteration 311, loss = 0.31988766\n",
      "Iteration 312, loss = 0.31969193\n",
      "Iteration 313, loss = 0.31956677\n",
      "Iteration 314, loss = 0.31904681\n",
      "Iteration 315, loss = 0.31856233\n",
      "Iteration 316, loss = 0.31864464\n",
      "Iteration 317, loss = 0.31975008\n",
      "Iteration 318, loss = 0.31809782\n",
      "Iteration 319, loss = 0.31951393\n",
      "Iteration 320, loss = 0.31913648\n",
      "Iteration 321, loss = 0.31917871\n",
      "Iteration 322, loss = 0.31831036\n",
      "Iteration 323, loss = 0.31933754\n",
      "Iteration 324, loss = 0.31846804\n",
      "Iteration 325, loss = 0.31723728\n",
      "Iteration 326, loss = 0.31753754\n",
      "Iteration 327, loss = 0.31900529\n",
      "Iteration 328, loss = 0.31812220\n",
      "Iteration 329, loss = 0.31664432\n",
      "Iteration 330, loss = 0.31876557\n",
      "Iteration 331, loss = 0.31845393\n",
      "Iteration 332, loss = 0.31730137\n",
      "Iteration 333, loss = 0.31867120\n",
      "Iteration 334, loss = 0.31827817\n",
      "Iteration 335, loss = 0.31734611\n",
      "Iteration 336, loss = 0.31650825\n",
      "Iteration 337, loss = 0.31741875\n",
      "Iteration 338, loss = 0.31623212\n",
      "Iteration 339, loss = 0.31715517\n",
      "Iteration 340, loss = 0.31809507\n",
      "Iteration 341, loss = 0.31640871\n",
      "Iteration 342, loss = 0.31850429\n",
      "Iteration 343, loss = 0.31751692\n",
      "Iteration 344, loss = 0.31606467\n",
      "Iteration 345, loss = 0.31610195\n",
      "Iteration 346, loss = 0.31645489\n",
      "Iteration 347, loss = 0.31775271\n",
      "Iteration 348, loss = 0.31619466\n",
      "Iteration 349, loss = 0.31666981\n",
      "Iteration 350, loss = 0.31752550\n",
      "Iteration 351, loss = 0.31634206\n",
      "Iteration 352, loss = 0.31570268\n",
      "Iteration 353, loss = 0.31681164\n",
      "Iteration 354, loss = 0.31546171\n",
      "Iteration 355, loss = 0.31706083\n",
      "Iteration 356, loss = 0.31537798\n",
      "Iteration 357, loss = 0.31635265\n",
      "Iteration 358, loss = 0.31587092\n",
      "Iteration 359, loss = 0.31709473\n",
      "Iteration 360, loss = 0.31525124\n",
      "Iteration 361, loss = 0.31458720\n",
      "Iteration 362, loss = 0.31671649\n",
      "Iteration 363, loss = 0.31560482\n",
      "Iteration 364, loss = 0.31468519\n",
      "Iteration 365, loss = 0.31536126\n",
      "Iteration 366, loss = 0.31564985\n",
      "Iteration 367, loss = 0.31445328\n",
      "Iteration 368, loss = 0.31658199\n",
      "Iteration 369, loss = 0.31466569\n",
      "Iteration 370, loss = 0.31512896\n",
      "Iteration 371, loss = 0.31456735\n",
      "Iteration 372, loss = 0.31462018\n",
      "Iteration 373, loss = 0.31464987\n",
      "Iteration 374, loss = 0.31465806\n",
      "Iteration 375, loss = 0.31362739\n",
      "Iteration 376, loss = 0.31382811\n",
      "Iteration 377, loss = 0.31682595\n",
      "Iteration 378, loss = 0.31409664\n",
      "Iteration 379, loss = 0.31598885\n",
      "Iteration 380, loss = 0.31441585\n",
      "Iteration 381, loss = 0.31400789\n",
      "Iteration 382, loss = 0.31576152\n",
      "Iteration 383, loss = 0.31441533\n",
      "Iteration 384, loss = 0.31418743\n",
      "Iteration 385, loss = 0.31393737\n",
      "Iteration 386, loss = 0.31380977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51725041\n",
      "Iteration 2, loss = 0.46627679\n",
      "Iteration 3, loss = 0.45729822\n",
      "Iteration 4, loss = 0.45226981\n",
      "Iteration 5, loss = 0.44768662\n",
      "Iteration 6, loss = 0.44282162\n",
      "Iteration 7, loss = 0.43940994\n",
      "Iteration 8, loss = 0.43619525\n",
      "Iteration 9, loss = 0.43314937\n",
      "Iteration 10, loss = 0.43058853\n",
      "Iteration 11, loss = 0.42753266\n",
      "Iteration 12, loss = 0.42506289\n",
      "Iteration 13, loss = 0.42308782\n",
      "Iteration 14, loss = 0.42085882\n",
      "Iteration 15, loss = 0.41909810\n",
      "Iteration 16, loss = 0.41716892\n",
      "Iteration 17, loss = 0.41485577\n",
      "Iteration 18, loss = 0.41316411\n",
      "Iteration 19, loss = 0.41154495\n",
      "Iteration 20, loss = 0.41011647\n",
      "Iteration 21, loss = 0.40902334\n",
      "Iteration 22, loss = 0.40746525\n",
      "Iteration 23, loss = 0.40619937\n",
      "Iteration 24, loss = 0.40468812\n",
      "Iteration 25, loss = 0.40306790\n",
      "Iteration 26, loss = 0.40237313\n",
      "Iteration 27, loss = 0.40116632\n",
      "Iteration 28, loss = 0.39966436\n",
      "Iteration 29, loss = 0.39874494\n",
      "Iteration 30, loss = 0.39765108\n",
      "Iteration 31, loss = 0.39617485\n",
      "Iteration 32, loss = 0.39533629\n",
      "Iteration 33, loss = 0.39468267\n",
      "Iteration 34, loss = 0.39369941\n",
      "Iteration 35, loss = 0.39307787\n",
      "Iteration 36, loss = 0.39153176\n",
      "Iteration 37, loss = 0.39112494\n",
      "Iteration 38, loss = 0.39026984\n",
      "Iteration 39, loss = 0.38867248\n",
      "Iteration 40, loss = 0.38846555\n",
      "Iteration 41, loss = 0.38814078\n",
      "Iteration 42, loss = 0.38704644\n",
      "Iteration 43, loss = 0.38679619\n",
      "Iteration 44, loss = 0.38517190\n",
      "Iteration 45, loss = 0.38433069\n",
      "Iteration 46, loss = 0.38369138\n",
      "Iteration 47, loss = 0.38404136\n",
      "Iteration 48, loss = 0.38261197\n",
      "Iteration 49, loss = 0.38150725\n",
      "Iteration 50, loss = 0.38091332\n",
      "Iteration 51, loss = 0.38095680\n",
      "Iteration 52, loss = 0.38002661\n",
      "Iteration 53, loss = 0.37959070\n",
      "Iteration 54, loss = 0.37888209\n",
      "Iteration 55, loss = 0.37791904\n",
      "Iteration 56, loss = 0.37722455\n",
      "Iteration 57, loss = 0.37689808\n",
      "Iteration 58, loss = 0.37612608\n",
      "Iteration 59, loss = 0.37527409\n",
      "Iteration 60, loss = 0.37491570\n",
      "Iteration 61, loss = 0.37454099\n",
      "Iteration 62, loss = 0.37475666\n",
      "Iteration 63, loss = 0.37430394\n",
      "Iteration 64, loss = 0.37270732\n",
      "Iteration 65, loss = 0.37220954\n",
      "Iteration 66, loss = 0.37213506\n",
      "Iteration 67, loss = 0.37135947\n",
      "Iteration 68, loss = 0.37070710\n",
      "Iteration 69, loss = 0.37060443\n",
      "Iteration 70, loss = 0.37036640\n",
      "Iteration 71, loss = 0.36970465\n",
      "Iteration 72, loss = 0.36879914\n",
      "Iteration 73, loss = 0.36855773\n",
      "Iteration 74, loss = 0.36823510\n",
      "Iteration 75, loss = 0.36773019\n",
      "Iteration 76, loss = 0.36710991\n",
      "Iteration 77, loss = 0.36714536\n",
      "Iteration 78, loss = 0.36721194\n",
      "Iteration 79, loss = 0.36616061\n",
      "Iteration 80, loss = 0.36612070\n",
      "Iteration 81, loss = 0.36569673\n",
      "Iteration 82, loss = 0.36399670\n",
      "Iteration 83, loss = 0.36440391\n",
      "Iteration 84, loss = 0.36337236\n",
      "Iteration 85, loss = 0.36358187\n",
      "Iteration 86, loss = 0.36318489\n",
      "Iteration 87, loss = 0.36310179\n",
      "Iteration 88, loss = 0.36217581\n",
      "Iteration 89, loss = 0.36156752\n",
      "Iteration 90, loss = 0.36145627\n",
      "Iteration 91, loss = 0.36134621\n",
      "Iteration 92, loss = 0.36073733\n",
      "Iteration 93, loss = 0.36070857\n",
      "Iteration 94, loss = 0.35995936\n",
      "Iteration 95, loss = 0.35893560\n",
      "Iteration 96, loss = 0.35904512\n",
      "Iteration 97, loss = 0.35982631\n",
      "Iteration 98, loss = 0.35802475\n",
      "Iteration 99, loss = 0.35822585\n",
      "Iteration 100, loss = 0.35823692\n",
      "Iteration 101, loss = 0.35726618\n",
      "Iteration 102, loss = 0.35793371\n",
      "Iteration 103, loss = 0.35645857\n",
      "Iteration 104, loss = 0.35716708\n",
      "Iteration 105, loss = 0.35613317\n",
      "Iteration 106, loss = 0.35517725\n",
      "Iteration 107, loss = 0.35608491\n",
      "Iteration 108, loss = 0.35463341\n",
      "Iteration 109, loss = 0.35465168\n",
      "Iteration 110, loss = 0.35462941\n",
      "Iteration 111, loss = 0.35388375\n",
      "Iteration 112, loss = 0.35452188\n",
      "Iteration 113, loss = 0.35394447\n",
      "Iteration 114, loss = 0.35337061\n",
      "Iteration 115, loss = 0.35252082\n",
      "Iteration 116, loss = 0.35384014\n",
      "Iteration 117, loss = 0.35279104\n",
      "Iteration 118, loss = 0.35266575\n",
      "Iteration 119, loss = 0.35173698\n",
      "Iteration 120, loss = 0.35165352\n",
      "Iteration 121, loss = 0.35157048\n",
      "Iteration 122, loss = 0.35131431\n",
      "Iteration 123, loss = 0.35087043\n",
      "Iteration 124, loss = 0.35086501\n",
      "Iteration 125, loss = 0.34998210\n",
      "Iteration 126, loss = 0.35044415\n",
      "Iteration 127, loss = 0.34945541\n",
      "Iteration 128, loss = 0.34950694\n",
      "Iteration 129, loss = 0.35005098\n",
      "Iteration 130, loss = 0.34800201\n",
      "Iteration 131, loss = 0.34911312\n",
      "Iteration 132, loss = 0.34839237\n",
      "Iteration 133, loss = 0.34855838\n",
      "Iteration 134, loss = 0.34808068\n",
      "Iteration 135, loss = 0.34928501\n",
      "Iteration 136, loss = 0.34754772\n",
      "Iteration 137, loss = 0.34731017\n",
      "Iteration 138, loss = 0.34746634\n",
      "Iteration 139, loss = 0.34690796\n",
      "Iteration 140, loss = 0.34682698\n",
      "Iteration 141, loss = 0.34641110\n",
      "Iteration 142, loss = 0.34549531\n",
      "Iteration 143, loss = 0.34541364\n",
      "Iteration 144, loss = 0.34622568\n",
      "Iteration 145, loss = 0.34561345\n",
      "Iteration 146, loss = 0.34548212\n",
      "Iteration 147, loss = 0.34506424\n",
      "Iteration 148, loss = 0.34504910\n",
      "Iteration 149, loss = 0.34489585\n",
      "Iteration 150, loss = 0.34431519\n",
      "Iteration 151, loss = 0.34360882\n",
      "Iteration 152, loss = 0.34368072\n",
      "Iteration 153, loss = 0.34296915\n",
      "Iteration 154, loss = 0.34299925\n",
      "Iteration 155, loss = 0.34271473\n",
      "Iteration 156, loss = 0.34284225\n",
      "Iteration 157, loss = 0.34314548\n",
      "Iteration 158, loss = 0.34255498\n",
      "Iteration 159, loss = 0.34283827\n",
      "Iteration 160, loss = 0.34293390\n",
      "Iteration 161, loss = 0.34219269\n",
      "Iteration 162, loss = 0.34210920\n",
      "Iteration 163, loss = 0.34226046\n",
      "Iteration 164, loss = 0.34163432\n",
      "Iteration 165, loss = 0.34081121\n",
      "Iteration 166, loss = 0.34155806\n",
      "Iteration 167, loss = 0.34051178\n",
      "Iteration 168, loss = 0.34069191\n",
      "Iteration 169, loss = 0.33975724\n",
      "Iteration 170, loss = 0.34066144\n",
      "Iteration 171, loss = 0.33946611\n",
      "Iteration 172, loss = 0.33904468\n",
      "Iteration 173, loss = 0.34134307\n",
      "Iteration 174, loss = 0.33937445\n",
      "Iteration 175, loss = 0.33969927\n",
      "Iteration 176, loss = 0.33935358\n",
      "Iteration 177, loss = 0.33879856\n",
      "Iteration 178, loss = 0.33969945\n",
      "Iteration 179, loss = 0.33892475\n",
      "Iteration 180, loss = 0.33940767\n",
      "Iteration 181, loss = 0.33939650\n",
      "Iteration 182, loss = 0.33830079\n",
      "Iteration 183, loss = 0.33858661\n",
      "Iteration 184, loss = 0.33747305\n",
      "Iteration 185, loss = 0.33776742\n",
      "Iteration 186, loss = 0.33721621\n",
      "Iteration 187, loss = 0.33864897\n",
      "Iteration 188, loss = 0.33761928\n",
      "Iteration 189, loss = 0.33788703\n",
      "Iteration 190, loss = 0.33729978\n",
      "Iteration 191, loss = 0.33666474\n",
      "Iteration 192, loss = 0.33684240\n",
      "Iteration 193, loss = 0.33694829\n",
      "Iteration 194, loss = 0.33588942\n",
      "Iteration 195, loss = 0.33661528\n",
      "Iteration 196, loss = 0.33704211\n",
      "Iteration 197, loss = 0.33553947\n",
      "Iteration 198, loss = 0.33540168\n",
      "Iteration 199, loss = 0.33537189\n",
      "Iteration 200, loss = 0.33576282\n",
      "Iteration 201, loss = 0.33631014\n",
      "Iteration 202, loss = 0.33562346\n",
      "Iteration 203, loss = 0.33543439\n",
      "Iteration 204, loss = 0.33550871\n",
      "Iteration 205, loss = 0.33516444\n",
      "Iteration 206, loss = 0.33446371\n",
      "Iteration 207, loss = 0.33492834\n",
      "Iteration 208, loss = 0.33514372\n",
      "Iteration 209, loss = 0.33385993\n",
      "Iteration 210, loss = 0.33434263\n",
      "Iteration 211, loss = 0.33357093\n",
      "Iteration 212, loss = 0.33542313\n",
      "Iteration 213, loss = 0.33391361\n",
      "Iteration 214, loss = 0.33360841\n",
      "Iteration 215, loss = 0.33331207\n",
      "Iteration 216, loss = 0.33407750\n",
      "Iteration 217, loss = 0.33414421\n",
      "Iteration 218, loss = 0.33236760\n",
      "Iteration 219, loss = 0.33372493\n",
      "Iteration 220, loss = 0.33214313\n",
      "Iteration 221, loss = 0.33225723\n",
      "Iteration 222, loss = 0.33311941\n",
      "Iteration 223, loss = 0.33355662\n",
      "Iteration 224, loss = 0.33256132\n",
      "Iteration 225, loss = 0.33112751\n",
      "Iteration 226, loss = 0.33189547\n",
      "Iteration 227, loss = 0.33186573\n",
      "Iteration 228, loss = 0.33213507\n",
      "Iteration 229, loss = 0.33150605\n",
      "Iteration 230, loss = 0.33185375\n",
      "Iteration 231, loss = 0.33248476\n",
      "Iteration 232, loss = 0.33127647\n",
      "Iteration 233, loss = 0.33098142\n",
      "Iteration 234, loss = 0.33078964\n",
      "Iteration 235, loss = 0.33103489\n",
      "Iteration 236, loss = 0.33112878\n",
      "Iteration 237, loss = 0.33047948\n",
      "Iteration 238, loss = 0.33096337\n",
      "Iteration 239, loss = 0.32978970\n",
      "Iteration 240, loss = 0.33125178\n",
      "Iteration 241, loss = 0.33094146\n",
      "Iteration 242, loss = 0.33096379\n",
      "Iteration 243, loss = 0.32992191\n",
      "Iteration 244, loss = 0.32930332\n",
      "Iteration 245, loss = 0.32957072\n",
      "Iteration 246, loss = 0.32900539\n",
      "Iteration 247, loss = 0.32924776\n",
      "Iteration 248, loss = 0.32965229\n",
      "Iteration 249, loss = 0.32963175\n",
      "Iteration 250, loss = 0.32912704\n",
      "Iteration 251, loss = 0.32898701\n",
      "Iteration 252, loss = 0.32860029\n",
      "Iteration 253, loss = 0.32879597\n",
      "Iteration 254, loss = 0.32859391\n",
      "Iteration 255, loss = 0.32928275\n",
      "Iteration 256, loss = 0.32842900\n",
      "Iteration 257, loss = 0.32807603\n",
      "Iteration 258, loss = 0.32917142\n",
      "Iteration 259, loss = 0.32805238\n",
      "Iteration 260, loss = 0.32805968\n",
      "Iteration 261, loss = 0.32779873\n",
      "Iteration 262, loss = 0.32810524\n",
      "Iteration 263, loss = 0.32722690\n",
      "Iteration 264, loss = 0.32751350\n",
      "Iteration 265, loss = 0.32736485\n",
      "Iteration 266, loss = 0.32690341\n",
      "Iteration 267, loss = 0.32783568\n",
      "Iteration 268, loss = 0.32822203\n",
      "Iteration 269, loss = 0.32731274\n",
      "Iteration 270, loss = 0.32729563\n",
      "Iteration 271, loss = 0.32681042\n",
      "Iteration 272, loss = 0.32752035\n",
      "Iteration 273, loss = 0.32679775\n",
      "Iteration 274, loss = 0.32598462\n",
      "Iteration 275, loss = 0.32670896\n",
      "Iteration 276, loss = 0.32667290\n",
      "Iteration 277, loss = 0.32649863\n",
      "Iteration 278, loss = 0.32641707\n",
      "Iteration 279, loss = 0.32699224\n",
      "Iteration 280, loss = 0.32547566\n",
      "Iteration 281, loss = 0.32513670\n",
      "Iteration 282, loss = 0.32632542\n",
      "Iteration 283, loss = 0.32717930\n",
      "Iteration 284, loss = 0.32688377\n",
      "Iteration 285, loss = 0.32535422\n",
      "Iteration 286, loss = 0.32505663\n",
      "Iteration 287, loss = 0.32565270\n",
      "Iteration 288, loss = 0.32516864\n",
      "Iteration 289, loss = 0.32423838\n",
      "Iteration 290, loss = 0.32528434\n",
      "Iteration 291, loss = 0.32483803\n",
      "Iteration 292, loss = 0.32479948\n",
      "Iteration 293, loss = 0.32390985\n",
      "Iteration 294, loss = 0.32509352\n",
      "Iteration 295, loss = 0.32534919\n",
      "Iteration 296, loss = 0.32474703\n",
      "Iteration 297, loss = 0.32439795\n",
      "Iteration 298, loss = 0.32502479\n",
      "Iteration 299, loss = 0.32458875\n",
      "Iteration 300, loss = 0.32353523\n",
      "Iteration 301, loss = 0.32404099\n",
      "Iteration 302, loss = 0.32339835\n",
      "Iteration 303, loss = 0.32389393\n",
      "Iteration 304, loss = 0.32381125\n",
      "Iteration 305, loss = 0.32436295\n",
      "Iteration 306, loss = 0.32367711\n",
      "Iteration 307, loss = 0.32431579\n",
      "Iteration 308, loss = 0.32327591\n",
      "Iteration 309, loss = 0.32373422\n",
      "Iteration 310, loss = 0.32374032\n",
      "Iteration 311, loss = 0.32242847\n",
      "Iteration 312, loss = 0.32329523\n",
      "Iteration 313, loss = 0.32283137\n",
      "Iteration 314, loss = 0.32360072\n",
      "Iteration 315, loss = 0.32281709\n",
      "Iteration 316, loss = 0.32316663\n",
      "Iteration 317, loss = 0.32240298\n",
      "Iteration 318, loss = 0.32250018\n",
      "Iteration 319, loss = 0.32216633\n",
      "Iteration 320, loss = 0.32220321\n",
      "Iteration 321, loss = 0.32199087\n",
      "Iteration 322, loss = 0.32264728\n",
      "Iteration 323, loss = 0.32172336\n",
      "Iteration 324, loss = 0.32198184\n",
      "Iteration 325, loss = 0.32139326\n",
      "Iteration 326, loss = 0.32297293\n",
      "Iteration 327, loss = 0.32249788\n",
      "Iteration 328, loss = 0.32152295\n",
      "Iteration 329, loss = 0.32288174\n",
      "Iteration 330, loss = 0.32097625\n",
      "Iteration 331, loss = 0.32126509\n",
      "Iteration 332, loss = 0.32126104\n",
      "Iteration 333, loss = 0.32111434\n",
      "Iteration 334, loss = 0.32099264\n",
      "Iteration 335, loss = 0.32110252\n",
      "Iteration 336, loss = 0.32052561\n",
      "Iteration 337, loss = 0.32115928\n",
      "Iteration 338, loss = 0.32005570\n",
      "Iteration 339, loss = 0.32054901\n",
      "Iteration 340, loss = 0.32124146\n",
      "Iteration 341, loss = 0.32138562\n",
      "Iteration 342, loss = 0.32006459\n",
      "Iteration 343, loss = 0.32128156\n",
      "Iteration 344, loss = 0.32040062\n",
      "Iteration 345, loss = 0.32067553\n",
      "Iteration 346, loss = 0.32013412\n",
      "Iteration 347, loss = 0.32041424\n",
      "Iteration 348, loss = 0.32053773\n",
      "Iteration 349, loss = 0.31988392\n",
      "Iteration 350, loss = 0.32122998\n",
      "Iteration 351, loss = 0.32108783\n",
      "Iteration 352, loss = 0.31974621\n",
      "Iteration 353, loss = 0.32003875\n",
      "Iteration 354, loss = 0.31883956\n",
      "Iteration 355, loss = 0.32094408\n",
      "Iteration 356, loss = 0.31948900\n",
      "Iteration 357, loss = 0.31908446\n",
      "Iteration 358, loss = 0.31929421\n",
      "Iteration 359, loss = 0.31889198\n",
      "Iteration 360, loss = 0.31974896\n",
      "Iteration 361, loss = 0.32004379\n",
      "Iteration 362, loss = 0.31901972\n",
      "Iteration 363, loss = 0.31830251\n",
      "Iteration 364, loss = 0.31872828\n",
      "Iteration 365, loss = 0.32042269\n",
      "Iteration 366, loss = 0.31886000\n",
      "Iteration 367, loss = 0.31931940\n",
      "Iteration 368, loss = 0.31779992\n",
      "Iteration 369, loss = 0.31869490\n",
      "Iteration 370, loss = 0.31838398\n",
      "Iteration 371, loss = 0.31886515\n",
      "Iteration 372, loss = 0.31833127\n",
      "Iteration 373, loss = 0.31834511\n",
      "Iteration 374, loss = 0.31945436\n",
      "Iteration 375, loss = 0.31954629\n",
      "Iteration 376, loss = 0.31881783\n",
      "Iteration 377, loss = 0.31822833\n",
      "Iteration 378, loss = 0.31792271\n",
      "Iteration 379, loss = 0.31860156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.58729782\n",
      "Iteration 2, loss = 0.47618521\n",
      "Iteration 3, loss = 0.45063993\n",
      "Iteration 4, loss = 0.43722567\n",
      "Iteration 5, loss = 0.42780608\n",
      "Iteration 6, loss = 0.42139029\n",
      "Iteration 7, loss = 0.41520476\n",
      "Iteration 8, loss = 0.41175500\n",
      "Iteration 9, loss = 0.40650752\n",
      "Iteration 10, loss = 0.40313968\n",
      "Iteration 11, loss = 0.40038622\n",
      "Iteration 12, loss = 0.39745692\n",
      "Iteration 13, loss = 0.39439255\n",
      "Iteration 14, loss = 0.39179444\n",
      "Iteration 15, loss = 0.38963252\n",
      "Iteration 16, loss = 0.38713680\n",
      "Iteration 17, loss = 0.38479016\n",
      "Iteration 18, loss = 0.38347530\n",
      "Iteration 19, loss = 0.38112404\n",
      "Iteration 20, loss = 0.37955750\n",
      "Iteration 21, loss = 0.37806752\n",
      "Iteration 22, loss = 0.37647036\n",
      "Iteration 23, loss = 0.37413532\n",
      "Iteration 24, loss = 0.37298869\n",
      "Iteration 25, loss = 0.37171667\n",
      "Iteration 26, loss = 0.37042704\n",
      "Iteration 27, loss = 0.36929722\n",
      "Iteration 28, loss = 0.36770733\n",
      "Iteration 29, loss = 0.36621117\n",
      "Iteration 30, loss = 0.36488225\n",
      "Iteration 31, loss = 0.36415740\n",
      "Iteration 32, loss = 0.36255482\n",
      "Iteration 33, loss = 0.36140783\n",
      "Iteration 34, loss = 0.36050241\n",
      "Iteration 35, loss = 0.35877393\n",
      "Iteration 36, loss = 0.35753962\n",
      "Iteration 37, loss = 0.35632048\n",
      "Iteration 38, loss = 0.35602506\n",
      "Iteration 39, loss = 0.35510270\n",
      "Iteration 40, loss = 0.35353615\n",
      "Iteration 41, loss = 0.35301444\n",
      "Iteration 42, loss = 0.35163067\n",
      "Iteration 43, loss = 0.35068293\n",
      "Iteration 44, loss = 0.34971465\n",
      "Iteration 45, loss = 0.34929619\n",
      "Iteration 46, loss = 0.34831762\n",
      "Iteration 47, loss = 0.34667048\n",
      "Iteration 48, loss = 0.34646033\n",
      "Iteration 49, loss = 0.34507564\n",
      "Iteration 50, loss = 0.34504568\n",
      "Iteration 51, loss = 0.34353253\n",
      "Iteration 52, loss = 0.34283275\n",
      "Iteration 53, loss = 0.34297554\n",
      "Iteration 54, loss = 0.34151616\n",
      "Iteration 55, loss = 0.34036638\n",
      "Iteration 56, loss = 0.34062443\n",
      "Iteration 57, loss = 0.33913979\n",
      "Iteration 58, loss = 0.33875910\n",
      "Iteration 59, loss = 0.33764909\n",
      "Iteration 60, loss = 0.33738916\n",
      "Iteration 61, loss = 0.33592207\n",
      "Iteration 62, loss = 0.33517844\n",
      "Iteration 63, loss = 0.33513618\n",
      "Iteration 64, loss = 0.33454895\n",
      "Iteration 65, loss = 0.33373980\n",
      "Iteration 66, loss = 0.33364970\n",
      "Iteration 67, loss = 0.33274763\n",
      "Iteration 68, loss = 0.33166323\n",
      "Iteration 69, loss = 0.33043274\n",
      "Iteration 70, loss = 0.33015941\n",
      "Iteration 71, loss = 0.32997077\n",
      "Iteration 72, loss = 0.32998084\n",
      "Iteration 73, loss = 0.32894069\n",
      "Iteration 74, loss = 0.32697587\n",
      "Iteration 75, loss = 0.32721660\n",
      "Iteration 76, loss = 0.32682664\n",
      "Iteration 77, loss = 0.32612876\n",
      "Iteration 78, loss = 0.32618563\n",
      "Iteration 79, loss = 0.32463201\n",
      "Iteration 80, loss = 0.32423874\n",
      "Iteration 81, loss = 0.32317609\n",
      "Iteration 82, loss = 0.32309550\n",
      "Iteration 83, loss = 0.32313822\n",
      "Iteration 84, loss = 0.32229254\n",
      "Iteration 85, loss = 0.32169493\n",
      "Iteration 86, loss = 0.32090218\n",
      "Iteration 87, loss = 0.31996071\n",
      "Iteration 88, loss = 0.31982230\n",
      "Iteration 89, loss = 0.31965450\n",
      "Iteration 90, loss = 0.31968322\n",
      "Iteration 91, loss = 0.31948408\n",
      "Iteration 92, loss = 0.31814817\n",
      "Iteration 93, loss = 0.31724889\n",
      "Iteration 94, loss = 0.31636661\n",
      "Iteration 95, loss = 0.31697729\n",
      "Iteration 96, loss = 0.31696122\n",
      "Iteration 97, loss = 0.31609812\n",
      "Iteration 98, loss = 0.31601973\n",
      "Iteration 99, loss = 0.31425849\n",
      "Iteration 100, loss = 0.31445789\n",
      "Iteration 101, loss = 0.31421703\n",
      "Iteration 102, loss = 0.31413106\n",
      "Iteration 103, loss = 0.31414250\n",
      "Iteration 104, loss = 0.31310661\n",
      "Iteration 105, loss = 0.31299348\n",
      "Iteration 106, loss = 0.31227476\n",
      "Iteration 107, loss = 0.31197806\n",
      "Iteration 108, loss = 0.31200732\n",
      "Iteration 109, loss = 0.31105840\n",
      "Iteration 110, loss = 0.31061644\n",
      "Iteration 111, loss = 0.31059751\n",
      "Iteration 112, loss = 0.31063265\n",
      "Iteration 113, loss = 0.31049307\n",
      "Iteration 114, loss = 0.30947229\n",
      "Iteration 115, loss = 0.30899863\n",
      "Iteration 116, loss = 0.30963293\n",
      "Iteration 117, loss = 0.30835576\n",
      "Iteration 118, loss = 0.30777742\n",
      "Iteration 119, loss = 0.30822856\n",
      "Iteration 120, loss = 0.30829827\n",
      "Iteration 121, loss = 0.30779966\n",
      "Iteration 122, loss = 0.30697264\n",
      "Iteration 123, loss = 0.30669149\n",
      "Iteration 124, loss = 0.30617974\n",
      "Iteration 125, loss = 0.30572946\n",
      "Iteration 126, loss = 0.30527849\n",
      "Iteration 127, loss = 0.30565347\n",
      "Iteration 128, loss = 0.30441505\n",
      "Iteration 129, loss = 0.30524540\n",
      "Iteration 130, loss = 0.30466793\n",
      "Iteration 131, loss = 0.30405046\n",
      "Iteration 132, loss = 0.30398359\n",
      "Iteration 133, loss = 0.30340566\n",
      "Iteration 134, loss = 0.30283401\n",
      "Iteration 135, loss = 0.30333617\n",
      "Iteration 136, loss = 0.30238309\n",
      "Iteration 137, loss = 0.30220639\n",
      "Iteration 138, loss = 0.30301001\n",
      "Iteration 139, loss = 0.30153047\n",
      "Iteration 140, loss = 0.30188939\n",
      "Iteration 141, loss = 0.30097942\n",
      "Iteration 142, loss = 0.30126015\n",
      "Iteration 143, loss = 0.30133906\n",
      "Iteration 144, loss = 0.30092187\n",
      "Iteration 145, loss = 0.29984171\n",
      "Iteration 146, loss = 0.29937106\n",
      "Iteration 147, loss = 0.29902371\n",
      "Iteration 148, loss = 0.29968119\n",
      "Iteration 149, loss = 0.29913709\n",
      "Iteration 150, loss = 0.29854773\n",
      "Iteration 151, loss = 0.29882296\n",
      "Iteration 152, loss = 0.29867843\n",
      "Iteration 153, loss = 0.29774603\n",
      "Iteration 154, loss = 0.29774063\n",
      "Iteration 155, loss = 0.29700031\n",
      "Iteration 156, loss = 0.29769227\n",
      "Iteration 157, loss = 0.29682422\n",
      "Iteration 158, loss = 0.29647632\n",
      "Iteration 159, loss = 0.29672404\n",
      "Iteration 160, loss = 0.29672408\n",
      "Iteration 161, loss = 0.29661839\n",
      "Iteration 162, loss = 0.29653484\n",
      "Iteration 163, loss = 0.29495220\n",
      "Iteration 164, loss = 0.29560330\n",
      "Iteration 165, loss = 0.29548931\n",
      "Iteration 166, loss = 0.29452972\n",
      "Iteration 167, loss = 0.29559108\n",
      "Iteration 168, loss = 0.29438036\n",
      "Iteration 169, loss = 0.29418227\n",
      "Iteration 170, loss = 0.29380148\n",
      "Iteration 171, loss = 0.29363248\n",
      "Iteration 172, loss = 0.29337301\n",
      "Iteration 173, loss = 0.29461186\n",
      "Iteration 174, loss = 0.29379969\n",
      "Iteration 175, loss = 0.29313171\n",
      "Iteration 176, loss = 0.29304657\n",
      "Iteration 177, loss = 0.29294522\n",
      "Iteration 178, loss = 0.29111050\n",
      "Iteration 179, loss = 0.29190636\n",
      "Iteration 180, loss = 0.29166079\n",
      "Iteration 181, loss = 0.29261620\n",
      "Iteration 182, loss = 0.29169677\n",
      "Iteration 183, loss = 0.29123292\n",
      "Iteration 184, loss = 0.29070407\n",
      "Iteration 185, loss = 0.29118325\n",
      "Iteration 186, loss = 0.29097056\n",
      "Iteration 187, loss = 0.29113480\n",
      "Iteration 188, loss = 0.29022133\n",
      "Iteration 189, loss = 0.29017534\n",
      "Iteration 190, loss = 0.29000034\n",
      "Iteration 191, loss = 0.28949798\n",
      "Iteration 192, loss = 0.28911253\n",
      "Iteration 193, loss = 0.29032144\n",
      "Iteration 194, loss = 0.28879964\n",
      "Iteration 195, loss = 0.28901183\n",
      "Iteration 196, loss = 0.28908571\n",
      "Iteration 197, loss = 0.28887449\n",
      "Iteration 198, loss = 0.28854576\n",
      "Iteration 199, loss = 0.28783478\n",
      "Iteration 200, loss = 0.28744173\n",
      "Iteration 201, loss = 0.28725519\n",
      "Iteration 202, loss = 0.28668050\n",
      "Iteration 203, loss = 0.28694859\n",
      "Iteration 204, loss = 0.28770172\n",
      "Iteration 205, loss = 0.28735114\n",
      "Iteration 206, loss = 0.28610397\n",
      "Iteration 207, loss = 0.28829848\n",
      "Iteration 208, loss = 0.28583244\n",
      "Iteration 209, loss = 0.28548621\n",
      "Iteration 210, loss = 0.28692784\n",
      "Iteration 211, loss = 0.28646570\n",
      "Iteration 212, loss = 0.28607810\n",
      "Iteration 213, loss = 0.28575693\n",
      "Iteration 214, loss = 0.28603539\n",
      "Iteration 215, loss = 0.28465748\n",
      "Iteration 216, loss = 0.28513465\n",
      "Iteration 217, loss = 0.28458272\n",
      "Iteration 218, loss = 0.28499883\n",
      "Iteration 219, loss = 0.28453237\n",
      "Iteration 220, loss = 0.28477477\n",
      "Iteration 221, loss = 0.28451623\n",
      "Iteration 222, loss = 0.28402018\n",
      "Iteration 223, loss = 0.28479829\n",
      "Iteration 224, loss = 0.28376740\n",
      "Iteration 225, loss = 0.28385648\n",
      "Iteration 226, loss = 0.28322081\n",
      "Iteration 227, loss = 0.28407443\n",
      "Iteration 228, loss = 0.28290986\n",
      "Iteration 229, loss = 0.28387632\n",
      "Iteration 230, loss = 0.28163581\n",
      "Iteration 231, loss = 0.28418835\n",
      "Iteration 232, loss = 0.28475726\n",
      "Iteration 233, loss = 0.28225172\n",
      "Iteration 234, loss = 0.28251270\n",
      "Iteration 235, loss = 0.28217585\n",
      "Iteration 236, loss = 0.28216137\n",
      "Iteration 237, loss = 0.28120369\n",
      "Iteration 238, loss = 0.28155064\n",
      "Iteration 239, loss = 0.28170268\n",
      "Iteration 240, loss = 0.28142485\n",
      "Iteration 241, loss = 0.28259895\n",
      "Iteration 242, loss = 0.28161843\n",
      "Iteration 243, loss = 0.28094826\n",
      "Iteration 244, loss = 0.28139107\n",
      "Iteration 245, loss = 0.28142006\n",
      "Iteration 246, loss = 0.28091431\n",
      "Iteration 247, loss = 0.27996883\n",
      "Iteration 248, loss = 0.28006789\n",
      "Iteration 249, loss = 0.28100593\n",
      "Iteration 250, loss = 0.28072016\n",
      "Iteration 251, loss = 0.27901224\n",
      "Iteration 252, loss = 0.28026091\n",
      "Iteration 253, loss = 0.28052095\n",
      "Iteration 254, loss = 0.27965842\n",
      "Iteration 255, loss = 0.28025961\n",
      "Iteration 256, loss = 0.27901870\n",
      "Iteration 257, loss = 0.27945405\n",
      "Iteration 258, loss = 0.27915056\n",
      "Iteration 259, loss = 0.27921880\n",
      "Iteration 260, loss = 0.27801168\n",
      "Iteration 261, loss = 0.27825160\n",
      "Iteration 262, loss = 0.27900521\n",
      "Iteration 263, loss = 0.27798805\n",
      "Iteration 264, loss = 0.27852011\n",
      "Iteration 265, loss = 0.27760108\n",
      "Iteration 266, loss = 0.27741592\n",
      "Iteration 267, loss = 0.27761512\n",
      "Iteration 268, loss = 0.27833180\n",
      "Iteration 269, loss = 0.27844605\n",
      "Iteration 270, loss = 0.27686303\n",
      "Iteration 271, loss = 0.27682243\n",
      "Iteration 272, loss = 0.27768366\n",
      "Iteration 273, loss = 0.27705792\n",
      "Iteration 274, loss = 0.27650971\n",
      "Iteration 275, loss = 0.27668932\n",
      "Iteration 276, loss = 0.27789999\n",
      "Iteration 277, loss = 0.27649351\n",
      "Iteration 278, loss = 0.27622028\n",
      "Iteration 279, loss = 0.27651370\n",
      "Iteration 280, loss = 0.27559345\n",
      "Iteration 281, loss = 0.27687394\n",
      "Iteration 282, loss = 0.27606454\n",
      "Iteration 283, loss = 0.27645699\n",
      "Iteration 284, loss = 0.27599132\n",
      "Iteration 285, loss = 0.27612495\n",
      "Iteration 286, loss = 0.27538740\n",
      "Iteration 287, loss = 0.27557927\n",
      "Iteration 288, loss = 0.27480326\n",
      "Iteration 289, loss = 0.27433264\n",
      "Iteration 290, loss = 0.27520767\n",
      "Iteration 291, loss = 0.27451491\n",
      "Iteration 292, loss = 0.27486915\n",
      "Iteration 293, loss = 0.27484112\n",
      "Iteration 294, loss = 0.27596200\n",
      "Iteration 295, loss = 0.27632933\n",
      "Iteration 296, loss = 0.27405938\n",
      "Iteration 297, loss = 0.27415377\n",
      "Iteration 298, loss = 0.27414170\n",
      "Iteration 299, loss = 0.27511268\n",
      "Iteration 300, loss = 0.27312953\n",
      "Iteration 301, loss = 0.27439943\n",
      "Iteration 302, loss = 0.27270870\n",
      "Iteration 303, loss = 0.27394232\n",
      "Iteration 304, loss = 0.27339201\n",
      "Iteration 305, loss = 0.27323490\n",
      "Iteration 306, loss = 0.27284098\n",
      "Iteration 307, loss = 0.27344698\n",
      "Iteration 308, loss = 0.27418348\n",
      "Iteration 309, loss = 0.27378845\n",
      "Iteration 310, loss = 0.27305549\n",
      "Iteration 311, loss = 0.27305912\n",
      "Iteration 312, loss = 0.27193869\n",
      "Iteration 313, loss = 0.27263658\n",
      "Iteration 314, loss = 0.27285783\n",
      "Iteration 315, loss = 0.27207903\n",
      "Iteration 316, loss = 0.27319014\n",
      "Iteration 317, loss = 0.27185663\n",
      "Iteration 318, loss = 0.27184735\n",
      "Iteration 319, loss = 0.27146384\n",
      "Iteration 320, loss = 0.27272765\n",
      "Iteration 321, loss = 0.27172750\n",
      "Iteration 322, loss = 0.27274536\n",
      "Iteration 323, loss = 0.27179199\n",
      "Iteration 324, loss = 0.27150632\n",
      "Iteration 325, loss = 0.27216612\n",
      "Iteration 326, loss = 0.27096561\n",
      "Iteration 327, loss = 0.27170624\n",
      "Iteration 328, loss = 0.27116438\n",
      "Iteration 329, loss = 0.27065224\n",
      "Iteration 330, loss = 0.27014348\n",
      "Iteration 331, loss = 0.27110596\n",
      "Iteration 332, loss = 0.27022204\n",
      "Iteration 333, loss = 0.27034866\n",
      "Iteration 334, loss = 0.27105131\n",
      "Iteration 335, loss = 0.27031452\n",
      "Iteration 336, loss = 0.27035230\n",
      "Iteration 337, loss = 0.27101959\n",
      "Iteration 338, loss = 0.27076707\n",
      "Iteration 339, loss = 0.26945901\n",
      "Iteration 340, loss = 0.27016384\n",
      "Iteration 341, loss = 0.26940202\n",
      "Iteration 342, loss = 0.26988988\n",
      "Iteration 343, loss = 0.26938926\n",
      "Iteration 344, loss = 0.26916986\n",
      "Iteration 345, loss = 0.26976227\n",
      "Iteration 346, loss = 0.26932101\n",
      "Iteration 347, loss = 0.27079651\n",
      "Iteration 348, loss = 0.26951432\n",
      "Iteration 349, loss = 0.27001473\n",
      "Iteration 350, loss = 0.26898114\n",
      "Iteration 351, loss = 0.26910433\n",
      "Iteration 352, loss = 0.26900037\n",
      "Iteration 353, loss = 0.27093059\n",
      "Iteration 354, loss = 0.26805608\n",
      "Iteration 355, loss = 0.26983984\n",
      "Iteration 356, loss = 0.26953231\n",
      "Iteration 357, loss = 0.26842756\n",
      "Iteration 358, loss = 0.26790279\n",
      "Iteration 359, loss = 0.26852577\n",
      "Iteration 360, loss = 0.26736677\n",
      "Iteration 361, loss = 0.26900666\n",
      "Iteration 362, loss = 0.26941246\n",
      "Iteration 363, loss = 0.26808274\n",
      "Iteration 364, loss = 0.26805485\n",
      "Iteration 365, loss = 0.26964474\n",
      "Iteration 366, loss = 0.26807933\n",
      "Iteration 367, loss = 0.26753059\n",
      "Iteration 368, loss = 0.26855658\n",
      "Iteration 369, loss = 0.26745450\n",
      "Iteration 370, loss = 0.26724782\n",
      "Iteration 371, loss = 0.26836249\n",
      "Iteration 372, loss = 0.26800169\n",
      "Iteration 373, loss = 0.26598471\n",
      "Iteration 374, loss = 0.26847310\n",
      "Iteration 375, loss = 0.26670943\n",
      "Iteration 376, loss = 0.26781642\n",
      "Iteration 377, loss = 0.26770205\n",
      "Iteration 378, loss = 0.26698331\n",
      "Iteration 379, loss = 0.26632550\n",
      "Iteration 380, loss = 0.26573162\n",
      "Iteration 381, loss = 0.26730336\n",
      "Iteration 382, loss = 0.26626742\n",
      "Iteration 383, loss = 0.26644668\n",
      "Iteration 384, loss = 0.26641813\n",
      "Iteration 385, loss = 0.26704661\n",
      "Iteration 386, loss = 0.26559542\n",
      "Iteration 387, loss = 0.26628897\n",
      "Iteration 388, loss = 0.26725689\n",
      "Iteration 389, loss = 0.26581124\n",
      "Iteration 390, loss = 0.26544218\n",
      "Iteration 391, loss = 0.26604111\n",
      "Iteration 392, loss = 0.26561174\n",
      "Iteration 393, loss = 0.26586141\n",
      "Iteration 394, loss = 0.26538958\n",
      "Iteration 395, loss = 0.26480923\n",
      "Iteration 396, loss = 0.26598707\n",
      "Iteration 397, loss = 0.26572049\n",
      "Iteration 398, loss = 0.26583782\n",
      "Iteration 399, loss = 0.26629829\n",
      "Iteration 400, loss = 0.26691184\n",
      "Iteration 401, loss = 0.26528881\n",
      "Iteration 402, loss = 0.26727697\n",
      "Iteration 403, loss = 0.26451167\n",
      "Iteration 404, loss = 0.26508312\n",
      "Iteration 405, loss = 0.26502850\n",
      "Iteration 406, loss = 0.26561583\n",
      "Iteration 407, loss = 0.26483250\n",
      "Iteration 408, loss = 0.26561843\n",
      "Iteration 409, loss = 0.26381332\n",
      "Iteration 410, loss = 0.26435539\n",
      "Iteration 411, loss = 0.26553716\n",
      "Iteration 412, loss = 0.26530197\n",
      "Iteration 413, loss = 0.26464503\n",
      "Iteration 414, loss = 0.26500639\n",
      "Iteration 415, loss = 0.26442429\n",
      "Iteration 416, loss = 0.26372705\n",
      "Iteration 417, loss = 0.26456483\n",
      "Iteration 418, loss = 0.26365611\n",
      "Iteration 419, loss = 0.26483511\n",
      "Iteration 420, loss = 0.26374295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.43983430\n",
      "Iteration 2, loss = 0.38173717\n",
      "Iteration 3, loss = 0.37469207\n",
      "Iteration 4, loss = 0.37005351\n",
      "Iteration 5, loss = 0.36623663\n",
      "Iteration 6, loss = 0.36310627\n",
      "Iteration 7, loss = 0.35974528\n",
      "Iteration 8, loss = 0.35736138\n",
      "Iteration 9, loss = 0.35496129\n",
      "Iteration 10, loss = 0.35278826\n",
      "Iteration 11, loss = 0.35086817\n",
      "Iteration 12, loss = 0.34898194\n",
      "Iteration 13, loss = 0.34698668\n",
      "Iteration 14, loss = 0.34537804\n",
      "Iteration 15, loss = 0.34338288\n",
      "Iteration 16, loss = 0.34218543\n",
      "Iteration 17, loss = 0.34032451\n",
      "Iteration 18, loss = 0.33956248\n",
      "Iteration 19, loss = 0.33828115\n",
      "Iteration 20, loss = 0.33696651\n",
      "Iteration 21, loss = 0.33610979\n",
      "Iteration 22, loss = 0.33481424\n",
      "Iteration 23, loss = 0.33385924\n",
      "Iteration 24, loss = 0.33272688\n",
      "Iteration 25, loss = 0.33156975\n",
      "Iteration 26, loss = 0.33089322\n",
      "Iteration 27, loss = 0.32948384\n",
      "Iteration 28, loss = 0.32841317\n",
      "Iteration 29, loss = 0.32774780\n",
      "Iteration 30, loss = 0.32691512\n",
      "Iteration 31, loss = 0.32622851\n",
      "Iteration 32, loss = 0.32562704\n",
      "Iteration 33, loss = 0.32431707\n",
      "Iteration 34, loss = 0.32398032\n",
      "Iteration 35, loss = 0.32251399\n",
      "Iteration 36, loss = 0.32212673\n",
      "Iteration 37, loss = 0.32181738\n",
      "Iteration 38, loss = 0.32056292\n",
      "Iteration 39, loss = 0.32037274\n",
      "Iteration 40, loss = 0.31921558\n",
      "Iteration 41, loss = 0.31896108\n",
      "Iteration 42, loss = 0.31886979\n",
      "Iteration 43, loss = 0.31750553\n",
      "Iteration 44, loss = 0.31725024\n",
      "Iteration 45, loss = 0.31630884\n",
      "Iteration 46, loss = 0.31572416\n",
      "Iteration 47, loss = 0.31577332\n",
      "Iteration 48, loss = 0.31471558\n",
      "Iteration 49, loss = 0.31469463\n",
      "Iteration 50, loss = 0.31353752\n",
      "Iteration 51, loss = 0.31304909\n",
      "Iteration 52, loss = 0.31232240\n",
      "Iteration 53, loss = 0.31222531\n",
      "Iteration 54, loss = 0.31186035\n",
      "Iteration 55, loss = 0.31122473\n",
      "Iteration 56, loss = 0.31135454\n",
      "Iteration 57, loss = 0.31058147\n",
      "Iteration 58, loss = 0.30954645\n",
      "Iteration 59, loss = 0.30907358\n",
      "Iteration 60, loss = 0.30928288\n",
      "Iteration 61, loss = 0.30888890\n",
      "Iteration 62, loss = 0.30782493\n",
      "Iteration 63, loss = 0.30707093\n",
      "Iteration 64, loss = 0.30687153\n",
      "Iteration 65, loss = 0.30716064\n",
      "Iteration 66, loss = 0.30637099\n",
      "Iteration 67, loss = 0.30633196\n",
      "Iteration 68, loss = 0.30529986\n",
      "Iteration 69, loss = 0.30530162\n",
      "Iteration 70, loss = 0.30442295\n",
      "Iteration 71, loss = 0.30507074\n",
      "Iteration 72, loss = 0.30419773\n",
      "Iteration 73, loss = 0.30408489\n",
      "Iteration 74, loss = 0.30314991\n",
      "Iteration 75, loss = 0.30282555\n",
      "Iteration 76, loss = 0.30221036\n",
      "Iteration 77, loss = 0.30286357\n",
      "Iteration 78, loss = 0.30172578\n",
      "Iteration 79, loss = 0.30186639\n",
      "Iteration 80, loss = 0.30089571\n",
      "Iteration 81, loss = 0.30139607\n",
      "Iteration 82, loss = 0.30057833\n",
      "Iteration 83, loss = 0.30046500\n",
      "Iteration 84, loss = 0.30012451\n",
      "Iteration 85, loss = 0.29985427\n",
      "Iteration 86, loss = 0.29990409\n",
      "Iteration 87, loss = 0.29879303\n",
      "Iteration 88, loss = 0.29884588\n",
      "Iteration 89, loss = 0.29864777\n",
      "Iteration 90, loss = 0.29771691\n",
      "Iteration 91, loss = 0.29862116\n",
      "Iteration 92, loss = 0.29747048\n",
      "Iteration 93, loss = 0.29786832\n",
      "Iteration 94, loss = 0.29660647\n",
      "Iteration 95, loss = 0.29688844\n",
      "Iteration 96, loss = 0.29579794\n",
      "Iteration 97, loss = 0.29594011\n",
      "Iteration 98, loss = 0.29605246\n",
      "Iteration 99, loss = 0.29610429\n",
      "Iteration 100, loss = 0.29531878\n",
      "Iteration 101, loss = 0.29514094\n",
      "Iteration 102, loss = 0.29458472\n",
      "Iteration 103, loss = 0.29467138\n",
      "Iteration 104, loss = 0.29407831\n",
      "Iteration 105, loss = 0.29323835\n",
      "Iteration 106, loss = 0.29447102\n",
      "Iteration 107, loss = 0.29417469\n",
      "Iteration 108, loss = 0.29414346\n",
      "Iteration 109, loss = 0.29320441\n",
      "Iteration 110, loss = 0.29461162\n",
      "Iteration 111, loss = 0.29294505\n",
      "Iteration 112, loss = 0.29271910\n",
      "Iteration 113, loss = 0.29249299\n",
      "Iteration 114, loss = 0.29242790\n",
      "Iteration 115, loss = 0.29196869\n",
      "Iteration 116, loss = 0.29202538\n",
      "Iteration 117, loss = 0.29256416\n",
      "Iteration 118, loss = 0.29144900\n",
      "Iteration 119, loss = 0.29085702\n",
      "Iteration 120, loss = 0.29167751\n",
      "Iteration 121, loss = 0.29133981\n",
      "Iteration 122, loss = 0.29065899\n",
      "Iteration 123, loss = 0.29026502\n",
      "Iteration 124, loss = 0.29055802\n",
      "Iteration 125, loss = 0.29020147\n",
      "Iteration 126, loss = 0.28945209\n",
      "Iteration 127, loss = 0.28974295\n",
      "Iteration 128, loss = 0.28913925\n",
      "Iteration 129, loss = 0.28939159\n",
      "Iteration 130, loss = 0.28908216\n",
      "Iteration 131, loss = 0.28877402\n",
      "Iteration 132, loss = 0.28873852\n",
      "Iteration 133, loss = 0.28841978\n",
      "Iteration 134, loss = 0.28762811\n",
      "Iteration 135, loss = 0.28879583\n",
      "Iteration 136, loss = 0.28852572\n",
      "Iteration 137, loss = 0.28801994\n",
      "Iteration 138, loss = 0.28785937\n",
      "Iteration 139, loss = 0.28738059\n",
      "Iteration 140, loss = 0.28622718\n",
      "Iteration 141, loss = 0.28687136\n",
      "Iteration 142, loss = 0.28747296\n",
      "Iteration 143, loss = 0.28719033\n",
      "Iteration 144, loss = 0.28682628\n",
      "Iteration 145, loss = 0.28714348\n",
      "Iteration 146, loss = 0.28632237\n",
      "Iteration 147, loss = 0.28637144\n",
      "Iteration 148, loss = 0.28662058\n",
      "Iteration 149, loss = 0.28573765\n",
      "Iteration 150, loss = 0.28568408\n",
      "Iteration 151, loss = 0.28596310\n",
      "Iteration 152, loss = 0.28552452\n",
      "Iteration 153, loss = 0.28565059\n",
      "Iteration 154, loss = 0.28605551\n",
      "Iteration 155, loss = 0.28532087\n",
      "Iteration 156, loss = 0.28494569\n",
      "Iteration 157, loss = 0.28433743\n",
      "Iteration 158, loss = 0.28474863\n",
      "Iteration 159, loss = 0.28444583\n",
      "Iteration 160, loss = 0.28517496\n",
      "Iteration 161, loss = 0.28463980\n",
      "Iteration 162, loss = 0.28397024\n",
      "Iteration 163, loss = 0.28380109\n",
      "Iteration 164, loss = 0.28362638\n",
      "Iteration 165, loss = 0.28366784\n",
      "Iteration 166, loss = 0.28329502\n",
      "Iteration 167, loss = 0.28368139\n",
      "Iteration 168, loss = 0.28325757\n",
      "Iteration 169, loss = 0.28292934\n",
      "Iteration 170, loss = 0.28282539\n",
      "Iteration 171, loss = 0.28218493\n",
      "Iteration 172, loss = 0.28288181\n",
      "Iteration 173, loss = 0.28328695\n",
      "Iteration 174, loss = 0.28229731\n",
      "Iteration 175, loss = 0.28156743\n",
      "Iteration 176, loss = 0.28159495\n",
      "Iteration 177, loss = 0.28178104\n",
      "Iteration 178, loss = 0.28254238\n",
      "Iteration 179, loss = 0.28160369\n",
      "Iteration 180, loss = 0.28163934\n",
      "Iteration 181, loss = 0.28161502\n",
      "Iteration 182, loss = 0.28201706\n",
      "Iteration 183, loss = 0.28143401\n",
      "Iteration 184, loss = 0.28054360\n",
      "Iteration 185, loss = 0.28078122\n",
      "Iteration 186, loss = 0.28142672\n",
      "Iteration 187, loss = 0.28218276\n",
      "Iteration 188, loss = 0.28027179\n",
      "Iteration 189, loss = 0.28167528\n",
      "Iteration 190, loss = 0.27983569\n",
      "Iteration 191, loss = 0.28029389\n",
      "Iteration 192, loss = 0.27898116\n",
      "Iteration 193, loss = 0.28030422\n",
      "Iteration 194, loss = 0.27995478\n",
      "Iteration 195, loss = 0.27941553\n",
      "Iteration 196, loss = 0.28011092\n",
      "Iteration 197, loss = 0.27965597\n",
      "Iteration 198, loss = 0.27966838\n",
      "Iteration 199, loss = 0.27865789\n",
      "Iteration 200, loss = 0.27863488\n",
      "Iteration 201, loss = 0.27924709\n",
      "Iteration 202, loss = 0.27798551\n",
      "Iteration 203, loss = 0.27801615\n",
      "Iteration 204, loss = 0.27968976\n",
      "Iteration 205, loss = 0.27822046\n",
      "Iteration 206, loss = 0.27812595\n",
      "Iteration 207, loss = 0.27878718\n",
      "Iteration 208, loss = 0.27791979\n",
      "Iteration 209, loss = 0.27756431\n",
      "Iteration 210, loss = 0.27790897\n",
      "Iteration 211, loss = 0.27853502\n",
      "Iteration 212, loss = 0.27661947\n",
      "Iteration 213, loss = 0.27782038\n",
      "Iteration 214, loss = 0.27654390\n",
      "Iteration 215, loss = 0.27820533\n",
      "Iteration 216, loss = 0.27829525\n",
      "Iteration 217, loss = 0.27676436\n",
      "Iteration 218, loss = 0.27632857\n",
      "Iteration 219, loss = 0.27662093\n",
      "Iteration 220, loss = 0.27627172\n",
      "Iteration 221, loss = 0.27663638\n",
      "Iteration 222, loss = 0.27624743\n",
      "Iteration 223, loss = 0.27570781\n",
      "Iteration 224, loss = 0.27692979\n",
      "Iteration 225, loss = 0.27590419\n",
      "Iteration 226, loss = 0.27670390\n",
      "Iteration 227, loss = 0.27659310\n",
      "Iteration 228, loss = 0.27589055\n",
      "Iteration 229, loss = 0.27544106\n",
      "Iteration 230, loss = 0.27514306\n",
      "Iteration 231, loss = 0.27587521\n",
      "Iteration 232, loss = 0.27505073\n",
      "Iteration 233, loss = 0.27613645\n",
      "Iteration 234, loss = 0.27544876\n",
      "Iteration 235, loss = 0.27504140\n",
      "Iteration 236, loss = 0.27460627\n",
      "Iteration 237, loss = 0.27475317\n",
      "Iteration 238, loss = 0.27480370\n",
      "Iteration 239, loss = 0.27475455\n",
      "Iteration 240, loss = 0.27413845\n",
      "Iteration 241, loss = 0.27427985\n",
      "Iteration 242, loss = 0.27517315\n",
      "Iteration 243, loss = 0.27425060\n",
      "Iteration 244, loss = 0.27397142\n",
      "Iteration 245, loss = 0.27372253\n",
      "Iteration 246, loss = 0.27370061\n",
      "Iteration 247, loss = 0.27386726\n",
      "Iteration 248, loss = 0.27426557\n",
      "Iteration 249, loss = 0.27410491\n",
      "Iteration 250, loss = 0.27276200\n",
      "Iteration 251, loss = 0.27288396\n",
      "Iteration 252, loss = 0.27287535\n",
      "Iteration 253, loss = 0.27380792\n",
      "Iteration 254, loss = 0.27209311\n",
      "Iteration 255, loss = 0.27246541\n",
      "Iteration 256, loss = 0.27276533\n",
      "Iteration 257, loss = 0.27304582\n",
      "Iteration 258, loss = 0.27298536\n",
      "Iteration 259, loss = 0.27225815\n",
      "Iteration 260, loss = 0.27208567\n",
      "Iteration 261, loss = 0.27217691\n",
      "Iteration 262, loss = 0.27181884\n",
      "Iteration 263, loss = 0.27103002\n",
      "Iteration 264, loss = 0.27146233\n",
      "Iteration 265, loss = 0.27112123\n",
      "Iteration 266, loss = 0.27216101\n",
      "Iteration 267, loss = 0.27052016\n",
      "Iteration 268, loss = 0.27090521\n",
      "Iteration 269, loss = 0.27144191\n",
      "Iteration 270, loss = 0.27118965\n",
      "Iteration 271, loss = 0.27053532\n",
      "Iteration 272, loss = 0.27092795\n",
      "Iteration 273, loss = 0.27057616\n",
      "Iteration 274, loss = 0.27126259\n",
      "Iteration 275, loss = 0.27138994\n",
      "Iteration 276, loss = 0.26963887\n",
      "Iteration 277, loss = 0.27043906\n",
      "Iteration 278, loss = 0.26994940\n",
      "Iteration 279, loss = 0.27051053\n",
      "Iteration 280, loss = 0.26936139\n",
      "Iteration 281, loss = 0.27016689\n",
      "Iteration 282, loss = 0.26963613\n",
      "Iteration 283, loss = 0.26910039\n",
      "Iteration 284, loss = 0.27029320\n",
      "Iteration 285, loss = 0.26933570\n",
      "Iteration 286, loss = 0.26997691\n",
      "Iteration 287, loss = 0.26940911\n",
      "Iteration 288, loss = 0.26903982\n",
      "Iteration 289, loss = 0.26872530\n",
      "Iteration 290, loss = 0.26874907\n",
      "Iteration 291, loss = 0.26850791\n",
      "Iteration 292, loss = 0.26830824\n",
      "Iteration 293, loss = 0.26905891\n",
      "Iteration 294, loss = 0.26925991\n",
      "Iteration 295, loss = 0.26853660\n",
      "Iteration 296, loss = 0.26868142\n",
      "Iteration 297, loss = 0.26758799\n",
      "Iteration 298, loss = 0.26814720\n",
      "Iteration 299, loss = 0.26852457\n",
      "Iteration 300, loss = 0.26747110\n",
      "Iteration 301, loss = 0.26897409\n",
      "Iteration 302, loss = 0.26714068\n",
      "Iteration 303, loss = 0.26740377\n",
      "Iteration 304, loss = 0.26785310\n",
      "Iteration 305, loss = 0.26725129\n",
      "Iteration 306, loss = 0.26805391\n",
      "Iteration 307, loss = 0.26721010\n",
      "Iteration 308, loss = 0.26738592\n",
      "Iteration 309, loss = 0.26730707\n",
      "Iteration 310, loss = 0.26696687\n",
      "Iteration 311, loss = 0.26775282\n",
      "Iteration 312, loss = 0.26615417\n",
      "Iteration 313, loss = 0.26691130\n",
      "Iteration 314, loss = 0.26665586\n",
      "Iteration 315, loss = 0.26700274\n",
      "Iteration 316, loss = 0.26648995\n",
      "Iteration 317, loss = 0.26727347\n",
      "Iteration 318, loss = 0.26591823\n",
      "Iteration 319, loss = 0.26672151\n",
      "Iteration 320, loss = 0.26701440\n",
      "Iteration 321, loss = 0.26575921\n",
      "Iteration 322, loss = 0.26617980\n",
      "Iteration 323, loss = 0.26636476\n",
      "Iteration 324, loss = 0.26579976\n",
      "Iteration 325, loss = 0.26614886\n",
      "Iteration 326, loss = 0.26588146\n",
      "Iteration 327, loss = 0.26551554\n",
      "Iteration 328, loss = 0.26665295\n",
      "Iteration 329, loss = 0.26459810\n",
      "Iteration 330, loss = 0.26530876\n",
      "Iteration 331, loss = 0.26546677\n",
      "Iteration 332, loss = 0.26559844\n",
      "Iteration 333, loss = 0.26526982\n",
      "Iteration 334, loss = 0.26565397\n",
      "Iteration 335, loss = 0.26474470\n",
      "Iteration 336, loss = 0.26445043\n",
      "Iteration 337, loss = 0.26440214\n",
      "Iteration 338, loss = 0.26490874\n",
      "Iteration 339, loss = 0.26585731\n",
      "Iteration 340, loss = 0.26527283\n",
      "Iteration 341, loss = 0.26434554\n",
      "Iteration 342, loss = 0.26400822\n",
      "Iteration 343, loss = 0.26428530\n",
      "Iteration 344, loss = 0.26416913\n",
      "Iteration 345, loss = 0.26335978\n",
      "Iteration 346, loss = 0.26467386\n",
      "Iteration 347, loss = 0.26526537\n",
      "Iteration 348, loss = 0.26387340\n",
      "Iteration 349, loss = 0.26400067\n",
      "Iteration 350, loss = 0.26348464\n",
      "Iteration 351, loss = 0.26428291\n",
      "Iteration 352, loss = 0.26388182\n",
      "Iteration 353, loss = 0.26329501\n",
      "Iteration 354, loss = 0.26424001\n",
      "Iteration 355, loss = 0.26336101\n",
      "Iteration 356, loss = 0.26294675\n",
      "Iteration 357, loss = 0.26360094\n",
      "Iteration 358, loss = 0.26446648\n",
      "Iteration 359, loss = 0.26289141\n",
      "Iteration 360, loss = 0.26297840\n",
      "Iteration 361, loss = 0.26360688\n",
      "Iteration 362, loss = 0.26197598\n",
      "Iteration 363, loss = 0.26355247\n",
      "Iteration 364, loss = 0.26306599\n",
      "Iteration 365, loss = 0.26218965\n",
      "Iteration 366, loss = 0.26241581\n",
      "Iteration 367, loss = 0.26427600\n",
      "Iteration 368, loss = 0.26207747\n",
      "Iteration 369, loss = 0.26318752\n",
      "Iteration 370, loss = 0.26231202\n",
      "Iteration 371, loss = 0.26259132\n",
      "Iteration 372, loss = 0.26200469\n",
      "Iteration 373, loss = 0.26314360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50398969\n",
      "Iteration 2, loss = 0.45654729\n",
      "Iteration 3, loss = 0.44834212\n",
      "Iteration 4, loss = 0.44360025\n",
      "Iteration 5, loss = 0.43935354\n",
      "Iteration 6, loss = 0.43641053\n",
      "Iteration 7, loss = 0.43307398\n",
      "Iteration 8, loss = 0.43001720\n",
      "Iteration 9, loss = 0.42766436\n",
      "Iteration 10, loss = 0.42479022\n",
      "Iteration 11, loss = 0.42207280\n",
      "Iteration 12, loss = 0.41994429\n",
      "Iteration 13, loss = 0.41851242\n",
      "Iteration 14, loss = 0.41643957\n",
      "Iteration 15, loss = 0.41499916\n",
      "Iteration 16, loss = 0.41251852\n",
      "Iteration 17, loss = 0.41108285\n",
      "Iteration 18, loss = 0.40934383\n",
      "Iteration 19, loss = 0.40820148\n",
      "Iteration 20, loss = 0.40632960\n",
      "Iteration 21, loss = 0.40472444\n",
      "Iteration 22, loss = 0.40399758\n",
      "Iteration 23, loss = 0.40247760\n",
      "Iteration 24, loss = 0.40147082\n",
      "Iteration 25, loss = 0.40038296\n",
      "Iteration 26, loss = 0.39893839\n",
      "Iteration 27, loss = 0.39822926\n",
      "Iteration 28, loss = 0.39671983\n",
      "Iteration 29, loss = 0.39564762\n",
      "Iteration 30, loss = 0.39467073\n",
      "Iteration 31, loss = 0.39397273\n",
      "Iteration 32, loss = 0.39222464\n",
      "Iteration 33, loss = 0.39175191\n",
      "Iteration 34, loss = 0.39032701\n",
      "Iteration 35, loss = 0.38964359\n",
      "Iteration 36, loss = 0.38909663\n",
      "Iteration 37, loss = 0.38835098\n",
      "Iteration 38, loss = 0.38700270\n",
      "Iteration 39, loss = 0.38561651\n",
      "Iteration 40, loss = 0.38558580\n",
      "Iteration 41, loss = 0.38450101\n",
      "Iteration 42, loss = 0.38366897\n",
      "Iteration 43, loss = 0.38306325\n",
      "Iteration 44, loss = 0.38222225\n",
      "Iteration 45, loss = 0.38120443\n",
      "Iteration 46, loss = 0.38075442\n",
      "Iteration 47, loss = 0.38065073\n",
      "Iteration 48, loss = 0.37995284\n",
      "Iteration 49, loss = 0.37837782\n",
      "Iteration 50, loss = 0.37832088\n",
      "Iteration 51, loss = 0.37784206\n",
      "Iteration 52, loss = 0.37664618\n",
      "Iteration 53, loss = 0.37668429\n",
      "Iteration 54, loss = 0.37540933\n",
      "Iteration 55, loss = 0.37654830\n",
      "Iteration 56, loss = 0.37439222\n",
      "Iteration 57, loss = 0.37394042\n",
      "Iteration 58, loss = 0.37401874\n",
      "Iteration 59, loss = 0.37264790\n",
      "Iteration 60, loss = 0.37272153\n",
      "Iteration 61, loss = 0.37172690\n",
      "Iteration 62, loss = 0.37145293\n",
      "Iteration 63, loss = 0.37137072\n",
      "Iteration 64, loss = 0.37045117\n",
      "Iteration 65, loss = 0.36970837\n",
      "Iteration 66, loss = 0.36907072\n",
      "Iteration 67, loss = 0.36920732\n",
      "Iteration 68, loss = 0.36878811\n",
      "Iteration 69, loss = 0.36750350\n",
      "Iteration 70, loss = 0.36706371\n",
      "Iteration 71, loss = 0.36710692\n",
      "Iteration 72, loss = 0.36700618\n",
      "Iteration 73, loss = 0.36576602\n",
      "Iteration 74, loss = 0.36581291\n",
      "Iteration 75, loss = 0.36415598\n",
      "Iteration 76, loss = 0.36464020\n",
      "Iteration 77, loss = 0.36399347\n",
      "Iteration 78, loss = 0.36296492\n",
      "Iteration 79, loss = 0.36403772\n",
      "Iteration 80, loss = 0.36273034\n",
      "Iteration 81, loss = 0.36292833\n",
      "Iteration 82, loss = 0.36145897\n",
      "Iteration 83, loss = 0.36147625\n",
      "Iteration 84, loss = 0.36086424\n",
      "Iteration 85, loss = 0.36122555\n",
      "Iteration 86, loss = 0.36016882\n",
      "Iteration 87, loss = 0.35986666\n",
      "Iteration 88, loss = 0.35921890\n",
      "Iteration 89, loss = 0.35955395\n",
      "Iteration 90, loss = 0.35897505\n",
      "Iteration 91, loss = 0.35826421\n",
      "Iteration 92, loss = 0.35736006\n",
      "Iteration 93, loss = 0.35803407\n",
      "Iteration 94, loss = 0.35683481\n",
      "Iteration 95, loss = 0.35643564\n",
      "Iteration 96, loss = 0.35746345\n",
      "Iteration 97, loss = 0.35608566\n",
      "Iteration 98, loss = 0.35585236\n",
      "Iteration 99, loss = 0.35547512\n",
      "Iteration 100, loss = 0.35507649\n",
      "Iteration 101, loss = 0.35477259\n",
      "Iteration 102, loss = 0.35396563\n",
      "Iteration 103, loss = 0.35488786\n",
      "Iteration 104, loss = 0.35430228\n",
      "Iteration 105, loss = 0.35286725\n",
      "Iteration 106, loss = 0.35292833\n",
      "Iteration 107, loss = 0.35310900\n",
      "Iteration 108, loss = 0.35255602\n",
      "Iteration 109, loss = 0.35196620\n",
      "Iteration 110, loss = 0.35170404\n",
      "Iteration 111, loss = 0.35143761\n",
      "Iteration 112, loss = 0.35169477\n",
      "Iteration 113, loss = 0.35033486\n",
      "Iteration 114, loss = 0.35118332\n",
      "Iteration 115, loss = 0.35068351\n",
      "Iteration 116, loss = 0.34988778\n",
      "Iteration 117, loss = 0.34956656\n",
      "Iteration 118, loss = 0.34890791\n",
      "Iteration 119, loss = 0.35012527\n",
      "Iteration 120, loss = 0.34891315\n",
      "Iteration 121, loss = 0.34875060\n",
      "Iteration 122, loss = 0.34872751\n",
      "Iteration 123, loss = 0.34851434\n",
      "Iteration 124, loss = 0.34758094\n",
      "Iteration 125, loss = 0.34675222\n",
      "Iteration 126, loss = 0.34698325\n",
      "Iteration 127, loss = 0.34689807\n",
      "Iteration 128, loss = 0.34634615\n",
      "Iteration 129, loss = 0.34683930\n",
      "Iteration 130, loss = 0.34598460\n",
      "Iteration 131, loss = 0.34701884\n",
      "Iteration 132, loss = 0.34499756\n",
      "Iteration 133, loss = 0.34562031\n",
      "Iteration 134, loss = 0.34565953\n",
      "Iteration 135, loss = 0.34466688\n",
      "Iteration 136, loss = 0.34441990\n",
      "Iteration 137, loss = 0.34591483\n",
      "Iteration 138, loss = 0.34383337\n",
      "Iteration 139, loss = 0.34393029\n",
      "Iteration 140, loss = 0.34362258\n",
      "Iteration 141, loss = 0.34339335\n",
      "Iteration 142, loss = 0.34286538\n",
      "Iteration 143, loss = 0.34397862\n",
      "Iteration 144, loss = 0.34234289\n",
      "Iteration 145, loss = 0.34332991\n",
      "Iteration 146, loss = 0.34154962\n",
      "Iteration 147, loss = 0.34226377\n",
      "Iteration 148, loss = 0.34224188\n",
      "Iteration 149, loss = 0.34168661\n",
      "Iteration 150, loss = 0.34090519\n",
      "Iteration 151, loss = 0.34040930\n",
      "Iteration 152, loss = 0.34093805\n",
      "Iteration 153, loss = 0.34076629\n",
      "Iteration 154, loss = 0.34087987\n",
      "Iteration 155, loss = 0.34045150\n",
      "Iteration 156, loss = 0.34142448\n",
      "Iteration 157, loss = 0.33927037\n",
      "Iteration 158, loss = 0.33974912\n",
      "Iteration 159, loss = 0.33997130\n",
      "Iteration 160, loss = 0.33908005\n",
      "Iteration 161, loss = 0.33939226\n",
      "Iteration 162, loss = 0.33883408\n",
      "Iteration 163, loss = 0.33921471\n",
      "Iteration 164, loss = 0.33840627\n",
      "Iteration 165, loss = 0.33857222\n",
      "Iteration 166, loss = 0.33776315\n",
      "Iteration 167, loss = 0.33737171\n",
      "Iteration 168, loss = 0.33889296\n",
      "Iteration 169, loss = 0.33726048\n",
      "Iteration 170, loss = 0.33716071\n",
      "Iteration 171, loss = 0.33726786\n",
      "Iteration 172, loss = 0.33681108\n",
      "Iteration 173, loss = 0.33684251\n",
      "Iteration 174, loss = 0.33644942\n",
      "Iteration 175, loss = 0.33637884\n",
      "Iteration 176, loss = 0.33733171\n",
      "Iteration 177, loss = 0.33566055\n",
      "Iteration 178, loss = 0.33645548\n",
      "Iteration 179, loss = 0.33611820\n",
      "Iteration 180, loss = 0.33663464\n",
      "Iteration 181, loss = 0.33512187\n",
      "Iteration 182, loss = 0.33658693\n",
      "Iteration 183, loss = 0.33624513\n",
      "Iteration 184, loss = 0.33494511\n",
      "Iteration 185, loss = 0.33488200\n",
      "Iteration 186, loss = 0.33466873\n",
      "Iteration 187, loss = 0.33513041\n",
      "Iteration 188, loss = 0.33370059\n",
      "Iteration 189, loss = 0.33487058\n",
      "Iteration 190, loss = 0.33398940\n",
      "Iteration 191, loss = 0.33411088\n",
      "Iteration 192, loss = 0.33398348\n",
      "Iteration 193, loss = 0.33416315\n",
      "Iteration 194, loss = 0.33507139\n",
      "Iteration 195, loss = 0.33326971\n",
      "Iteration 196, loss = 0.33377979\n",
      "Iteration 197, loss = 0.33443250\n",
      "Iteration 198, loss = 0.33315495\n",
      "Iteration 199, loss = 0.33310311\n",
      "Iteration 200, loss = 0.33323498\n",
      "Iteration 201, loss = 0.33285747\n",
      "Iteration 202, loss = 0.33294119\n",
      "Iteration 203, loss = 0.33230477\n",
      "Iteration 204, loss = 0.33163159\n",
      "Iteration 205, loss = 0.33212258\n",
      "Iteration 206, loss = 0.33230140\n",
      "Iteration 207, loss = 0.33275849\n",
      "Iteration 208, loss = 0.33142817\n",
      "Iteration 209, loss = 0.33242936\n",
      "Iteration 210, loss = 0.33128282\n",
      "Iteration 211, loss = 0.33088775\n",
      "Iteration 212, loss = 0.33112284\n",
      "Iteration 213, loss = 0.33182678\n",
      "Iteration 214, loss = 0.33100317\n",
      "Iteration 215, loss = 0.33152400\n",
      "Iteration 216, loss = 0.33080354\n",
      "Iteration 217, loss = 0.33087491\n",
      "Iteration 218, loss = 0.33232829\n",
      "Iteration 219, loss = 0.32982937\n",
      "Iteration 220, loss = 0.32997906\n",
      "Iteration 221, loss = 0.32984903\n",
      "Iteration 222, loss = 0.32977193\n",
      "Iteration 223, loss = 0.33011970\n",
      "Iteration 224, loss = 0.32968110\n",
      "Iteration 225, loss = 0.32967150\n",
      "Iteration 226, loss = 0.33052287\n",
      "Iteration 227, loss = 0.32908708\n",
      "Iteration 228, loss = 0.32896577\n",
      "Iteration 229, loss = 0.32871638\n",
      "Iteration 230, loss = 0.32931588\n",
      "Iteration 231, loss = 0.32858469\n",
      "Iteration 232, loss = 0.32928999\n",
      "Iteration 233, loss = 0.32941406\n",
      "Iteration 234, loss = 0.32763924\n",
      "Iteration 235, loss = 0.32883472\n",
      "Iteration 236, loss = 0.32810940\n",
      "Iteration 237, loss = 0.32822066\n",
      "Iteration 238, loss = 0.32869336\n",
      "Iteration 239, loss = 0.32823999\n",
      "Iteration 240, loss = 0.32751012\n",
      "Iteration 241, loss = 0.32796624\n",
      "Iteration 242, loss = 0.32767594\n",
      "Iteration 243, loss = 0.32737149\n",
      "Iteration 244, loss = 0.32750840\n",
      "Iteration 245, loss = 0.32779364\n",
      "Iteration 246, loss = 0.32678844\n",
      "Iteration 247, loss = 0.32646415\n",
      "Iteration 248, loss = 0.32743102\n",
      "Iteration 249, loss = 0.32646325\n",
      "Iteration 250, loss = 0.32674019\n",
      "Iteration 251, loss = 0.32647265\n",
      "Iteration 252, loss = 0.32783272\n",
      "Iteration 253, loss = 0.32538762\n",
      "Iteration 254, loss = 0.32706983\n",
      "Iteration 255, loss = 0.32564275\n",
      "Iteration 256, loss = 0.32666215\n",
      "Iteration 257, loss = 0.32603333\n",
      "Iteration 258, loss = 0.32532203\n",
      "Iteration 259, loss = 0.32656686\n",
      "Iteration 260, loss = 0.32671246\n",
      "Iteration 261, loss = 0.32560302\n",
      "Iteration 262, loss = 0.32546462\n",
      "Iteration 263, loss = 0.32561677\n",
      "Iteration 264, loss = 0.32601309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49101751\n",
      "Iteration 2, loss = 0.42265461\n",
      "Iteration 3, loss = 0.40555571\n",
      "Iteration 4, loss = 0.39496465\n",
      "Iteration 5, loss = 0.38597427\n",
      "Iteration 6, loss = 0.37959298\n",
      "Iteration 7, loss = 0.37418809\n",
      "Iteration 8, loss = 0.36914330\n",
      "Iteration 9, loss = 0.36442831\n",
      "Iteration 10, loss = 0.36050672\n",
      "Iteration 11, loss = 0.35616744\n",
      "Iteration 12, loss = 0.35286367\n",
      "Iteration 13, loss = 0.34967817\n",
      "Iteration 14, loss = 0.34750656\n",
      "Iteration 15, loss = 0.34361093\n",
      "Iteration 16, loss = 0.33959093\n",
      "Iteration 17, loss = 0.33733885\n",
      "Iteration 18, loss = 0.33459714\n",
      "Iteration 19, loss = 0.33215559\n",
      "Iteration 20, loss = 0.33014153\n",
      "Iteration 21, loss = 0.32679134\n",
      "Iteration 22, loss = 0.32458055\n",
      "Iteration 23, loss = 0.32268551\n",
      "Iteration 24, loss = 0.32035265\n",
      "Iteration 25, loss = 0.31903896\n",
      "Iteration 26, loss = 0.31684792\n",
      "Iteration 27, loss = 0.31409740\n",
      "Iteration 28, loss = 0.31202053\n",
      "Iteration 29, loss = 0.31076999\n",
      "Iteration 30, loss = 0.30928705\n",
      "Iteration 31, loss = 0.30761928\n",
      "Iteration 32, loss = 0.30661608\n",
      "Iteration 33, loss = 0.30491332\n",
      "Iteration 34, loss = 0.30298152\n",
      "Iteration 35, loss = 0.30166197\n",
      "Iteration 36, loss = 0.30069696\n",
      "Iteration 37, loss = 0.30034681\n",
      "Iteration 38, loss = 0.29848132\n",
      "Iteration 39, loss = 0.29680221\n",
      "Iteration 40, loss = 0.29606929\n",
      "Iteration 41, loss = 0.29415121\n",
      "Iteration 42, loss = 0.29272021\n",
      "Iteration 43, loss = 0.29281766\n",
      "Iteration 44, loss = 0.29062318\n",
      "Iteration 45, loss = 0.28964944\n",
      "Iteration 46, loss = 0.28856648\n",
      "Iteration 47, loss = 0.28675810\n",
      "Iteration 48, loss = 0.28556223\n",
      "Iteration 49, loss = 0.28502305\n",
      "Iteration 50, loss = 0.28385604\n",
      "Iteration 51, loss = 0.28372918\n",
      "Iteration 52, loss = 0.28216774\n",
      "Iteration 53, loss = 0.28179787\n",
      "Iteration 54, loss = 0.28209310\n",
      "Iteration 55, loss = 0.28005088\n",
      "Iteration 56, loss = 0.28059531\n",
      "Iteration 57, loss = 0.27774148\n",
      "Iteration 58, loss = 0.27823003\n",
      "Iteration 59, loss = 0.27687423\n",
      "Iteration 60, loss = 0.27598360\n",
      "Iteration 61, loss = 0.27575785\n",
      "Iteration 62, loss = 0.27441865\n",
      "Iteration 63, loss = 0.27404094\n",
      "Iteration 64, loss = 0.27321326\n",
      "Iteration 65, loss = 0.27264975\n",
      "Iteration 66, loss = 0.27206707\n",
      "Iteration 67, loss = 0.27195928\n",
      "Iteration 68, loss = 0.27077939\n",
      "Iteration 69, loss = 0.27006313\n",
      "Iteration 70, loss = 0.26872350\n",
      "Iteration 71, loss = 0.26882672\n",
      "Iteration 72, loss = 0.26825427\n",
      "Iteration 73, loss = 0.26702546\n",
      "Iteration 74, loss = 0.26621074\n",
      "Iteration 75, loss = 0.26615965\n",
      "Iteration 76, loss = 0.26613763\n",
      "Iteration 77, loss = 0.26504649\n",
      "Iteration 78, loss = 0.26501744\n",
      "Iteration 79, loss = 0.26410777\n",
      "Iteration 80, loss = 0.26318160\n",
      "Iteration 81, loss = 0.26362523\n",
      "Iteration 82, loss = 0.26177098\n",
      "Iteration 83, loss = 0.26209652\n",
      "Iteration 84, loss = 0.26175962\n",
      "Iteration 85, loss = 0.26176997\n",
      "Iteration 86, loss = 0.26008242\n",
      "Iteration 87, loss = 0.25880547\n",
      "Iteration 88, loss = 0.25925045\n",
      "Iteration 89, loss = 0.25864738\n",
      "Iteration 90, loss = 0.25803496\n",
      "Iteration 91, loss = 0.25733996\n",
      "Iteration 92, loss = 0.25649546\n",
      "Iteration 93, loss = 0.25588133\n",
      "Iteration 94, loss = 0.25542141\n",
      "Iteration 95, loss = 0.25608885\n",
      "Iteration 96, loss = 0.25459102\n",
      "Iteration 97, loss = 0.25557988\n",
      "Iteration 98, loss = 0.25424188\n",
      "Iteration 99, loss = 0.25522405\n",
      "Iteration 100, loss = 0.25332892\n",
      "Iteration 101, loss = 0.25297774\n",
      "Iteration 102, loss = 0.25296790\n",
      "Iteration 103, loss = 0.25256441\n",
      "Iteration 104, loss = 0.25221021\n",
      "Iteration 105, loss = 0.25227866\n",
      "Iteration 106, loss = 0.25198552\n",
      "Iteration 107, loss = 0.25115036\n",
      "Iteration 108, loss = 0.25074171\n",
      "Iteration 109, loss = 0.25129273\n",
      "Iteration 110, loss = 0.24989629\n",
      "Iteration 111, loss = 0.24931053\n",
      "Iteration 112, loss = 0.24969640\n",
      "Iteration 113, loss = 0.24894940\n",
      "Iteration 114, loss = 0.24953094\n",
      "Iteration 115, loss = 0.24785014\n",
      "Iteration 116, loss = 0.24702358\n",
      "Iteration 117, loss = 0.24769901\n",
      "Iteration 118, loss = 0.24623753\n",
      "Iteration 119, loss = 0.24654653\n",
      "Iteration 120, loss = 0.24622223\n",
      "Iteration 121, loss = 0.24550311\n",
      "Iteration 122, loss = 0.24628165\n",
      "Iteration 123, loss = 0.24475229\n",
      "Iteration 124, loss = 0.24511992\n",
      "Iteration 125, loss = 0.24422214\n",
      "Iteration 126, loss = 0.24501851\n",
      "Iteration 127, loss = 0.24323535\n",
      "Iteration 128, loss = 0.24381276\n",
      "Iteration 129, loss = 0.24315405\n",
      "Iteration 130, loss = 0.24335857\n",
      "Iteration 131, loss = 0.24191079\n",
      "Iteration 132, loss = 0.24147574\n",
      "Iteration 133, loss = 0.24100740\n",
      "Iteration 134, loss = 0.24242293\n",
      "Iteration 135, loss = 0.24093717\n",
      "Iteration 136, loss = 0.24183068\n",
      "Iteration 137, loss = 0.24078059\n",
      "Iteration 138, loss = 0.23958187\n",
      "Iteration 139, loss = 0.23967686\n",
      "Iteration 140, loss = 0.23990785\n",
      "Iteration 141, loss = 0.23940164\n",
      "Iteration 142, loss = 0.24005490\n",
      "Iteration 143, loss = 0.23876668\n",
      "Iteration 144, loss = 0.23777273\n",
      "Iteration 145, loss = 0.23875982\n",
      "Iteration 146, loss = 0.23901955\n",
      "Iteration 147, loss = 0.23874515\n",
      "Iteration 148, loss = 0.23763659\n",
      "Iteration 149, loss = 0.23673879\n",
      "Iteration 150, loss = 0.23656014\n",
      "Iteration 151, loss = 0.23685656\n",
      "Iteration 152, loss = 0.23681491\n",
      "Iteration 153, loss = 0.23635216\n",
      "Iteration 154, loss = 0.23645554\n",
      "Iteration 155, loss = 0.23559827\n",
      "Iteration 156, loss = 0.23618720\n",
      "Iteration 157, loss = 0.23521954\n",
      "Iteration 158, loss = 0.23478720\n",
      "Iteration 159, loss = 0.23452981\n",
      "Iteration 160, loss = 0.23433086\n",
      "Iteration 161, loss = 0.23513223\n",
      "Iteration 162, loss = 0.23368804\n",
      "Iteration 163, loss = 0.23283943\n",
      "Iteration 164, loss = 0.23422974\n",
      "Iteration 165, loss = 0.23323883\n",
      "Iteration 166, loss = 0.23378010\n",
      "Iteration 167, loss = 0.23146150\n",
      "Iteration 168, loss = 0.23165509\n",
      "Iteration 169, loss = 0.23134803\n",
      "Iteration 170, loss = 0.23207910\n",
      "Iteration 171, loss = 0.23116775\n",
      "Iteration 172, loss = 0.23178273\n",
      "Iteration 173, loss = 0.23026283\n",
      "Iteration 174, loss = 0.23071873\n",
      "Iteration 175, loss = 0.23089570\n",
      "Iteration 176, loss = 0.22925326\n",
      "Iteration 177, loss = 0.22996583\n",
      "Iteration 178, loss = 0.22940972\n",
      "Iteration 179, loss = 0.22863116\n",
      "Iteration 180, loss = 0.22789855\n",
      "Iteration 181, loss = 0.22916104\n",
      "Iteration 182, loss = 0.22855509\n",
      "Iteration 183, loss = 0.22758162\n",
      "Iteration 184, loss = 0.22835246\n",
      "Iteration 185, loss = 0.22783114\n",
      "Iteration 186, loss = 0.22808495\n",
      "Iteration 187, loss = 0.22698995\n",
      "Iteration 188, loss = 0.22772879\n",
      "Iteration 189, loss = 0.22754388\n",
      "Iteration 190, loss = 0.22658183\n",
      "Iteration 191, loss = 0.22744686\n",
      "Iteration 192, loss = 0.22602244\n",
      "Iteration 193, loss = 0.22583589\n",
      "Iteration 194, loss = 0.22716536\n",
      "Iteration 195, loss = 0.22597474\n",
      "Iteration 196, loss = 0.22570022\n",
      "Iteration 197, loss = 0.22581405\n",
      "Iteration 198, loss = 0.22530973\n",
      "Iteration 199, loss = 0.22385036\n",
      "Iteration 200, loss = 0.22386401\n",
      "Iteration 201, loss = 0.22625674\n",
      "Iteration 202, loss = 0.22449318\n",
      "Iteration 203, loss = 0.22433631\n",
      "Iteration 204, loss = 0.22424993\n",
      "Iteration 205, loss = 0.22395934\n",
      "Iteration 206, loss = 0.22362122\n",
      "Iteration 207, loss = 0.22264698\n",
      "Iteration 208, loss = 0.22275953\n",
      "Iteration 209, loss = 0.22226380\n",
      "Iteration 210, loss = 0.22319646\n",
      "Iteration 211, loss = 0.22226877\n",
      "Iteration 212, loss = 0.22279123\n",
      "Iteration 213, loss = 0.22237550\n",
      "Iteration 214, loss = 0.22151992\n",
      "Iteration 215, loss = 0.22220437\n",
      "Iteration 216, loss = 0.22182991\n",
      "Iteration 217, loss = 0.22306085\n",
      "Iteration 218, loss = 0.22172332\n",
      "Iteration 219, loss = 0.22063632\n",
      "Iteration 220, loss = 0.22318951\n",
      "Iteration 221, loss = 0.22078262\n",
      "Iteration 222, loss = 0.22088724\n",
      "Iteration 223, loss = 0.22112436\n",
      "Iteration 224, loss = 0.22137808\n",
      "Iteration 225, loss = 0.22026243\n",
      "Iteration 226, loss = 0.21963995\n",
      "Iteration 227, loss = 0.21878498\n",
      "Iteration 228, loss = 0.21993388\n",
      "Iteration 229, loss = 0.21893783\n",
      "Iteration 230, loss = 0.21972711\n",
      "Iteration 231, loss = 0.21759946\n",
      "Iteration 232, loss = 0.21870097\n",
      "Iteration 233, loss = 0.21884650\n",
      "Iteration 234, loss = 0.21807756\n",
      "Iteration 235, loss = 0.21876257\n",
      "Iteration 236, loss = 0.21755276\n",
      "Iteration 237, loss = 0.21888189\n",
      "Iteration 238, loss = 0.21751962\n",
      "Iteration 239, loss = 0.21754533\n",
      "Iteration 240, loss = 0.21697329\n",
      "Iteration 241, loss = 0.21626903\n",
      "Iteration 242, loss = 0.21760853\n",
      "Iteration 243, loss = 0.21681336\n",
      "Iteration 244, loss = 0.21691366\n",
      "Iteration 245, loss = 0.21682170\n",
      "Iteration 246, loss = 0.21648952\n",
      "Iteration 247, loss = 0.21614155\n",
      "Iteration 248, loss = 0.21506027\n",
      "Iteration 249, loss = 0.21621532\n",
      "Iteration 250, loss = 0.21538788\n",
      "Iteration 251, loss = 0.21684940\n",
      "Iteration 252, loss = 0.21561952\n",
      "Iteration 253, loss = 0.21532493\n",
      "Iteration 254, loss = 0.21558128\n",
      "Iteration 255, loss = 0.21502219\n",
      "Iteration 256, loss = 0.21503172\n",
      "Iteration 257, loss = 0.21422020\n",
      "Iteration 258, loss = 0.21420002\n",
      "Iteration 259, loss = 0.21452575\n",
      "Iteration 260, loss = 0.21545006\n",
      "Iteration 261, loss = 0.21410299\n",
      "Iteration 262, loss = 0.21326708\n",
      "Iteration 263, loss = 0.21355032\n",
      "Iteration 264, loss = 0.21312525\n",
      "Iteration 265, loss = 0.21403008\n",
      "Iteration 266, loss = 0.21471238\n",
      "Iteration 267, loss = 0.21260483\n",
      "Iteration 268, loss = 0.21331855\n",
      "Iteration 269, loss = 0.21341009\n",
      "Iteration 270, loss = 0.21298247\n",
      "Iteration 271, loss = 0.21240597\n",
      "Iteration 272, loss = 0.21233299\n",
      "Iteration 273, loss = 0.21203792\n",
      "Iteration 274, loss = 0.21215953\n",
      "Iteration 275, loss = 0.21318483\n",
      "Iteration 276, loss = 0.21239754\n",
      "Iteration 277, loss = 0.21140881\n",
      "Iteration 278, loss = 0.21330309\n",
      "Iteration 279, loss = 0.21160633\n",
      "Iteration 280, loss = 0.21166351\n",
      "Iteration 281, loss = 0.21185970\n",
      "Iteration 282, loss = 0.21102868\n",
      "Iteration 283, loss = 0.21056198\n",
      "Iteration 284, loss = 0.21070710\n",
      "Iteration 285, loss = 0.21052982\n",
      "Iteration 286, loss = 0.21065159\n",
      "Iteration 287, loss = 0.21095721\n",
      "Iteration 288, loss = 0.21024794\n",
      "Iteration 289, loss = 0.21046368\n",
      "Iteration 290, loss = 0.21097486\n",
      "Iteration 291, loss = 0.21031050\n",
      "Iteration 292, loss = 0.21011574\n",
      "Iteration 293, loss = 0.21065725\n",
      "Iteration 294, loss = 0.20969104\n",
      "Iteration 295, loss = 0.20977381\n",
      "Iteration 296, loss = 0.21137339\n",
      "Iteration 297, loss = 0.20870474\n",
      "Iteration 298, loss = 0.21038666\n",
      "Iteration 299, loss = 0.21028085\n",
      "Iteration 300, loss = 0.20920098\n",
      "Iteration 301, loss = 0.21008336\n",
      "Iteration 302, loss = 0.20861314\n",
      "Iteration 303, loss = 0.20833538\n",
      "Iteration 304, loss = 0.20902333\n",
      "Iteration 305, loss = 0.20873202\n",
      "Iteration 306, loss = 0.20945686\n",
      "Iteration 307, loss = 0.20860573\n",
      "Iteration 308, loss = 0.20842812\n",
      "Iteration 309, loss = 0.20938304\n",
      "Iteration 310, loss = 0.20772475\n",
      "Iteration 311, loss = 0.20730776\n",
      "Iteration 312, loss = 0.20890898\n",
      "Iteration 313, loss = 0.20729163\n",
      "Iteration 314, loss = 0.20869328\n",
      "Iteration 315, loss = 0.20787524\n",
      "Iteration 316, loss = 0.20828555\n",
      "Iteration 317, loss = 0.20968933\n",
      "Iteration 318, loss = 0.20765420\n",
      "Iteration 319, loss = 0.20700801\n",
      "Iteration 320, loss = 0.20725595\n",
      "Iteration 321, loss = 0.20796700\n",
      "Iteration 322, loss = 0.20624460\n",
      "Iteration 323, loss = 0.20815667\n",
      "Iteration 324, loss = 0.20588005\n",
      "Iteration 325, loss = 0.20594723\n",
      "Iteration 326, loss = 0.20748483\n",
      "Iteration 327, loss = 0.20655501\n",
      "Iteration 328, loss = 0.20592290\n",
      "Iteration 329, loss = 0.20727843\n",
      "Iteration 330, loss = 0.20585851\n",
      "Iteration 331, loss = 0.20605364\n",
      "Iteration 332, loss = 0.20559770\n",
      "Iteration 333, loss = 0.20705628\n",
      "Iteration 334, loss = 0.20593995\n",
      "Iteration 335, loss = 0.20664151\n",
      "Iteration 336, loss = 0.20488282\n",
      "Iteration 337, loss = 0.20679719\n",
      "Iteration 338, loss = 0.20428848\n",
      "Iteration 339, loss = 0.20589875\n",
      "Iteration 340, loss = 0.20501998\n",
      "Iteration 341, loss = 0.20604708\n",
      "Iteration 342, loss = 0.20636186\n",
      "Iteration 343, loss = 0.20429963\n",
      "Iteration 344, loss = 0.20569560\n",
      "Iteration 345, loss = 0.20427657\n",
      "Iteration 346, loss = 0.20420585\n",
      "Iteration 347, loss = 0.20341170\n",
      "Iteration 348, loss = 0.20620747\n",
      "Iteration 349, loss = 0.20442970\n",
      "Iteration 350, loss = 0.20362937\n",
      "Iteration 351, loss = 0.20447851\n",
      "Iteration 352, loss = 0.20509078\n",
      "Iteration 353, loss = 0.20398448\n",
      "Iteration 354, loss = 0.20422324\n",
      "Iteration 355, loss = 0.20388812\n",
      "Iteration 356, loss = 0.20405317\n",
      "Iteration 357, loss = 0.20450913\n",
      "Iteration 358, loss = 0.20352435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.37310148\n",
      "Iteration 2, loss = 0.31321573\n",
      "Iteration 3, loss = 0.30664267\n",
      "Iteration 4, loss = 0.30217767\n",
      "Iteration 5, loss = 0.29870640\n",
      "Iteration 6, loss = 0.29600400\n",
      "Iteration 7, loss = 0.29326519\n",
      "Iteration 8, loss = 0.29064711\n",
      "Iteration 9, loss = 0.28854718\n",
      "Iteration 10, loss = 0.28588758\n",
      "Iteration 11, loss = 0.28400702\n",
      "Iteration 12, loss = 0.28196070\n",
      "Iteration 13, loss = 0.28022215\n",
      "Iteration 14, loss = 0.27864337\n",
      "Iteration 15, loss = 0.27691839\n",
      "Iteration 16, loss = 0.27562224\n",
      "Iteration 17, loss = 0.27384645\n",
      "Iteration 18, loss = 0.27312231\n",
      "Iteration 19, loss = 0.27231725\n",
      "Iteration 20, loss = 0.27060481\n",
      "Iteration 21, loss = 0.26938892\n",
      "Iteration 22, loss = 0.26848509\n",
      "Iteration 23, loss = 0.26736467\n",
      "Iteration 24, loss = 0.26730963\n",
      "Iteration 25, loss = 0.26549157\n",
      "Iteration 26, loss = 0.26489344\n",
      "Iteration 27, loss = 0.26389370\n",
      "Iteration 28, loss = 0.26298550\n",
      "Iteration 29, loss = 0.26288804\n",
      "Iteration 30, loss = 0.26141767\n",
      "Iteration 31, loss = 0.26151679\n",
      "Iteration 32, loss = 0.26042816\n",
      "Iteration 33, loss = 0.25985848\n",
      "Iteration 34, loss = 0.25898850\n",
      "Iteration 35, loss = 0.25829096\n",
      "Iteration 36, loss = 0.25791655\n",
      "Iteration 37, loss = 0.25713564\n",
      "Iteration 38, loss = 0.25624233\n",
      "Iteration 39, loss = 0.25574560\n",
      "Iteration 40, loss = 0.25537232\n",
      "Iteration 41, loss = 0.25454182\n",
      "Iteration 42, loss = 0.25346413\n",
      "Iteration 43, loss = 0.25342078\n",
      "Iteration 44, loss = 0.25215753\n",
      "Iteration 45, loss = 0.25182150\n",
      "Iteration 46, loss = 0.25105764\n",
      "Iteration 47, loss = 0.25103295\n",
      "Iteration 48, loss = 0.25021493\n",
      "Iteration 49, loss = 0.24952836\n",
      "Iteration 50, loss = 0.24839403\n",
      "Iteration 51, loss = 0.24775350\n",
      "Iteration 52, loss = 0.24690347\n",
      "Iteration 53, loss = 0.24661944\n",
      "Iteration 54, loss = 0.24662093\n",
      "Iteration 55, loss = 0.24519253\n",
      "Iteration 56, loss = 0.24476488\n",
      "Iteration 57, loss = 0.24427093\n",
      "Iteration 58, loss = 0.24383095\n",
      "Iteration 59, loss = 0.24281045\n",
      "Iteration 60, loss = 0.24212265\n",
      "Iteration 61, loss = 0.24170823\n",
      "Iteration 62, loss = 0.24121302\n",
      "Iteration 63, loss = 0.24136048\n",
      "Iteration 64, loss = 0.24013087\n",
      "Iteration 65, loss = 0.23955077\n",
      "Iteration 66, loss = 0.23921049\n",
      "Iteration 67, loss = 0.23824148\n",
      "Iteration 68, loss = 0.23819589\n",
      "Iteration 69, loss = 0.23791216\n",
      "Iteration 70, loss = 0.23664740\n",
      "Iteration 71, loss = 0.23633494\n",
      "Iteration 72, loss = 0.23603042\n",
      "Iteration 73, loss = 0.23629962\n",
      "Iteration 74, loss = 0.23600953\n",
      "Iteration 75, loss = 0.23467321\n",
      "Iteration 76, loss = 0.23476986\n",
      "Iteration 77, loss = 0.23380147\n",
      "Iteration 78, loss = 0.23375172\n",
      "Iteration 79, loss = 0.23284845\n",
      "Iteration 80, loss = 0.23346602\n",
      "Iteration 81, loss = 0.23239080\n",
      "Iteration 82, loss = 0.23203452\n",
      "Iteration 83, loss = 0.23195756\n",
      "Iteration 84, loss = 0.23141595\n",
      "Iteration 85, loss = 0.23073000\n",
      "Iteration 86, loss = 0.23073728\n",
      "Iteration 87, loss = 0.23002538\n",
      "Iteration 88, loss = 0.23083138\n",
      "Iteration 89, loss = 0.22942983\n",
      "Iteration 90, loss = 0.22911812\n",
      "Iteration 91, loss = 0.22896027\n",
      "Iteration 92, loss = 0.22872341\n",
      "Iteration 93, loss = 0.22843636\n",
      "Iteration 94, loss = 0.22760117\n",
      "Iteration 95, loss = 0.22820396\n",
      "Iteration 96, loss = 0.22704665\n",
      "Iteration 97, loss = 0.22717278\n",
      "Iteration 98, loss = 0.22654350\n",
      "Iteration 99, loss = 0.22615836\n",
      "Iteration 100, loss = 0.22689415\n",
      "Iteration 101, loss = 0.22568909\n",
      "Iteration 102, loss = 0.22619034\n",
      "Iteration 103, loss = 0.22525102\n",
      "Iteration 104, loss = 0.22479505\n",
      "Iteration 105, loss = 0.22503587\n",
      "Iteration 106, loss = 0.22441048\n",
      "Iteration 107, loss = 0.22480861\n",
      "Iteration 108, loss = 0.22398195\n",
      "Iteration 109, loss = 0.22462503\n",
      "Iteration 110, loss = 0.22338316\n",
      "Iteration 111, loss = 0.22319731\n",
      "Iteration 112, loss = 0.22284772\n",
      "Iteration 113, loss = 0.22293251\n",
      "Iteration 114, loss = 0.22331225\n",
      "Iteration 115, loss = 0.22189571\n",
      "Iteration 116, loss = 0.22207283\n",
      "Iteration 117, loss = 0.22192823\n",
      "Iteration 118, loss = 0.22168292\n",
      "Iteration 119, loss = 0.22129844\n",
      "Iteration 120, loss = 0.22099298\n",
      "Iteration 121, loss = 0.22037820\n",
      "Iteration 122, loss = 0.22093332\n",
      "Iteration 123, loss = 0.22070548\n",
      "Iteration 124, loss = 0.22016974\n",
      "Iteration 125, loss = 0.21983098\n",
      "Iteration 126, loss = 0.21977932\n",
      "Iteration 127, loss = 0.21882023\n",
      "Iteration 128, loss = 0.21900957\n",
      "Iteration 129, loss = 0.21947923\n",
      "Iteration 130, loss = 0.21887649\n",
      "Iteration 131, loss = 0.21886529\n",
      "Iteration 132, loss = 0.21739220\n",
      "Iteration 133, loss = 0.21779655\n",
      "Iteration 134, loss = 0.21763191\n",
      "Iteration 135, loss = 0.21670729\n",
      "Iteration 136, loss = 0.21683231\n",
      "Iteration 137, loss = 0.21655565\n",
      "Iteration 138, loss = 0.21672270\n",
      "Iteration 139, loss = 0.21617280\n",
      "Iteration 140, loss = 0.21572619\n",
      "Iteration 141, loss = 0.21656752\n",
      "Iteration 142, loss = 0.21588711\n",
      "Iteration 143, loss = 0.21551668\n",
      "Iteration 144, loss = 0.21527008\n",
      "Iteration 145, loss = 0.21467442\n",
      "Iteration 146, loss = 0.21424507\n",
      "Iteration 147, loss = 0.21414397\n",
      "Iteration 148, loss = 0.21421519\n",
      "Iteration 149, loss = 0.21393255\n",
      "Iteration 150, loss = 0.21383383\n",
      "Iteration 151, loss = 0.21359686\n",
      "Iteration 152, loss = 0.21343586\n",
      "Iteration 153, loss = 0.21298447\n",
      "Iteration 154, loss = 0.21239462\n",
      "Iteration 155, loss = 0.21248421\n",
      "Iteration 156, loss = 0.21219764\n",
      "Iteration 157, loss = 0.21211638\n",
      "Iteration 158, loss = 0.21250682\n",
      "Iteration 159, loss = 0.21143332\n",
      "Iteration 160, loss = 0.21118441\n",
      "Iteration 161, loss = 0.21161387\n",
      "Iteration 162, loss = 0.21142864\n",
      "Iteration 163, loss = 0.21030488\n",
      "Iteration 164, loss = 0.21113253\n",
      "Iteration 165, loss = 0.21053330\n",
      "Iteration 166, loss = 0.21080299\n",
      "Iteration 167, loss = 0.20968376\n",
      "Iteration 168, loss = 0.21034087\n",
      "Iteration 169, loss = 0.20953608\n",
      "Iteration 170, loss = 0.20906336\n",
      "Iteration 171, loss = 0.20909220\n",
      "Iteration 172, loss = 0.20924218\n",
      "Iteration 173, loss = 0.20881754\n",
      "Iteration 174, loss = 0.20892275\n",
      "Iteration 175, loss = 0.20876947\n",
      "Iteration 176, loss = 0.20813769\n",
      "Iteration 177, loss = 0.20865549\n",
      "Iteration 178, loss = 0.20817921\n",
      "Iteration 179, loss = 0.20859432\n",
      "Iteration 180, loss = 0.20717620\n",
      "Iteration 181, loss = 0.20749383\n",
      "Iteration 182, loss = 0.20764708\n",
      "Iteration 183, loss = 0.20814108\n",
      "Iteration 184, loss = 0.20672868\n",
      "Iteration 185, loss = 0.20712405\n",
      "Iteration 186, loss = 0.20734736\n",
      "Iteration 187, loss = 0.20669090\n",
      "Iteration 188, loss = 0.20601745\n",
      "Iteration 189, loss = 0.20646320\n",
      "Iteration 190, loss = 0.20698426\n",
      "Iteration 191, loss = 0.20578669\n",
      "Iteration 192, loss = 0.20595272\n",
      "Iteration 193, loss = 0.20554857\n",
      "Iteration 194, loss = 0.20589101\n",
      "Iteration 195, loss = 0.20519879\n",
      "Iteration 196, loss = 0.20562760\n",
      "Iteration 197, loss = 0.20534430\n",
      "Iteration 198, loss = 0.20564315\n",
      "Iteration 199, loss = 0.20504303\n",
      "Iteration 200, loss = 0.20556184\n",
      "Iteration 201, loss = 0.20476954\n",
      "Iteration 202, loss = 0.20483088\n",
      "Iteration 203, loss = 0.20457858\n",
      "Iteration 204, loss = 0.20457885\n",
      "Iteration 205, loss = 0.20381744\n",
      "Iteration 206, loss = 0.20368147\n",
      "Iteration 207, loss = 0.20424491\n",
      "Iteration 208, loss = 0.20353045\n",
      "Iteration 209, loss = 0.20368131\n",
      "Iteration 210, loss = 0.20320354\n",
      "Iteration 211, loss = 0.20383982\n",
      "Iteration 212, loss = 0.20317071\n",
      "Iteration 213, loss = 0.20324215\n",
      "Iteration 214, loss = 0.20289806\n",
      "Iteration 215, loss = 0.20305999\n",
      "Iteration 216, loss = 0.20210043\n",
      "Iteration 217, loss = 0.20281029\n",
      "Iteration 218, loss = 0.20299845\n",
      "Iteration 219, loss = 0.20260344\n",
      "Iteration 220, loss = 0.20224205\n",
      "Iteration 221, loss = 0.20279870\n",
      "Iteration 222, loss = 0.20174870\n",
      "Iteration 223, loss = 0.20315230\n",
      "Iteration 224, loss = 0.20159174\n",
      "Iteration 225, loss = 0.20174990\n",
      "Iteration 226, loss = 0.20191694\n",
      "Iteration 227, loss = 0.20167979\n",
      "Iteration 228, loss = 0.20096199\n",
      "Iteration 229, loss = 0.20090387\n",
      "Iteration 230, loss = 0.20127921\n",
      "Iteration 231, loss = 0.20093051\n",
      "Iteration 232, loss = 0.20163815\n",
      "Iteration 233, loss = 0.20122890\n",
      "Iteration 234, loss = 0.20132964\n",
      "Iteration 235, loss = 0.20104107\n",
      "Iteration 236, loss = 0.19982059\n",
      "Iteration 237, loss = 0.20017174\n",
      "Iteration 238, loss = 0.20110021\n",
      "Iteration 239, loss = 0.20051967\n",
      "Iteration 240, loss = 0.20043203\n",
      "Iteration 241, loss = 0.19967825\n",
      "Iteration 242, loss = 0.20010104\n",
      "Iteration 243, loss = 0.19992091\n",
      "Iteration 244, loss = 0.20025298\n",
      "Iteration 245, loss = 0.20026643\n",
      "Iteration 246, loss = 0.19969843\n",
      "Iteration 247, loss = 0.19941234\n",
      "Iteration 248, loss = 0.19916956\n",
      "Iteration 249, loss = 0.19877523\n",
      "Iteration 250, loss = 0.19896154\n",
      "Iteration 251, loss = 0.19883303\n",
      "Iteration 252, loss = 0.19930441\n",
      "Iteration 253, loss = 0.19978837\n",
      "Iteration 254, loss = 0.19882618\n",
      "Iteration 255, loss = 0.19849613\n",
      "Iteration 256, loss = 0.19882003\n",
      "Iteration 257, loss = 0.19878081\n",
      "Iteration 258, loss = 0.19878726\n",
      "Iteration 259, loss = 0.19839426\n",
      "Iteration 260, loss = 0.19850127\n",
      "Iteration 261, loss = 0.19771048\n",
      "Iteration 262, loss = 0.19820319\n",
      "Iteration 263, loss = 0.19814787\n",
      "Iteration 264, loss = 0.19757982\n",
      "Iteration 265, loss = 0.19750600\n",
      "Iteration 266, loss = 0.19765318\n",
      "Iteration 267, loss = 0.19811512\n",
      "Iteration 268, loss = 0.19715383\n",
      "Iteration 269, loss = 0.19784681\n",
      "Iteration 270, loss = 0.19711161\n",
      "Iteration 271, loss = 0.19730472\n",
      "Iteration 272, loss = 0.19639608\n",
      "Iteration 273, loss = 0.19708440\n",
      "Iteration 274, loss = 0.19711863\n",
      "Iteration 275, loss = 0.19677829\n",
      "Iteration 276, loss = 0.19586138\n",
      "Iteration 277, loss = 0.19598719\n",
      "Iteration 278, loss = 0.19729736\n",
      "Iteration 279, loss = 0.19747522\n",
      "Iteration 280, loss = 0.19579466\n",
      "Iteration 281, loss = 0.19597523\n",
      "Iteration 282, loss = 0.19627158\n",
      "Iteration 283, loss = 0.19619095\n",
      "Iteration 284, loss = 0.19728605\n",
      "Iteration 285, loss = 0.19604197\n",
      "Iteration 286, loss = 0.19636488\n",
      "Iteration 287, loss = 0.19572839\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51514811\n",
      "Iteration 2, loss = 0.46360970\n",
      "Iteration 3, loss = 0.45532378\n",
      "Iteration 4, loss = 0.45040633\n",
      "Iteration 5, loss = 0.44609292\n",
      "Iteration 6, loss = 0.44252022\n",
      "Iteration 7, loss = 0.43994141\n",
      "Iteration 8, loss = 0.43597803\n",
      "Iteration 9, loss = 0.43392222\n",
      "Iteration 10, loss = 0.43089652\n",
      "Iteration 11, loss = 0.42834183\n",
      "Iteration 12, loss = 0.42694672\n",
      "Iteration 13, loss = 0.42436649\n",
      "Iteration 14, loss = 0.42247087\n",
      "Iteration 15, loss = 0.42093707\n",
      "Iteration 16, loss = 0.41921738\n",
      "Iteration 17, loss = 0.41783123\n",
      "Iteration 18, loss = 0.41561665\n",
      "Iteration 19, loss = 0.41420344\n",
      "Iteration 20, loss = 0.41264754\n",
      "Iteration 21, loss = 0.41152786\n",
      "Iteration 22, loss = 0.40948716\n",
      "Iteration 23, loss = 0.40820301\n",
      "Iteration 24, loss = 0.40732000\n",
      "Iteration 25, loss = 0.40564759\n",
      "Iteration 26, loss = 0.40455599\n",
      "Iteration 27, loss = 0.40343495\n",
      "Iteration 28, loss = 0.40231028\n",
      "Iteration 29, loss = 0.40108341\n",
      "Iteration 30, loss = 0.40009782\n",
      "Iteration 31, loss = 0.39884264\n",
      "Iteration 32, loss = 0.39813666\n",
      "Iteration 33, loss = 0.39694953\n",
      "Iteration 34, loss = 0.39622908\n",
      "Iteration 35, loss = 0.39470587\n",
      "Iteration 36, loss = 0.39439236\n",
      "Iteration 37, loss = 0.39312016\n",
      "Iteration 38, loss = 0.39220401\n",
      "Iteration 39, loss = 0.39190915\n",
      "Iteration 40, loss = 0.38990356\n",
      "Iteration 41, loss = 0.38923406\n",
      "Iteration 42, loss = 0.38897308\n",
      "Iteration 43, loss = 0.38811711\n",
      "Iteration 44, loss = 0.38690075\n",
      "Iteration 45, loss = 0.38655231\n",
      "Iteration 46, loss = 0.38539054\n",
      "Iteration 47, loss = 0.38548386\n",
      "Iteration 48, loss = 0.38382341\n",
      "Iteration 49, loss = 0.38353646\n",
      "Iteration 50, loss = 0.38306889\n",
      "Iteration 51, loss = 0.38179442\n",
      "Iteration 52, loss = 0.38171838\n",
      "Iteration 53, loss = 0.38062413\n",
      "Iteration 54, loss = 0.37964677\n",
      "Iteration 55, loss = 0.37963804\n",
      "Iteration 56, loss = 0.37928111\n",
      "Iteration 57, loss = 0.37826875\n",
      "Iteration 58, loss = 0.37711413\n",
      "Iteration 59, loss = 0.37708896\n",
      "Iteration 60, loss = 0.37653473\n",
      "Iteration 61, loss = 0.37591750\n",
      "Iteration 62, loss = 0.37559014\n",
      "Iteration 63, loss = 0.37383496\n",
      "Iteration 64, loss = 0.37426058\n",
      "Iteration 65, loss = 0.37329705\n",
      "Iteration 66, loss = 0.37422812\n",
      "Iteration 67, loss = 0.37194037\n",
      "Iteration 68, loss = 0.37302134\n",
      "Iteration 69, loss = 0.37145985\n",
      "Iteration 70, loss = 0.37097994\n",
      "Iteration 71, loss = 0.37010354\n",
      "Iteration 72, loss = 0.37010064\n",
      "Iteration 73, loss = 0.37000316\n",
      "Iteration 74, loss = 0.36950005\n",
      "Iteration 75, loss = 0.36867288\n",
      "Iteration 76, loss = 0.36811771\n",
      "Iteration 77, loss = 0.36824970\n",
      "Iteration 78, loss = 0.36773305\n",
      "Iteration 79, loss = 0.36744391\n",
      "Iteration 80, loss = 0.36678747\n",
      "Iteration 81, loss = 0.36593565\n",
      "Iteration 82, loss = 0.36646032\n",
      "Iteration 83, loss = 0.36485437\n",
      "Iteration 84, loss = 0.36512559\n",
      "Iteration 85, loss = 0.36478755\n",
      "Iteration 86, loss = 0.36427648\n",
      "Iteration 87, loss = 0.36362279\n",
      "Iteration 88, loss = 0.36355384\n",
      "Iteration 89, loss = 0.36276226\n",
      "Iteration 90, loss = 0.36319537\n",
      "Iteration 91, loss = 0.36266911\n",
      "Iteration 92, loss = 0.36228772\n",
      "Iteration 93, loss = 0.36206269\n",
      "Iteration 94, loss = 0.36076748\n",
      "Iteration 95, loss = 0.36097405\n",
      "Iteration 96, loss = 0.36076288\n",
      "Iteration 97, loss = 0.36065682\n",
      "Iteration 98, loss = 0.36033519\n",
      "Iteration 99, loss = 0.35968364\n",
      "Iteration 100, loss = 0.35985485\n",
      "Iteration 101, loss = 0.35809966\n",
      "Iteration 102, loss = 0.35905220\n",
      "Iteration 103, loss = 0.35846945\n",
      "Iteration 104, loss = 0.35856754\n",
      "Iteration 105, loss = 0.35827758\n",
      "Iteration 106, loss = 0.35730867\n",
      "Iteration 107, loss = 0.35634486\n",
      "Iteration 108, loss = 0.35747145\n",
      "Iteration 109, loss = 0.35690806\n",
      "Iteration 110, loss = 0.35634133\n",
      "Iteration 111, loss = 0.35574426\n",
      "Iteration 112, loss = 0.35535290\n",
      "Iteration 113, loss = 0.35589787\n",
      "Iteration 114, loss = 0.35471459\n",
      "Iteration 115, loss = 0.35398974\n",
      "Iteration 116, loss = 0.35360643\n",
      "Iteration 117, loss = 0.35415133\n",
      "Iteration 118, loss = 0.35351720\n",
      "Iteration 119, loss = 0.35309908\n",
      "Iteration 120, loss = 0.35289253\n",
      "Iteration 121, loss = 0.35234037\n",
      "Iteration 122, loss = 0.35288419\n",
      "Iteration 123, loss = 0.35286274\n",
      "Iteration 124, loss = 0.35157428\n",
      "Iteration 125, loss = 0.35094145\n",
      "Iteration 126, loss = 0.35044584\n",
      "Iteration 127, loss = 0.35142696\n",
      "Iteration 128, loss = 0.35001018\n",
      "Iteration 129, loss = 0.34971732\n",
      "Iteration 130, loss = 0.35011493\n",
      "Iteration 131, loss = 0.35061337\n",
      "Iteration 132, loss = 0.34829237\n",
      "Iteration 133, loss = 0.34815313\n",
      "Iteration 134, loss = 0.34847582\n",
      "Iteration 135, loss = 0.34778228\n",
      "Iteration 136, loss = 0.34801320\n",
      "Iteration 137, loss = 0.34820361\n",
      "Iteration 138, loss = 0.34756916\n",
      "Iteration 139, loss = 0.34687128\n",
      "Iteration 140, loss = 0.34662068\n",
      "Iteration 141, loss = 0.34699553\n",
      "Iteration 142, loss = 0.34631192\n",
      "Iteration 143, loss = 0.34658636\n",
      "Iteration 144, loss = 0.34524853\n",
      "Iteration 145, loss = 0.34505737\n",
      "Iteration 146, loss = 0.34527042\n",
      "Iteration 147, loss = 0.34533144\n",
      "Iteration 148, loss = 0.34498628\n",
      "Iteration 149, loss = 0.34508738\n",
      "Iteration 150, loss = 0.34380044\n",
      "Iteration 151, loss = 0.34415419\n",
      "Iteration 152, loss = 0.34368851\n",
      "Iteration 153, loss = 0.34371747\n",
      "Iteration 154, loss = 0.34333677\n",
      "Iteration 155, loss = 0.34279380\n",
      "Iteration 156, loss = 0.34307034\n",
      "Iteration 157, loss = 0.34297761\n",
      "Iteration 158, loss = 0.34171157\n",
      "Iteration 159, loss = 0.34278974\n",
      "Iteration 160, loss = 0.34201173\n",
      "Iteration 161, loss = 0.34146735\n",
      "Iteration 162, loss = 0.34099847\n",
      "Iteration 163, loss = 0.34119613\n",
      "Iteration 164, loss = 0.34057003\n",
      "Iteration 165, loss = 0.34168687\n",
      "Iteration 166, loss = 0.34024010\n",
      "Iteration 167, loss = 0.33963365\n",
      "Iteration 168, loss = 0.33976952\n",
      "Iteration 169, loss = 0.34009052\n",
      "Iteration 170, loss = 0.33950480\n",
      "Iteration 171, loss = 0.33927631\n",
      "Iteration 172, loss = 0.33924208\n",
      "Iteration 173, loss = 0.33876523\n",
      "Iteration 174, loss = 0.33787180\n",
      "Iteration 175, loss = 0.33925159\n",
      "Iteration 176, loss = 0.33964555\n",
      "Iteration 177, loss = 0.33838798\n",
      "Iteration 178, loss = 0.33774127\n",
      "Iteration 179, loss = 0.33823924\n",
      "Iteration 180, loss = 0.33742043\n",
      "Iteration 181, loss = 0.33736592\n",
      "Iteration 182, loss = 0.33723784\n",
      "Iteration 183, loss = 0.33675621\n",
      "Iteration 184, loss = 0.33610371\n",
      "Iteration 185, loss = 0.33672604\n",
      "Iteration 186, loss = 0.33578829\n",
      "Iteration 187, loss = 0.33572860\n",
      "Iteration 188, loss = 0.33579953\n",
      "Iteration 189, loss = 0.33609718\n",
      "Iteration 190, loss = 0.33572737\n",
      "Iteration 191, loss = 0.33695459\n",
      "Iteration 192, loss = 0.33475592\n",
      "Iteration 193, loss = 0.33510230\n",
      "Iteration 194, loss = 0.33438788\n",
      "Iteration 195, loss = 0.33525638\n",
      "Iteration 196, loss = 0.33524348\n",
      "Iteration 197, loss = 0.33425774\n",
      "Iteration 198, loss = 0.33422399\n",
      "Iteration 199, loss = 0.33435981\n",
      "Iteration 200, loss = 0.33379913\n",
      "Iteration 201, loss = 0.33480178\n",
      "Iteration 202, loss = 0.33306023\n",
      "Iteration 203, loss = 0.33309909\n",
      "Iteration 204, loss = 0.33384717\n",
      "Iteration 205, loss = 0.33362135\n",
      "Iteration 206, loss = 0.33319881\n",
      "Iteration 207, loss = 0.33264441\n",
      "Iteration 208, loss = 0.33215510\n",
      "Iteration 209, loss = 0.33313368\n",
      "Iteration 210, loss = 0.33284179\n",
      "Iteration 211, loss = 0.33248115\n",
      "Iteration 212, loss = 0.33188993\n",
      "Iteration 213, loss = 0.33174326\n",
      "Iteration 214, loss = 0.33192769\n",
      "Iteration 215, loss = 0.33175576\n",
      "Iteration 216, loss = 0.33137731\n",
      "Iteration 217, loss = 0.33160746\n",
      "Iteration 218, loss = 0.33071106\n",
      "Iteration 219, loss = 0.33070898\n",
      "Iteration 220, loss = 0.33209218\n",
      "Iteration 221, loss = 0.33095203\n",
      "Iteration 222, loss = 0.33137839\n",
      "Iteration 223, loss = 0.33059758\n",
      "Iteration 224, loss = 0.32962336\n",
      "Iteration 225, loss = 0.32997157\n",
      "Iteration 226, loss = 0.32972609\n",
      "Iteration 227, loss = 0.33096313\n",
      "Iteration 228, loss = 0.33039292\n",
      "Iteration 229, loss = 0.33015573\n",
      "Iteration 230, loss = 0.32929874\n",
      "Iteration 231, loss = 0.32935467\n",
      "Iteration 232, loss = 0.32889401\n",
      "Iteration 233, loss = 0.32985189\n",
      "Iteration 234, loss = 0.33023067\n",
      "Iteration 235, loss = 0.32878976\n",
      "Iteration 236, loss = 0.32908574\n",
      "Iteration 237, loss = 0.32846859\n",
      "Iteration 238, loss = 0.32736162\n",
      "Iteration 239, loss = 0.32788963\n",
      "Iteration 240, loss = 0.32812078\n",
      "Iteration 241, loss = 0.32789742\n",
      "Iteration 242, loss = 0.32813047\n",
      "Iteration 243, loss = 0.32786328\n",
      "Iteration 244, loss = 0.32657271\n",
      "Iteration 245, loss = 0.32820618\n",
      "Iteration 246, loss = 0.32743112\n",
      "Iteration 247, loss = 0.32707819\n",
      "Iteration 248, loss = 0.32668478\n",
      "Iteration 249, loss = 0.32651579\n",
      "Iteration 250, loss = 0.32632972\n",
      "Iteration 251, loss = 0.32723100\n",
      "Iteration 252, loss = 0.32633231\n",
      "Iteration 253, loss = 0.32645499\n",
      "Iteration 254, loss = 0.32649848\n",
      "Iteration 255, loss = 0.32634951\n",
      "Iteration 256, loss = 0.32578988\n",
      "Iteration 257, loss = 0.32552149\n",
      "Iteration 258, loss = 0.32530142\n",
      "Iteration 259, loss = 0.32582949\n",
      "Iteration 260, loss = 0.32637745\n",
      "Iteration 261, loss = 0.32548027\n",
      "Iteration 262, loss = 0.32521803\n",
      "Iteration 263, loss = 0.32522830\n",
      "Iteration 264, loss = 0.32546703\n",
      "Iteration 265, loss = 0.32626434\n",
      "Iteration 266, loss = 0.32578297\n",
      "Iteration 267, loss = 0.32375454\n",
      "Iteration 268, loss = 0.32534486\n",
      "Iteration 269, loss = 0.32462092\n",
      "Iteration 270, loss = 0.32490672\n",
      "Iteration 271, loss = 0.32400932\n",
      "Iteration 272, loss = 0.32476728\n",
      "Iteration 273, loss = 0.32479511\n",
      "Iteration 274, loss = 0.32472941\n",
      "Iteration 275, loss = 0.32398788\n",
      "Iteration 276, loss = 0.32354957\n",
      "Iteration 277, loss = 0.32438330\n",
      "Iteration 278, loss = 0.32420389\n",
      "Iteration 279, loss = 0.32503118\n",
      "Iteration 280, loss = 0.32395468\n",
      "Iteration 281, loss = 0.32269032\n",
      "Iteration 282, loss = 0.32296999\n",
      "Iteration 283, loss = 0.32395463\n",
      "Iteration 284, loss = 0.32297380\n",
      "Iteration 285, loss = 0.32421891\n",
      "Iteration 286, loss = 0.32467619\n",
      "Iteration 287, loss = 0.32325616\n",
      "Iteration 288, loss = 0.32278825\n",
      "Iteration 289, loss = 0.32280909\n",
      "Iteration 290, loss = 0.32305408\n",
      "Iteration 291, loss = 0.32264655\n",
      "Iteration 292, loss = 0.32209601\n",
      "Iteration 293, loss = 0.32257685\n",
      "Iteration 294, loss = 0.32146551\n",
      "Iteration 295, loss = 0.32212230\n",
      "Iteration 296, loss = 0.32210951\n",
      "Iteration 297, loss = 0.32456441\n",
      "Iteration 298, loss = 0.32178714\n",
      "Iteration 299, loss = 0.32155644\n",
      "Iteration 300, loss = 0.32243573\n",
      "Iteration 301, loss = 0.32192774\n",
      "Iteration 302, loss = 0.32209516\n",
      "Iteration 303, loss = 0.32262346\n",
      "Iteration 304, loss = 0.32202569\n",
      "Iteration 305, loss = 0.32191628\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51868421\n",
      "Iteration 2, loss = 0.46398392\n",
      "Iteration 3, loss = 0.45420446\n",
      "Iteration 4, loss = 0.44699733\n",
      "Iteration 5, loss = 0.44168604\n",
      "Iteration 6, loss = 0.43641304\n",
      "Iteration 7, loss = 0.43245545\n",
      "Iteration 8, loss = 0.42887363\n",
      "Iteration 9, loss = 0.42480610\n",
      "Iteration 10, loss = 0.42147414\n",
      "Iteration 11, loss = 0.41841443\n",
      "Iteration 12, loss = 0.41575625\n",
      "Iteration 13, loss = 0.41266958\n",
      "Iteration 14, loss = 0.41036849\n",
      "Iteration 15, loss = 0.40777769\n",
      "Iteration 16, loss = 0.40593564\n",
      "Iteration 17, loss = 0.40360591\n",
      "Iteration 18, loss = 0.40144511\n",
      "Iteration 19, loss = 0.39953520\n",
      "Iteration 20, loss = 0.39793052\n",
      "Iteration 21, loss = 0.39579834\n",
      "Iteration 22, loss = 0.39433351\n",
      "Iteration 23, loss = 0.39283423\n",
      "Iteration 24, loss = 0.39165472\n",
      "Iteration 25, loss = 0.38937133\n",
      "Iteration 26, loss = 0.38857369\n",
      "Iteration 27, loss = 0.38669657\n",
      "Iteration 28, loss = 0.38547833\n",
      "Iteration 29, loss = 0.38432759\n",
      "Iteration 30, loss = 0.38225714\n",
      "Iteration 31, loss = 0.38199930\n",
      "Iteration 32, loss = 0.38029261\n",
      "Iteration 33, loss = 0.37984857\n",
      "Iteration 34, loss = 0.37831321\n",
      "Iteration 35, loss = 0.37766946\n",
      "Iteration 36, loss = 0.37598196\n",
      "Iteration 37, loss = 0.37486111\n",
      "Iteration 38, loss = 0.37371875\n",
      "Iteration 39, loss = 0.37241752\n",
      "Iteration 40, loss = 0.37163237\n",
      "Iteration 41, loss = 0.37077057\n",
      "Iteration 42, loss = 0.37010039\n",
      "Iteration 43, loss = 0.36923019\n",
      "Iteration 44, loss = 0.36839296\n",
      "Iteration 45, loss = 0.36672524\n",
      "Iteration 46, loss = 0.36644372\n",
      "Iteration 47, loss = 0.36520931\n",
      "Iteration 48, loss = 0.36441795\n",
      "Iteration 49, loss = 0.36332777\n",
      "Iteration 50, loss = 0.36239127\n",
      "Iteration 51, loss = 0.36229085\n",
      "Iteration 52, loss = 0.36167966\n",
      "Iteration 53, loss = 0.35973650\n",
      "Iteration 54, loss = 0.35973210\n",
      "Iteration 55, loss = 0.35921488\n",
      "Iteration 56, loss = 0.35759885\n",
      "Iteration 57, loss = 0.35721552\n",
      "Iteration 58, loss = 0.35662541\n",
      "Iteration 59, loss = 0.35612535\n",
      "Iteration 60, loss = 0.35536312\n",
      "Iteration 61, loss = 0.35433783\n",
      "Iteration 62, loss = 0.35335325\n",
      "Iteration 63, loss = 0.35320290\n",
      "Iteration 64, loss = 0.35206004\n",
      "Iteration 65, loss = 0.35188457\n",
      "Iteration 66, loss = 0.35136948\n",
      "Iteration 67, loss = 0.35032814\n",
      "Iteration 68, loss = 0.34994435\n",
      "Iteration 69, loss = 0.34918879\n",
      "Iteration 70, loss = 0.34886078\n",
      "Iteration 71, loss = 0.34845347\n",
      "Iteration 72, loss = 0.34779840\n",
      "Iteration 73, loss = 0.34655264\n",
      "Iteration 74, loss = 0.34723758\n",
      "Iteration 75, loss = 0.34540476\n",
      "Iteration 76, loss = 0.34559467\n",
      "Iteration 77, loss = 0.34561542\n",
      "Iteration 78, loss = 0.34408559\n",
      "Iteration 79, loss = 0.34286749\n",
      "Iteration 80, loss = 0.34312653\n",
      "Iteration 81, loss = 0.34333558\n",
      "Iteration 82, loss = 0.34182746\n",
      "Iteration 83, loss = 0.34245762\n",
      "Iteration 84, loss = 0.34040742\n",
      "Iteration 85, loss = 0.34038614\n",
      "Iteration 86, loss = 0.33972147\n",
      "Iteration 87, loss = 0.33948121\n",
      "Iteration 88, loss = 0.33917894\n",
      "Iteration 89, loss = 0.33942840\n",
      "Iteration 90, loss = 0.33821225\n",
      "Iteration 91, loss = 0.33847118\n",
      "Iteration 92, loss = 0.33678267\n",
      "Iteration 93, loss = 0.33717711\n",
      "Iteration 94, loss = 0.33596857\n",
      "Iteration 95, loss = 0.33618484\n",
      "Iteration 96, loss = 0.33557702\n",
      "Iteration 97, loss = 0.33547766\n",
      "Iteration 98, loss = 0.33484986\n",
      "Iteration 99, loss = 0.33530392\n",
      "Iteration 100, loss = 0.33428603\n",
      "Iteration 101, loss = 0.33342038\n",
      "Iteration 102, loss = 0.33297289\n",
      "Iteration 103, loss = 0.33319615\n",
      "Iteration 104, loss = 0.33191734\n",
      "Iteration 105, loss = 0.33302494\n",
      "Iteration 106, loss = 0.33170906\n",
      "Iteration 107, loss = 0.33160003\n",
      "Iteration 108, loss = 0.33075031\n",
      "Iteration 109, loss = 0.33109608\n",
      "Iteration 110, loss = 0.33101616\n",
      "Iteration 111, loss = 0.33045214\n",
      "Iteration 112, loss = 0.33010096\n",
      "Iteration 113, loss = 0.32846559\n",
      "Iteration 114, loss = 0.33019148\n",
      "Iteration 115, loss = 0.32824709\n",
      "Iteration 116, loss = 0.32881621\n",
      "Iteration 117, loss = 0.32799365\n",
      "Iteration 118, loss = 0.32841106\n",
      "Iteration 119, loss = 0.32737320\n",
      "Iteration 120, loss = 0.32636863\n",
      "Iteration 121, loss = 0.32632954\n",
      "Iteration 122, loss = 0.32633921\n",
      "Iteration 123, loss = 0.32637008\n",
      "Iteration 124, loss = 0.32570140\n",
      "Iteration 125, loss = 0.32527952\n",
      "Iteration 126, loss = 0.32506101\n",
      "Iteration 127, loss = 0.32377718\n",
      "Iteration 128, loss = 0.32457761\n",
      "Iteration 129, loss = 0.32474233\n",
      "Iteration 130, loss = 0.32347396\n",
      "Iteration 131, loss = 0.32324445\n",
      "Iteration 132, loss = 0.32434205\n",
      "Iteration 133, loss = 0.32229555\n",
      "Iteration 134, loss = 0.32295479\n",
      "Iteration 135, loss = 0.32202200\n",
      "Iteration 136, loss = 0.32133842\n",
      "Iteration 137, loss = 0.32098613\n",
      "Iteration 138, loss = 0.32179910\n",
      "Iteration 139, loss = 0.32136683\n",
      "Iteration 140, loss = 0.31983082\n",
      "Iteration 141, loss = 0.32009786\n",
      "Iteration 142, loss = 0.32004151\n",
      "Iteration 143, loss = 0.31922046\n",
      "Iteration 144, loss = 0.31989940\n",
      "Iteration 145, loss = 0.31883631\n",
      "Iteration 146, loss = 0.31950920\n",
      "Iteration 147, loss = 0.31816003\n",
      "Iteration 148, loss = 0.31851907\n",
      "Iteration 149, loss = 0.31836860\n",
      "Iteration 150, loss = 0.31789715\n",
      "Iteration 151, loss = 0.31850644\n",
      "Iteration 152, loss = 0.31761984\n",
      "Iteration 153, loss = 0.31827207\n",
      "Iteration 154, loss = 0.31644485\n",
      "Iteration 155, loss = 0.31644912\n",
      "Iteration 156, loss = 0.31726704\n",
      "Iteration 157, loss = 0.31568207\n",
      "Iteration 158, loss = 0.31591242\n",
      "Iteration 159, loss = 0.31622978\n",
      "Iteration 160, loss = 0.31553281\n",
      "Iteration 161, loss = 0.31508136\n",
      "Iteration 162, loss = 0.31465977\n",
      "Iteration 163, loss = 0.31516534\n",
      "Iteration 164, loss = 0.31508758\n",
      "Iteration 165, loss = 0.31428553\n",
      "Iteration 166, loss = 0.31503332\n",
      "Iteration 167, loss = 0.31361069\n",
      "Iteration 168, loss = 0.31316153\n",
      "Iteration 169, loss = 0.31429171\n",
      "Iteration 170, loss = 0.31298435\n",
      "Iteration 171, loss = 0.31243113\n",
      "Iteration 172, loss = 0.31328079\n",
      "Iteration 173, loss = 0.31334867\n",
      "Iteration 174, loss = 0.31277050\n",
      "Iteration 175, loss = 0.31238911\n",
      "Iteration 176, loss = 0.31193038\n",
      "Iteration 177, loss = 0.31190068\n",
      "Iteration 178, loss = 0.31115795\n",
      "Iteration 179, loss = 0.31038045\n",
      "Iteration 180, loss = 0.31071292\n",
      "Iteration 181, loss = 0.31063448\n",
      "Iteration 182, loss = 0.31026765\n",
      "Iteration 183, loss = 0.31154453\n",
      "Iteration 184, loss = 0.30980502\n",
      "Iteration 185, loss = 0.30940236\n",
      "Iteration 186, loss = 0.30921866\n",
      "Iteration 187, loss = 0.30909051\n",
      "Iteration 188, loss = 0.30832350\n",
      "Iteration 189, loss = 0.30881564\n",
      "Iteration 190, loss = 0.30847772\n",
      "Iteration 191, loss = 0.31010855\n",
      "Iteration 192, loss = 0.30971357\n",
      "Iteration 193, loss = 0.30840246\n",
      "Iteration 194, loss = 0.30711374\n",
      "Iteration 195, loss = 0.30835275\n",
      "Iteration 196, loss = 0.30767135\n",
      "Iteration 197, loss = 0.30706323\n",
      "Iteration 198, loss = 0.30687254\n",
      "Iteration 199, loss = 0.30759140\n",
      "Iteration 200, loss = 0.30735281\n",
      "Iteration 201, loss = 0.30769674\n",
      "Iteration 202, loss = 0.30531506\n",
      "Iteration 203, loss = 0.30598547\n",
      "Iteration 204, loss = 0.30570586\n",
      "Iteration 205, loss = 0.30689867\n",
      "Iteration 206, loss = 0.30619380\n",
      "Iteration 207, loss = 0.30568814\n",
      "Iteration 208, loss = 0.30551619\n",
      "Iteration 209, loss = 0.30631031\n",
      "Iteration 210, loss = 0.30431608\n",
      "Iteration 211, loss = 0.30433438\n",
      "Iteration 212, loss = 0.30517586\n",
      "Iteration 213, loss = 0.30428435\n",
      "Iteration 214, loss = 0.30395360\n",
      "Iteration 215, loss = 0.30451968\n",
      "Iteration 216, loss = 0.30348195\n",
      "Iteration 217, loss = 0.30390593\n",
      "Iteration 218, loss = 0.30379477\n",
      "Iteration 219, loss = 0.30460992\n",
      "Iteration 220, loss = 0.30266433\n",
      "Iteration 221, loss = 0.30405653\n",
      "Iteration 222, loss = 0.30197415\n",
      "Iteration 223, loss = 0.30271563\n",
      "Iteration 224, loss = 0.30274311\n",
      "Iteration 225, loss = 0.30223797\n",
      "Iteration 226, loss = 0.30389379\n",
      "Iteration 227, loss = 0.30273562\n",
      "Iteration 228, loss = 0.30163174\n",
      "Iteration 229, loss = 0.30126914\n",
      "Iteration 230, loss = 0.30314823\n",
      "Iteration 231, loss = 0.30223632\n",
      "Iteration 232, loss = 0.30196380\n",
      "Iteration 233, loss = 0.30100043\n",
      "Iteration 234, loss = 0.30158355\n",
      "Iteration 235, loss = 0.30045360\n",
      "Iteration 236, loss = 0.30190464\n",
      "Iteration 237, loss = 0.30189865\n",
      "Iteration 238, loss = 0.30030925\n",
      "Iteration 239, loss = 0.30086206\n",
      "Iteration 240, loss = 0.30103702\n",
      "Iteration 241, loss = 0.30170472\n",
      "Iteration 242, loss = 0.29879257\n",
      "Iteration 243, loss = 0.30034119\n",
      "Iteration 244, loss = 0.29965792\n",
      "Iteration 245, loss = 0.30030213\n",
      "Iteration 246, loss = 0.30027463\n",
      "Iteration 247, loss = 0.29961165\n",
      "Iteration 248, loss = 0.29990601\n",
      "Iteration 249, loss = 0.29811249\n",
      "Iteration 250, loss = 0.29934574\n",
      "Iteration 251, loss = 0.29996293\n",
      "Iteration 252, loss = 0.30000583\n",
      "Iteration 253, loss = 0.29932183\n",
      "Iteration 254, loss = 0.29747572\n",
      "Iteration 255, loss = 0.29797849\n",
      "Iteration 256, loss = 0.29952080\n",
      "Iteration 257, loss = 0.29836477\n",
      "Iteration 258, loss = 0.29833606\n",
      "Iteration 259, loss = 0.29932834\n",
      "Iteration 260, loss = 0.29706112\n",
      "Iteration 261, loss = 0.29812882\n",
      "Iteration 262, loss = 0.29714960\n",
      "Iteration 263, loss = 0.29773319\n",
      "Iteration 264, loss = 0.29754879\n",
      "Iteration 265, loss = 0.29627977\n",
      "Iteration 266, loss = 0.29683321\n",
      "Iteration 267, loss = 0.29698338\n",
      "Iteration 268, loss = 0.29741787\n",
      "Iteration 269, loss = 0.29672787\n",
      "Iteration 270, loss = 0.29660707\n",
      "Iteration 271, loss = 0.29653893\n",
      "Iteration 272, loss = 0.29604764\n",
      "Iteration 273, loss = 0.29631864\n",
      "Iteration 274, loss = 0.29645003\n",
      "Iteration 275, loss = 0.29557892\n",
      "Iteration 276, loss = 0.29578488\n",
      "Iteration 277, loss = 0.29636883\n",
      "Iteration 278, loss = 0.29571895\n",
      "Iteration 279, loss = 0.29713991\n",
      "Iteration 280, loss = 0.29565835\n",
      "Iteration 281, loss = 0.29557909\n",
      "Iteration 282, loss = 0.29579737\n",
      "Iteration 283, loss = 0.29499745\n",
      "Iteration 284, loss = 0.29511600\n",
      "Iteration 285, loss = 0.29457478\n",
      "Iteration 286, loss = 0.29417369\n",
      "Iteration 287, loss = 0.29505614\n",
      "Iteration 288, loss = 0.29346060\n",
      "Iteration 289, loss = 0.29403105\n",
      "Iteration 290, loss = 0.29397086\n",
      "Iteration 291, loss = 0.29414731\n",
      "Iteration 292, loss = 0.29515776\n",
      "Iteration 293, loss = 0.29452670\n",
      "Iteration 294, loss = 0.29260701\n",
      "Iteration 295, loss = 0.29346644\n",
      "Iteration 296, loss = 0.29324555\n",
      "Iteration 297, loss = 0.29325505\n",
      "Iteration 298, loss = 0.29423576\n",
      "Iteration 299, loss = 0.29290895\n",
      "Iteration 300, loss = 0.29264286\n",
      "Iteration 301, loss = 0.29349759\n",
      "Iteration 302, loss = 0.29283458\n",
      "Iteration 303, loss = 0.29330434\n",
      "Iteration 304, loss = 0.29223067\n",
      "Iteration 305, loss = 0.29187303\n",
      "Iteration 306, loss = 0.29195055\n",
      "Iteration 307, loss = 0.29350084\n",
      "Iteration 308, loss = 0.29298177\n",
      "Iteration 309, loss = 0.29175429\n",
      "Iteration 310, loss = 0.29183525\n",
      "Iteration 311, loss = 0.29126369\n",
      "Iteration 312, loss = 0.29124601\n",
      "Iteration 313, loss = 0.29237076\n",
      "Iteration 314, loss = 0.29067689\n",
      "Iteration 315, loss = 0.29044326\n",
      "Iteration 316, loss = 0.29199886\n",
      "Iteration 317, loss = 0.29076222\n",
      "Iteration 318, loss = 0.29118567\n",
      "Iteration 319, loss = 0.29096812\n",
      "Iteration 320, loss = 0.29228848\n",
      "Iteration 321, loss = 0.29034105\n",
      "Iteration 322, loss = 0.29126815\n",
      "Iteration 323, loss = 0.29117323\n",
      "Iteration 324, loss = 0.28943351\n",
      "Iteration 325, loss = 0.29056324\n",
      "Iteration 326, loss = 0.28972287\n",
      "Iteration 327, loss = 0.28923094\n",
      "Iteration 328, loss = 0.29039453\n",
      "Iteration 329, loss = 0.29049290\n",
      "Iteration 330, loss = 0.29061399\n",
      "Iteration 331, loss = 0.28870973\n",
      "Iteration 332, loss = 0.28976009\n",
      "Iteration 333, loss = 0.28984926\n",
      "Iteration 334, loss = 0.28943575\n",
      "Iteration 335, loss = 0.28868717\n",
      "Iteration 336, loss = 0.28938563\n",
      "Iteration 337, loss = 0.28916471\n",
      "Iteration 338, loss = 0.28857285\n",
      "Iteration 339, loss = 0.28886816\n",
      "Iteration 340, loss = 0.28822016\n",
      "Iteration 341, loss = 0.28756078\n",
      "Iteration 342, loss = 0.28832695\n",
      "Iteration 343, loss = 0.28932435\n",
      "Iteration 344, loss = 0.28805483\n",
      "Iteration 345, loss = 0.28875524\n",
      "Iteration 346, loss = 0.28699917\n",
      "Iteration 347, loss = 0.28775295\n",
      "Iteration 348, loss = 0.28743657\n",
      "Iteration 349, loss = 0.28901214\n",
      "Iteration 350, loss = 0.28856120\n",
      "Iteration 351, loss = 0.28679181\n",
      "Iteration 352, loss = 0.28862083\n",
      "Iteration 353, loss = 0.28722251\n",
      "Iteration 354, loss = 0.28773285\n",
      "Iteration 355, loss = 0.28689399\n",
      "Iteration 356, loss = 0.28827801\n",
      "Iteration 357, loss = 0.28727942\n",
      "Iteration 358, loss = 0.28840835\n",
      "Iteration 359, loss = 0.28666322\n",
      "Iteration 360, loss = 0.28668751\n",
      "Iteration 361, loss = 0.28588238\n",
      "Iteration 362, loss = 0.28674252\n",
      "Iteration 363, loss = 0.28609757\n",
      "Iteration 364, loss = 0.28653087\n",
      "Iteration 365, loss = 0.28613984\n",
      "Iteration 366, loss = 0.28604121\n",
      "Iteration 367, loss = 0.28823504\n",
      "Iteration 368, loss = 0.28680777\n",
      "Iteration 369, loss = 0.28722496\n",
      "Iteration 370, loss = 0.28519921\n",
      "Iteration 371, loss = 0.28569284\n",
      "Iteration 372, loss = 0.28526233\n",
      "Iteration 373, loss = 0.28710237\n",
      "Iteration 374, loss = 0.28527529\n",
      "Iteration 375, loss = 0.28573608\n",
      "Iteration 376, loss = 0.28574390\n",
      "Iteration 377, loss = 0.28764989\n",
      "Iteration 378, loss = 0.28471107\n",
      "Iteration 379, loss = 0.28649804\n",
      "Iteration 380, loss = 0.28489914\n",
      "Iteration 381, loss = 0.28571584\n",
      "Iteration 382, loss = 0.28573791\n",
      "Iteration 383, loss = 0.28618964\n",
      "Iteration 384, loss = 0.28406264\n",
      "Iteration 385, loss = 0.28405042\n",
      "Iteration 386, loss = 0.28556685\n",
      "Iteration 387, loss = 0.28429948\n",
      "Iteration 388, loss = 0.28377073\n",
      "Iteration 389, loss = 0.28417430\n",
      "Iteration 390, loss = 0.28475597\n",
      "Iteration 391, loss = 0.28544154\n",
      "Iteration 392, loss = 0.28506867\n",
      "Iteration 393, loss = 0.28482154\n",
      "Iteration 394, loss = 0.28579293\n",
      "Iteration 395, loss = 0.28570039\n",
      "Iteration 396, loss = 0.28313837\n",
      "Iteration 397, loss = 0.28368756\n",
      "Iteration 398, loss = 0.28412342\n",
      "Iteration 399, loss = 0.28542335\n",
      "Iteration 400, loss = 0.28251592\n",
      "Iteration 401, loss = 0.28353256\n",
      "Iteration 402, loss = 0.28309182\n",
      "Iteration 403, loss = 0.28399226\n",
      "Iteration 404, loss = 0.28291576\n",
      "Iteration 405, loss = 0.28311156\n",
      "Iteration 406, loss = 0.28303192\n",
      "Iteration 407, loss = 0.28356113\n",
      "Iteration 408, loss = 0.28270618\n",
      "Iteration 409, loss = 0.28264636\n",
      "Iteration 410, loss = 0.28378116\n",
      "Iteration 411, loss = 0.28245395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.52155318\n",
      "Iteration 2, loss = 0.46870850\n",
      "Iteration 3, loss = 0.45879078\n",
      "Iteration 4, loss = 0.45115332\n",
      "Iteration 5, loss = 0.44546162\n",
      "Iteration 6, loss = 0.44001569\n",
      "Iteration 7, loss = 0.43521561\n",
      "Iteration 8, loss = 0.43094864\n",
      "Iteration 9, loss = 0.42660085\n",
      "Iteration 10, loss = 0.42363018\n",
      "Iteration 11, loss = 0.41974918\n",
      "Iteration 12, loss = 0.41617695\n",
      "Iteration 13, loss = 0.41343134\n",
      "Iteration 14, loss = 0.41057745\n",
      "Iteration 15, loss = 0.40820462\n",
      "Iteration 16, loss = 0.40570131\n",
      "Iteration 17, loss = 0.40382581\n",
      "Iteration 18, loss = 0.40165228\n",
      "Iteration 19, loss = 0.39940753\n",
      "Iteration 20, loss = 0.39743568\n",
      "Iteration 21, loss = 0.39592132\n",
      "Iteration 22, loss = 0.39410751\n",
      "Iteration 23, loss = 0.39279915\n",
      "Iteration 24, loss = 0.39160752\n",
      "Iteration 25, loss = 0.38958462\n",
      "Iteration 26, loss = 0.38843203\n",
      "Iteration 27, loss = 0.38685801\n",
      "Iteration 28, loss = 0.38527215\n",
      "Iteration 29, loss = 0.38523978\n",
      "Iteration 30, loss = 0.38374734\n",
      "Iteration 31, loss = 0.38186677\n",
      "Iteration 32, loss = 0.38033593\n",
      "Iteration 33, loss = 0.37975519\n",
      "Iteration 34, loss = 0.37852654\n",
      "Iteration 35, loss = 0.37727456\n",
      "Iteration 36, loss = 0.37603715\n",
      "Iteration 37, loss = 0.37554865\n",
      "Iteration 38, loss = 0.37460558\n",
      "Iteration 39, loss = 0.37316104\n",
      "Iteration 40, loss = 0.37299943\n",
      "Iteration 41, loss = 0.37112095\n",
      "Iteration 42, loss = 0.37102656\n",
      "Iteration 43, loss = 0.36982679\n",
      "Iteration 44, loss = 0.36896214\n",
      "Iteration 45, loss = 0.36775232\n",
      "Iteration 46, loss = 0.36722859\n",
      "Iteration 47, loss = 0.36658220\n",
      "Iteration 48, loss = 0.36551710\n",
      "Iteration 49, loss = 0.36496221\n",
      "Iteration 50, loss = 0.36390407\n",
      "Iteration 51, loss = 0.36403062\n",
      "Iteration 52, loss = 0.36256074\n",
      "Iteration 53, loss = 0.36236567\n",
      "Iteration 54, loss = 0.36073902\n",
      "Iteration 55, loss = 0.35971734\n",
      "Iteration 56, loss = 0.35969285\n",
      "Iteration 57, loss = 0.35879473\n",
      "Iteration 58, loss = 0.35807256\n",
      "Iteration 59, loss = 0.35746942\n",
      "Iteration 60, loss = 0.35669371\n",
      "Iteration 61, loss = 0.35554173\n",
      "Iteration 62, loss = 0.35558096\n",
      "Iteration 63, loss = 0.35546718\n",
      "Iteration 64, loss = 0.35424527\n",
      "Iteration 65, loss = 0.35422304\n",
      "Iteration 66, loss = 0.35363005\n",
      "Iteration 67, loss = 0.35240688\n",
      "Iteration 68, loss = 0.35247643\n",
      "Iteration 69, loss = 0.35141561\n",
      "Iteration 70, loss = 0.35118253\n",
      "Iteration 71, loss = 0.35007476\n",
      "Iteration 72, loss = 0.34979792\n",
      "Iteration 73, loss = 0.34877255\n",
      "Iteration 74, loss = 0.34871538\n",
      "Iteration 75, loss = 0.34845706\n",
      "Iteration 76, loss = 0.34706144\n",
      "Iteration 77, loss = 0.34749990\n",
      "Iteration 78, loss = 0.34634106\n",
      "Iteration 79, loss = 0.34615268\n",
      "Iteration 80, loss = 0.34582211\n",
      "Iteration 81, loss = 0.34513754\n",
      "Iteration 82, loss = 0.34483142\n",
      "Iteration 83, loss = 0.34433216\n",
      "Iteration 84, loss = 0.34491825\n",
      "Iteration 85, loss = 0.34390424\n",
      "Iteration 86, loss = 0.34318337\n",
      "Iteration 87, loss = 0.34187538\n",
      "Iteration 88, loss = 0.34201967\n",
      "Iteration 89, loss = 0.34152696\n",
      "Iteration 90, loss = 0.34094238\n",
      "Iteration 91, loss = 0.34137715\n",
      "Iteration 92, loss = 0.34052225\n",
      "Iteration 93, loss = 0.34024969\n",
      "Iteration 94, loss = 0.33920290\n",
      "Iteration 95, loss = 0.33892764\n",
      "Iteration 96, loss = 0.33856389\n",
      "Iteration 97, loss = 0.33867663\n",
      "Iteration 98, loss = 0.33816484\n",
      "Iteration 99, loss = 0.33765089\n",
      "Iteration 100, loss = 0.33743381\n",
      "Iteration 101, loss = 0.33663391\n",
      "Iteration 102, loss = 0.33667860\n",
      "Iteration 103, loss = 0.33576244\n",
      "Iteration 104, loss = 0.33620066\n",
      "Iteration 105, loss = 0.33561141\n",
      "Iteration 106, loss = 0.33399867\n",
      "Iteration 107, loss = 0.33572791\n",
      "Iteration 108, loss = 0.33475792\n",
      "Iteration 109, loss = 0.33331297\n",
      "Iteration 110, loss = 0.33383332\n",
      "Iteration 111, loss = 0.33315396\n",
      "Iteration 112, loss = 0.33257313\n",
      "Iteration 113, loss = 0.33322740\n",
      "Iteration 114, loss = 0.33236127\n",
      "Iteration 115, loss = 0.33217501\n",
      "Iteration 116, loss = 0.33208480\n",
      "Iteration 117, loss = 0.33172994\n",
      "Iteration 118, loss = 0.33112707\n",
      "Iteration 119, loss = 0.33194370\n",
      "Iteration 120, loss = 0.33055077\n",
      "Iteration 121, loss = 0.33065261\n",
      "Iteration 122, loss = 0.32937892\n",
      "Iteration 123, loss = 0.32986157\n",
      "Iteration 124, loss = 0.32980337\n",
      "Iteration 125, loss = 0.32901891\n",
      "Iteration 126, loss = 0.32803865\n",
      "Iteration 127, loss = 0.32876441\n",
      "Iteration 128, loss = 0.32812573\n",
      "Iteration 129, loss = 0.32865859\n",
      "Iteration 130, loss = 0.32712804\n",
      "Iteration 131, loss = 0.32752432\n",
      "Iteration 132, loss = 0.32603306\n",
      "Iteration 133, loss = 0.32756567\n",
      "Iteration 134, loss = 0.32693250\n",
      "Iteration 135, loss = 0.32717313\n",
      "Iteration 136, loss = 0.32599473\n",
      "Iteration 137, loss = 0.32600005\n",
      "Iteration 138, loss = 0.32549752\n",
      "Iteration 139, loss = 0.32540171\n",
      "Iteration 140, loss = 0.32530411\n",
      "Iteration 141, loss = 0.32506948\n",
      "Iteration 142, loss = 0.32530950\n",
      "Iteration 143, loss = 0.32416955\n",
      "Iteration 144, loss = 0.32377832\n",
      "Iteration 145, loss = 0.32353205\n",
      "Iteration 146, loss = 0.32430837\n",
      "Iteration 147, loss = 0.32364861\n",
      "Iteration 148, loss = 0.32310020\n",
      "Iteration 149, loss = 0.32310290\n",
      "Iteration 150, loss = 0.32259034\n",
      "Iteration 151, loss = 0.32196258\n",
      "Iteration 152, loss = 0.32146698\n",
      "Iteration 153, loss = 0.32201594\n",
      "Iteration 154, loss = 0.32176619\n",
      "Iteration 155, loss = 0.32128632\n",
      "Iteration 156, loss = 0.32115094\n",
      "Iteration 157, loss = 0.32140617\n",
      "Iteration 158, loss = 0.32036909\n",
      "Iteration 159, loss = 0.32107949\n",
      "Iteration 160, loss = 0.31997412\n",
      "Iteration 161, loss = 0.31978049\n",
      "Iteration 162, loss = 0.31922575\n",
      "Iteration 163, loss = 0.32016327\n",
      "Iteration 164, loss = 0.32072275\n",
      "Iteration 165, loss = 0.31853480\n",
      "Iteration 166, loss = 0.31906873\n",
      "Iteration 167, loss = 0.31893316\n",
      "Iteration 168, loss = 0.31875564\n",
      "Iteration 169, loss = 0.31858663\n",
      "Iteration 170, loss = 0.31874870\n",
      "Iteration 171, loss = 0.31711057\n",
      "Iteration 172, loss = 0.31820065\n",
      "Iteration 173, loss = 0.31799045\n",
      "Iteration 174, loss = 0.31754846\n",
      "Iteration 175, loss = 0.31671169\n",
      "Iteration 176, loss = 0.31698197\n",
      "Iteration 177, loss = 0.31669381\n",
      "Iteration 178, loss = 0.31613403\n",
      "Iteration 179, loss = 0.31615095\n",
      "Iteration 180, loss = 0.31672431\n",
      "Iteration 181, loss = 0.31602798\n",
      "Iteration 182, loss = 0.31580092\n",
      "Iteration 183, loss = 0.31542130\n",
      "Iteration 184, loss = 0.31496941\n",
      "Iteration 185, loss = 0.31498375\n",
      "Iteration 186, loss = 0.31433353\n",
      "Iteration 187, loss = 0.31518566\n",
      "Iteration 188, loss = 0.31412956\n",
      "Iteration 189, loss = 0.31527113\n",
      "Iteration 190, loss = 0.31375786\n",
      "Iteration 191, loss = 0.31459284\n",
      "Iteration 192, loss = 0.31480398\n",
      "Iteration 193, loss = 0.31294892\n",
      "Iteration 194, loss = 0.31325053\n",
      "Iteration 195, loss = 0.31304976\n",
      "Iteration 196, loss = 0.31293086\n",
      "Iteration 197, loss = 0.31333654\n",
      "Iteration 198, loss = 0.31337569\n",
      "Iteration 199, loss = 0.31337854\n",
      "Iteration 200, loss = 0.31235718\n",
      "Iteration 201, loss = 0.31340320\n",
      "Iteration 202, loss = 0.31220242\n",
      "Iteration 203, loss = 0.31133671\n",
      "Iteration 204, loss = 0.31183903\n",
      "Iteration 205, loss = 0.31116038\n",
      "Iteration 206, loss = 0.31199523\n",
      "Iteration 207, loss = 0.31203511\n",
      "Iteration 208, loss = 0.31094279\n",
      "Iteration 209, loss = 0.31137602\n",
      "Iteration 210, loss = 0.31058589\n",
      "Iteration 211, loss = 0.31047323\n",
      "Iteration 212, loss = 0.31040581\n",
      "Iteration 213, loss = 0.31046677\n",
      "Iteration 214, loss = 0.31081355\n",
      "Iteration 215, loss = 0.30975690\n",
      "Iteration 216, loss = 0.31025871\n",
      "Iteration 217, loss = 0.30932973\n",
      "Iteration 218, loss = 0.30935972\n",
      "Iteration 219, loss = 0.30976196\n",
      "Iteration 220, loss = 0.30933539\n",
      "Iteration 221, loss = 0.30918413\n",
      "Iteration 222, loss = 0.30850591\n",
      "Iteration 223, loss = 0.30922459\n",
      "Iteration 224, loss = 0.30837815\n",
      "Iteration 225, loss = 0.30883838\n",
      "Iteration 226, loss = 0.30815416\n",
      "Iteration 227, loss = 0.30863474\n",
      "Iteration 228, loss = 0.30754472\n",
      "Iteration 229, loss = 0.30809852\n",
      "Iteration 230, loss = 0.30757445\n",
      "Iteration 231, loss = 0.30833047\n",
      "Iteration 232, loss = 0.30737641\n",
      "Iteration 233, loss = 0.30722525\n",
      "Iteration 234, loss = 0.30829205\n",
      "Iteration 235, loss = 0.30664373\n",
      "Iteration 236, loss = 0.30732312\n",
      "Iteration 237, loss = 0.30785643\n",
      "Iteration 238, loss = 0.30660154\n",
      "Iteration 239, loss = 0.30628630\n",
      "Iteration 240, loss = 0.30634004\n",
      "Iteration 241, loss = 0.30696319\n",
      "Iteration 242, loss = 0.30685522\n",
      "Iteration 243, loss = 0.30626061\n",
      "Iteration 244, loss = 0.30601497\n",
      "Iteration 245, loss = 0.30568244\n",
      "Iteration 246, loss = 0.30661417\n",
      "Iteration 247, loss = 0.30651192\n",
      "Iteration 248, loss = 0.30528763\n",
      "Iteration 249, loss = 0.30553221\n",
      "Iteration 250, loss = 0.30451876\n",
      "Iteration 251, loss = 0.30475251\n",
      "Iteration 252, loss = 0.30529119\n",
      "Iteration 253, loss = 0.30475915\n",
      "Iteration 254, loss = 0.30413334\n",
      "Iteration 255, loss = 0.30465633\n",
      "Iteration 256, loss = 0.30445329\n",
      "Iteration 257, loss = 0.30338699\n",
      "Iteration 258, loss = 0.30502624\n",
      "Iteration 259, loss = 0.30529940\n",
      "Iteration 260, loss = 0.30374904\n",
      "Iteration 261, loss = 0.30346197\n",
      "Iteration 262, loss = 0.30305774\n",
      "Iteration 263, loss = 0.30418233\n",
      "Iteration 264, loss = 0.30411831\n",
      "Iteration 265, loss = 0.30356686\n",
      "Iteration 266, loss = 0.30255774\n",
      "Iteration 267, loss = 0.30249737\n",
      "Iteration 268, loss = 0.30363758\n",
      "Iteration 269, loss = 0.30244220\n",
      "Iteration 270, loss = 0.30192519\n",
      "Iteration 271, loss = 0.30169521\n",
      "Iteration 272, loss = 0.30245870\n",
      "Iteration 273, loss = 0.30288668\n",
      "Iteration 274, loss = 0.30195251\n",
      "Iteration 275, loss = 0.30235272\n",
      "Iteration 276, loss = 0.30129820\n",
      "Iteration 277, loss = 0.30092994\n",
      "Iteration 278, loss = 0.30133955\n",
      "Iteration 279, loss = 0.30118096\n",
      "Iteration 280, loss = 0.30287043\n",
      "Iteration 281, loss = 0.30122018\n",
      "Iteration 282, loss = 0.30229254\n",
      "Iteration 283, loss = 0.30051897\n",
      "Iteration 284, loss = 0.30157277\n",
      "Iteration 285, loss = 0.30122082\n",
      "Iteration 286, loss = 0.30004697\n",
      "Iteration 287, loss = 0.30034016\n",
      "Iteration 288, loss = 0.29944713\n",
      "Iteration 289, loss = 0.29990947\n",
      "Iteration 290, loss = 0.30150507\n",
      "Iteration 291, loss = 0.30014501\n",
      "Iteration 292, loss = 0.30089960\n",
      "Iteration 293, loss = 0.30077902\n",
      "Iteration 294, loss = 0.30065919\n",
      "Iteration 295, loss = 0.29926007\n",
      "Iteration 296, loss = 0.29919345\n",
      "Iteration 297, loss = 0.29929914\n",
      "Iteration 298, loss = 0.29925439\n",
      "Iteration 299, loss = 0.30008122\n",
      "Iteration 300, loss = 0.30058688\n",
      "Iteration 301, loss = 0.29905092\n",
      "Iteration 302, loss = 0.29894253\n",
      "Iteration 303, loss = 0.30034735\n",
      "Iteration 304, loss = 0.30008047\n",
      "Iteration 305, loss = 0.29845625\n",
      "Iteration 306, loss = 0.29880857\n",
      "Iteration 307, loss = 0.29816240\n",
      "Iteration 308, loss = 0.29967432\n",
      "Iteration 309, loss = 0.29948397\n",
      "Iteration 310, loss = 0.29786962\n",
      "Iteration 311, loss = 0.30068074\n",
      "Iteration 312, loss = 0.29771043\n",
      "Iteration 313, loss = 0.29791719\n",
      "Iteration 314, loss = 0.29787349\n",
      "Iteration 315, loss = 0.29747631\n",
      "Iteration 316, loss = 0.29748071\n",
      "Iteration 317, loss = 0.29769944\n",
      "Iteration 318, loss = 0.29792638\n",
      "Iteration 319, loss = 0.29783002\n",
      "Iteration 320, loss = 0.29794078\n",
      "Iteration 321, loss = 0.29654253\n",
      "Iteration 322, loss = 0.29681417\n",
      "Iteration 323, loss = 0.29804153\n",
      "Iteration 324, loss = 0.29689262\n",
      "Iteration 325, loss = 0.29772873\n",
      "Iteration 326, loss = 0.29640296\n",
      "Iteration 327, loss = 0.29638556\n",
      "Iteration 328, loss = 0.29716980\n",
      "Iteration 329, loss = 0.29740007\n",
      "Iteration 330, loss = 0.29637760\n",
      "Iteration 331, loss = 0.29724721\n",
      "Iteration 332, loss = 0.29530595\n",
      "Iteration 333, loss = 0.29628721\n",
      "Iteration 334, loss = 0.29729431\n",
      "Iteration 335, loss = 0.29612137\n",
      "Iteration 336, loss = 0.29677762\n",
      "Iteration 337, loss = 0.29602083\n",
      "Iteration 338, loss = 0.29579958\n",
      "Iteration 339, loss = 0.29557698\n",
      "Iteration 340, loss = 0.29643175\n",
      "Iteration 341, loss = 0.29594877\n",
      "Iteration 342, loss = 0.29501519\n",
      "Iteration 343, loss = 0.29549126\n",
      "Iteration 344, loss = 0.29497656\n",
      "Iteration 345, loss = 0.29501148\n",
      "Iteration 346, loss = 0.29527217\n",
      "Iteration 347, loss = 0.29452534\n",
      "Iteration 348, loss = 0.29513016\n",
      "Iteration 349, loss = 0.29484970\n",
      "Iteration 350, loss = 0.29491266\n",
      "Iteration 351, loss = 0.29448729\n",
      "Iteration 352, loss = 0.29462211\n",
      "Iteration 353, loss = 0.29429693\n",
      "Iteration 354, loss = 0.29413198\n",
      "Iteration 355, loss = 0.29394905\n",
      "Iteration 356, loss = 0.29404307\n",
      "Iteration 357, loss = 0.29412306\n",
      "Iteration 358, loss = 0.29559865\n",
      "Iteration 359, loss = 0.29299566\n",
      "Iteration 360, loss = 0.29381659\n",
      "Iteration 361, loss = 0.29393311\n",
      "Iteration 362, loss = 0.29418069\n",
      "Iteration 363, loss = 0.29338943\n",
      "Iteration 364, loss = 0.29406082\n",
      "Iteration 365, loss = 0.29459957\n",
      "Iteration 366, loss = 0.29348468\n",
      "Iteration 367, loss = 0.29373576\n",
      "Iteration 368, loss = 0.29365052\n",
      "Iteration 369, loss = 0.29317168\n",
      "Iteration 370, loss = 0.29423987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51250161\n",
      "Iteration 2, loss = 0.46445959\n",
      "Iteration 3, loss = 0.45583655\n",
      "Iteration 4, loss = 0.45071530\n",
      "Iteration 5, loss = 0.44694944\n",
      "Iteration 6, loss = 0.44328111\n",
      "Iteration 7, loss = 0.43976187\n",
      "Iteration 8, loss = 0.43757244\n",
      "Iteration 9, loss = 0.43517720\n",
      "Iteration 10, loss = 0.43138346\n",
      "Iteration 11, loss = 0.42903990\n",
      "Iteration 12, loss = 0.42695181\n",
      "Iteration 13, loss = 0.42467142\n",
      "Iteration 14, loss = 0.42248061\n",
      "Iteration 15, loss = 0.42061395\n",
      "Iteration 16, loss = 0.41825996\n",
      "Iteration 17, loss = 0.41713331\n",
      "Iteration 18, loss = 0.41460557\n",
      "Iteration 19, loss = 0.41365604\n",
      "Iteration 20, loss = 0.41226559\n",
      "Iteration 21, loss = 0.41050805\n",
      "Iteration 22, loss = 0.40860960\n",
      "Iteration 23, loss = 0.40710032\n",
      "Iteration 24, loss = 0.40594381\n",
      "Iteration 25, loss = 0.40476488\n",
      "Iteration 26, loss = 0.40355420\n",
      "Iteration 27, loss = 0.40245963\n",
      "Iteration 28, loss = 0.40074646\n",
      "Iteration 29, loss = 0.39972502\n",
      "Iteration 30, loss = 0.39867648\n",
      "Iteration 31, loss = 0.39742828\n",
      "Iteration 32, loss = 0.39620080\n",
      "Iteration 33, loss = 0.39536647\n",
      "Iteration 34, loss = 0.39429451\n",
      "Iteration 35, loss = 0.39309807\n",
      "Iteration 36, loss = 0.39246815\n",
      "Iteration 37, loss = 0.39177823\n",
      "Iteration 38, loss = 0.39070229\n",
      "Iteration 39, loss = 0.38986444\n",
      "Iteration 40, loss = 0.38825362\n",
      "Iteration 41, loss = 0.38851574\n",
      "Iteration 42, loss = 0.38686298\n",
      "Iteration 43, loss = 0.38673982\n",
      "Iteration 44, loss = 0.38554124\n",
      "Iteration 45, loss = 0.38394525\n",
      "Iteration 46, loss = 0.38397643\n",
      "Iteration 47, loss = 0.38288457\n",
      "Iteration 48, loss = 0.38274619\n",
      "Iteration 49, loss = 0.38204365\n",
      "Iteration 50, loss = 0.38072682\n",
      "Iteration 51, loss = 0.38068654\n",
      "Iteration 52, loss = 0.37931988\n",
      "Iteration 53, loss = 0.37874661\n",
      "Iteration 54, loss = 0.37850617\n",
      "Iteration 55, loss = 0.37778317\n",
      "Iteration 56, loss = 0.37702434\n",
      "Iteration 57, loss = 0.37607236\n",
      "Iteration 58, loss = 0.37473845\n",
      "Iteration 59, loss = 0.37487388\n",
      "Iteration 60, loss = 0.37427959\n",
      "Iteration 61, loss = 0.37370695\n",
      "Iteration 62, loss = 0.37349736\n",
      "Iteration 63, loss = 0.37225899\n",
      "Iteration 64, loss = 0.37176628\n",
      "Iteration 65, loss = 0.37114120\n",
      "Iteration 66, loss = 0.37069635\n",
      "Iteration 67, loss = 0.37060374\n",
      "Iteration 68, loss = 0.36907377\n",
      "Iteration 69, loss = 0.36900990\n",
      "Iteration 70, loss = 0.36928077\n",
      "Iteration 71, loss = 0.36826834\n",
      "Iteration 72, loss = 0.36780781\n",
      "Iteration 73, loss = 0.36729921\n",
      "Iteration 74, loss = 0.36715981\n",
      "Iteration 75, loss = 0.36571037\n",
      "Iteration 76, loss = 0.36592436\n",
      "Iteration 77, loss = 0.36560301\n",
      "Iteration 78, loss = 0.36390898\n",
      "Iteration 79, loss = 0.36431900\n",
      "Iteration 80, loss = 0.36459404\n",
      "Iteration 81, loss = 0.36391469\n",
      "Iteration 82, loss = 0.36285305\n",
      "Iteration 83, loss = 0.36291468\n",
      "Iteration 84, loss = 0.36208603\n",
      "Iteration 85, loss = 0.36137997\n",
      "Iteration 86, loss = 0.36115327\n",
      "Iteration 87, loss = 0.36048159\n",
      "Iteration 88, loss = 0.36117784\n",
      "Iteration 89, loss = 0.35952468\n",
      "Iteration 90, loss = 0.36034716\n",
      "Iteration 91, loss = 0.35912686\n",
      "Iteration 92, loss = 0.35825538\n",
      "Iteration 93, loss = 0.35799960\n",
      "Iteration 94, loss = 0.35740899\n",
      "Iteration 95, loss = 0.35763691\n",
      "Iteration 96, loss = 0.35685095\n",
      "Iteration 97, loss = 0.35668700\n",
      "Iteration 98, loss = 0.35643119\n",
      "Iteration 99, loss = 0.35590235\n",
      "Iteration 100, loss = 0.35539691\n",
      "Iteration 101, loss = 0.35554392\n",
      "Iteration 102, loss = 0.35530331\n",
      "Iteration 103, loss = 0.35343750\n",
      "Iteration 104, loss = 0.35390519\n",
      "Iteration 105, loss = 0.35448362\n",
      "Iteration 106, loss = 0.35393868\n",
      "Iteration 107, loss = 0.35367550\n",
      "Iteration 108, loss = 0.35310866\n",
      "Iteration 109, loss = 0.35242840\n",
      "Iteration 110, loss = 0.35180309\n",
      "Iteration 111, loss = 0.35150163\n",
      "Iteration 112, loss = 0.35165342\n",
      "Iteration 113, loss = 0.35135066\n",
      "Iteration 114, loss = 0.35045633\n",
      "Iteration 115, loss = 0.35073978\n",
      "Iteration 116, loss = 0.35003697\n",
      "Iteration 117, loss = 0.34985103\n",
      "Iteration 118, loss = 0.34957046\n",
      "Iteration 119, loss = 0.34847618\n",
      "Iteration 120, loss = 0.34915225\n",
      "Iteration 121, loss = 0.34812352\n",
      "Iteration 122, loss = 0.34837686\n",
      "Iteration 123, loss = 0.34799723\n",
      "Iteration 124, loss = 0.34741248\n",
      "Iteration 125, loss = 0.34790574\n",
      "Iteration 126, loss = 0.34751612\n",
      "Iteration 127, loss = 0.34684335\n",
      "Iteration 128, loss = 0.34748700\n",
      "Iteration 129, loss = 0.34714472\n",
      "Iteration 130, loss = 0.34591487\n",
      "Iteration 131, loss = 0.34540125\n",
      "Iteration 132, loss = 0.34491061\n",
      "Iteration 133, loss = 0.34589300\n",
      "Iteration 134, loss = 0.34365134\n",
      "Iteration 135, loss = 0.34485284\n",
      "Iteration 136, loss = 0.34556274\n",
      "Iteration 137, loss = 0.34458887\n",
      "Iteration 138, loss = 0.34420220\n",
      "Iteration 139, loss = 0.34299375\n",
      "Iteration 140, loss = 0.34313552\n",
      "Iteration 141, loss = 0.34334465\n",
      "Iteration 142, loss = 0.34273856\n",
      "Iteration 143, loss = 0.34259356\n",
      "Iteration 144, loss = 0.34259341\n",
      "Iteration 145, loss = 0.34239794\n",
      "Iteration 146, loss = 0.34293710\n",
      "Iteration 147, loss = 0.34228036\n",
      "Iteration 148, loss = 0.34160047\n",
      "Iteration 149, loss = 0.34161471\n",
      "Iteration 150, loss = 0.34107304\n",
      "Iteration 151, loss = 0.34114496\n",
      "Iteration 152, loss = 0.34038634\n",
      "Iteration 153, loss = 0.34147106\n",
      "Iteration 154, loss = 0.34004178\n",
      "Iteration 155, loss = 0.34028453\n",
      "Iteration 156, loss = 0.33981281\n",
      "Iteration 157, loss = 0.33896066\n",
      "Iteration 158, loss = 0.33976136\n",
      "Iteration 159, loss = 0.33930181\n",
      "Iteration 160, loss = 0.33984173\n",
      "Iteration 161, loss = 0.33799590\n",
      "Iteration 162, loss = 0.33843947\n",
      "Iteration 163, loss = 0.33827099\n",
      "Iteration 164, loss = 0.33791362\n",
      "Iteration 165, loss = 0.33745986\n",
      "Iteration 166, loss = 0.33824420\n",
      "Iteration 167, loss = 0.33757496\n",
      "Iteration 168, loss = 0.33685432\n",
      "Iteration 169, loss = 0.33690551\n",
      "Iteration 170, loss = 0.33716788\n",
      "Iteration 171, loss = 0.33684398\n",
      "Iteration 172, loss = 0.33735800\n",
      "Iteration 173, loss = 0.33625437\n",
      "Iteration 174, loss = 0.33582759\n",
      "Iteration 175, loss = 0.33568624\n",
      "Iteration 176, loss = 0.33604694\n",
      "Iteration 177, loss = 0.33538323\n",
      "Iteration 178, loss = 0.33461608\n",
      "Iteration 179, loss = 0.33408703\n",
      "Iteration 180, loss = 0.33525905\n",
      "Iteration 181, loss = 0.33472017\n",
      "Iteration 182, loss = 0.33471012\n",
      "Iteration 183, loss = 0.33283579\n",
      "Iteration 184, loss = 0.33381032\n",
      "Iteration 185, loss = 0.33332435\n",
      "Iteration 186, loss = 0.33395383\n",
      "Iteration 187, loss = 0.33347894\n",
      "Iteration 188, loss = 0.33467827\n",
      "Iteration 189, loss = 0.33423995\n",
      "Iteration 190, loss = 0.33261121\n",
      "Iteration 191, loss = 0.33236375\n",
      "Iteration 192, loss = 0.33168705\n",
      "Iteration 193, loss = 0.33194684\n",
      "Iteration 194, loss = 0.33231528\n",
      "Iteration 195, loss = 0.33297799\n",
      "Iteration 196, loss = 0.33156301\n",
      "Iteration 197, loss = 0.33121734\n",
      "Iteration 198, loss = 0.33113493\n",
      "Iteration 199, loss = 0.33120502\n",
      "Iteration 200, loss = 0.32991201\n",
      "Iteration 201, loss = 0.33113605\n",
      "Iteration 202, loss = 0.33013552\n",
      "Iteration 203, loss = 0.33069040\n",
      "Iteration 204, loss = 0.33014667\n",
      "Iteration 205, loss = 0.32974379\n",
      "Iteration 206, loss = 0.33035295\n",
      "Iteration 207, loss = 0.32982907\n",
      "Iteration 208, loss = 0.32976878\n",
      "Iteration 209, loss = 0.32913392\n",
      "Iteration 210, loss = 0.33057083\n",
      "Iteration 211, loss = 0.32896681\n",
      "Iteration 212, loss = 0.32851844\n",
      "Iteration 213, loss = 0.32831539\n",
      "Iteration 214, loss = 0.32927863\n",
      "Iteration 215, loss = 0.32816757\n",
      "Iteration 216, loss = 0.32862862\n",
      "Iteration 217, loss = 0.32763530\n",
      "Iteration 218, loss = 0.32804852\n",
      "Iteration 219, loss = 0.32737610\n",
      "Iteration 220, loss = 0.32732690\n",
      "Iteration 221, loss = 0.32851151\n",
      "Iteration 222, loss = 0.32699291\n",
      "Iteration 223, loss = 0.32673529\n",
      "Iteration 224, loss = 0.32718414\n",
      "Iteration 225, loss = 0.32759729\n",
      "Iteration 226, loss = 0.32620764\n",
      "Iteration 227, loss = 0.32691404\n",
      "Iteration 228, loss = 0.32681862\n",
      "Iteration 229, loss = 0.32607979\n",
      "Iteration 230, loss = 0.32571469\n",
      "Iteration 231, loss = 0.32543949\n",
      "Iteration 232, loss = 0.32570252\n",
      "Iteration 233, loss = 0.32663623\n",
      "Iteration 234, loss = 0.32626140\n",
      "Iteration 235, loss = 0.32496177\n",
      "Iteration 236, loss = 0.32551740\n",
      "Iteration 237, loss = 0.32505229\n",
      "Iteration 238, loss = 0.32499131\n",
      "Iteration 239, loss = 0.32529274\n",
      "Iteration 240, loss = 0.32375725\n",
      "Iteration 241, loss = 0.32368072\n",
      "Iteration 242, loss = 0.32400705\n",
      "Iteration 243, loss = 0.32381136\n",
      "Iteration 244, loss = 0.32442451\n",
      "Iteration 245, loss = 0.32393053\n",
      "Iteration 246, loss = 0.32365629\n",
      "Iteration 247, loss = 0.32443924\n",
      "Iteration 248, loss = 0.32323637\n",
      "Iteration 249, loss = 0.32335427\n",
      "Iteration 250, loss = 0.32320509\n",
      "Iteration 251, loss = 0.32359786\n",
      "Iteration 252, loss = 0.32225563\n",
      "Iteration 253, loss = 0.32348057\n",
      "Iteration 254, loss = 0.32291617\n",
      "Iteration 255, loss = 0.32219340\n",
      "Iteration 256, loss = 0.32263668\n",
      "Iteration 257, loss = 0.32192221\n",
      "Iteration 258, loss = 0.32224169\n",
      "Iteration 259, loss = 0.32249921\n",
      "Iteration 260, loss = 0.32226077\n",
      "Iteration 261, loss = 0.32180517\n",
      "Iteration 262, loss = 0.32091520\n",
      "Iteration 263, loss = 0.32189130\n",
      "Iteration 264, loss = 0.32197624\n",
      "Iteration 265, loss = 0.32125146\n",
      "Iteration 266, loss = 0.32162843\n",
      "Iteration 267, loss = 0.31990578\n",
      "Iteration 268, loss = 0.32121091\n",
      "Iteration 269, loss = 0.32297680\n",
      "Iteration 270, loss = 0.31984697\n",
      "Iteration 271, loss = 0.32063536\n",
      "Iteration 272, loss = 0.32103245\n",
      "Iteration 273, loss = 0.32097295\n",
      "Iteration 274, loss = 0.31895482\n",
      "Iteration 275, loss = 0.31972972\n",
      "Iteration 276, loss = 0.32010649\n",
      "Iteration 277, loss = 0.32011781\n",
      "Iteration 278, loss = 0.31963269\n",
      "Iteration 279, loss = 0.31922675\n",
      "Iteration 280, loss = 0.32010359\n",
      "Iteration 281, loss = 0.31943690\n",
      "Iteration 282, loss = 0.31998753\n",
      "Iteration 283, loss = 0.31975205\n",
      "Iteration 284, loss = 0.31828714\n",
      "Iteration 285, loss = 0.31840205\n",
      "Iteration 286, loss = 0.31846399\n",
      "Iteration 287, loss = 0.31821951\n",
      "Iteration 288, loss = 0.31997757\n",
      "Iteration 289, loss = 0.31808920\n",
      "Iteration 290, loss = 0.31911066\n",
      "Iteration 291, loss = 0.31914836\n",
      "Iteration 292, loss = 0.31765866\n",
      "Iteration 293, loss = 0.31843419\n",
      "Iteration 294, loss = 0.31782779\n",
      "Iteration 295, loss = 0.31885667\n",
      "Iteration 296, loss = 0.31981389\n",
      "Iteration 297, loss = 0.31675999\n",
      "Iteration 298, loss = 0.31899466\n",
      "Iteration 299, loss = 0.31694500\n",
      "Iteration 300, loss = 0.31696703\n",
      "Iteration 301, loss = 0.31727394\n",
      "Iteration 302, loss = 0.31729946\n",
      "Iteration 303, loss = 0.31676265\n",
      "Iteration 304, loss = 0.31762677\n",
      "Iteration 305, loss = 0.31700531\n",
      "Iteration 306, loss = 0.31714957\n",
      "Iteration 307, loss = 0.31622278\n",
      "Iteration 308, loss = 0.31610289\n",
      "Iteration 309, loss = 0.31627447\n",
      "Iteration 310, loss = 0.31674381\n",
      "Iteration 311, loss = 0.31598270\n",
      "Iteration 312, loss = 0.31631005\n",
      "Iteration 313, loss = 0.31638186\n",
      "Iteration 314, loss = 0.31548683\n",
      "Iteration 315, loss = 0.31606757\n",
      "Iteration 316, loss = 0.31567163\n",
      "Iteration 317, loss = 0.31651923\n",
      "Iteration 318, loss = 0.31542673\n",
      "Iteration 319, loss = 0.31576444\n",
      "Iteration 320, loss = 0.31563775\n",
      "Iteration 321, loss = 0.31588176\n",
      "Iteration 322, loss = 0.31495449\n",
      "Iteration 323, loss = 0.31538603\n",
      "Iteration 324, loss = 0.31562803\n",
      "Iteration 325, loss = 0.31565744\n",
      "Iteration 326, loss = 0.31447937\n",
      "Iteration 327, loss = 0.31560662\n",
      "Iteration 328, loss = 0.31480168\n",
      "Iteration 329, loss = 0.31467183\n",
      "Iteration 330, loss = 0.31428996\n",
      "Iteration 331, loss = 0.31440911\n",
      "Iteration 332, loss = 0.31394985\n",
      "Iteration 333, loss = 0.31404806\n",
      "Iteration 334, loss = 0.31467748\n",
      "Iteration 335, loss = 0.31493747\n",
      "Iteration 336, loss = 0.31395514\n",
      "Iteration 337, loss = 0.31404273\n",
      "Iteration 338, loss = 0.31330786\n",
      "Iteration 339, loss = 0.31428416\n",
      "Iteration 340, loss = 0.31364877\n",
      "Iteration 341, loss = 0.31347935\n",
      "Iteration 342, loss = 0.31299751\n",
      "Iteration 343, loss = 0.31364900\n",
      "Iteration 344, loss = 0.31332844\n",
      "Iteration 345, loss = 0.31372017\n",
      "Iteration 346, loss = 0.31294030\n",
      "Iteration 347, loss = 0.31424901\n",
      "Iteration 348, loss = 0.31233587\n",
      "Iteration 349, loss = 0.31333307\n",
      "Iteration 350, loss = 0.31252169\n",
      "Iteration 351, loss = 0.31242401\n",
      "Iteration 352, loss = 0.31277919\n",
      "Iteration 353, loss = 0.31290332\n",
      "Iteration 354, loss = 0.31289000\n",
      "Iteration 355, loss = 0.31176425\n",
      "Iteration 356, loss = 0.31338538\n",
      "Iteration 357, loss = 0.31214744\n",
      "Iteration 358, loss = 0.31208851\n",
      "Iteration 359, loss = 0.31203050\n",
      "Iteration 360, loss = 0.31158742\n",
      "Iteration 361, loss = 0.31229703\n",
      "Iteration 362, loss = 0.31100357\n",
      "Iteration 363, loss = 0.31244308\n",
      "Iteration 364, loss = 0.31205354\n",
      "Iteration 365, loss = 0.31071257\n",
      "Iteration 366, loss = 0.31155376\n",
      "Iteration 367, loss = 0.31179800\n",
      "Iteration 368, loss = 0.31201912\n",
      "Iteration 369, loss = 0.31106020\n",
      "Iteration 370, loss = 0.31169551\n",
      "Iteration 371, loss = 0.31099590\n",
      "Iteration 372, loss = 0.31073457\n",
      "Iteration 373, loss = 0.31033978\n",
      "Iteration 374, loss = 0.31114471\n",
      "Iteration 375, loss = 0.31011620\n",
      "Iteration 376, loss = 0.31143565\n",
      "Iteration 377, loss = 0.31095455\n",
      "Iteration 378, loss = 0.31141097\n",
      "Iteration 379, loss = 0.31020407\n",
      "Iteration 380, loss = 0.31046822\n",
      "Iteration 381, loss = 0.30928823\n",
      "Iteration 382, loss = 0.31130992\n",
      "Iteration 383, loss = 0.31052484\n",
      "Iteration 384, loss = 0.31181416\n",
      "Iteration 385, loss = 0.30983382\n",
      "Iteration 386, loss = 0.31012000\n",
      "Iteration 387, loss = 0.31016158\n",
      "Iteration 388, loss = 0.31057749\n",
      "Iteration 389, loss = 0.30969337\n",
      "Iteration 390, loss = 0.30925015\n",
      "Iteration 391, loss = 0.31032776\n",
      "Iteration 392, loss = 0.30960219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51832551\n",
      "Iteration 2, loss = 0.46850216\n",
      "Iteration 3, loss = 0.46006144\n",
      "Iteration 4, loss = 0.45522731\n",
      "Iteration 5, loss = 0.45126031\n",
      "Iteration 6, loss = 0.44748273\n",
      "Iteration 7, loss = 0.44421218\n",
      "Iteration 8, loss = 0.44159363\n",
      "Iteration 9, loss = 0.43849192\n",
      "Iteration 10, loss = 0.43606569\n",
      "Iteration 11, loss = 0.43401376\n",
      "Iteration 12, loss = 0.43161999\n",
      "Iteration 13, loss = 0.42942426\n",
      "Iteration 14, loss = 0.42809564\n",
      "Iteration 15, loss = 0.42604209\n",
      "Iteration 16, loss = 0.42452987\n",
      "Iteration 17, loss = 0.42320716\n",
      "Iteration 18, loss = 0.42157164\n",
      "Iteration 19, loss = 0.42000845\n",
      "Iteration 20, loss = 0.41899445\n",
      "Iteration 21, loss = 0.41724674\n",
      "Iteration 22, loss = 0.41647319\n",
      "Iteration 23, loss = 0.41499494\n",
      "Iteration 24, loss = 0.41454919\n",
      "Iteration 25, loss = 0.41270996\n",
      "Iteration 26, loss = 0.41230002\n",
      "Iteration 27, loss = 0.41094914\n",
      "Iteration 28, loss = 0.41015320\n",
      "Iteration 29, loss = 0.40882744\n",
      "Iteration 30, loss = 0.40852924\n",
      "Iteration 31, loss = 0.40732780\n",
      "Iteration 32, loss = 0.40613127\n",
      "Iteration 33, loss = 0.40504326\n",
      "Iteration 34, loss = 0.40447042\n",
      "Iteration 35, loss = 0.40364851\n",
      "Iteration 36, loss = 0.40299393\n",
      "Iteration 37, loss = 0.40119646\n",
      "Iteration 38, loss = 0.40080322\n",
      "Iteration 39, loss = 0.40001279\n",
      "Iteration 40, loss = 0.39872091\n",
      "Iteration 41, loss = 0.39870329\n",
      "Iteration 42, loss = 0.39752001\n",
      "Iteration 43, loss = 0.39723613\n",
      "Iteration 44, loss = 0.39602037\n",
      "Iteration 45, loss = 0.39483532\n",
      "Iteration 46, loss = 0.39453245\n",
      "Iteration 47, loss = 0.39323328\n",
      "Iteration 48, loss = 0.39255224\n",
      "Iteration 49, loss = 0.39176747\n",
      "Iteration 50, loss = 0.39109989\n",
      "Iteration 51, loss = 0.39128619\n",
      "Iteration 52, loss = 0.38930732\n",
      "Iteration 53, loss = 0.38886113\n",
      "Iteration 54, loss = 0.38803335\n",
      "Iteration 55, loss = 0.38729097\n",
      "Iteration 56, loss = 0.38753772\n",
      "Iteration 57, loss = 0.38567085\n",
      "Iteration 58, loss = 0.38553288\n",
      "Iteration 59, loss = 0.38436144\n",
      "Iteration 60, loss = 0.38339108\n",
      "Iteration 61, loss = 0.38415692\n",
      "Iteration 62, loss = 0.38283499\n",
      "Iteration 63, loss = 0.38310544\n",
      "Iteration 64, loss = 0.38200487\n",
      "Iteration 65, loss = 0.38000197\n",
      "Iteration 66, loss = 0.38083676\n",
      "Iteration 67, loss = 0.38085087\n",
      "Iteration 68, loss = 0.37978030\n",
      "Iteration 69, loss = 0.37895974\n",
      "Iteration 70, loss = 0.37867496\n",
      "Iteration 71, loss = 0.37844663\n",
      "Iteration 72, loss = 0.37714476\n",
      "Iteration 73, loss = 0.37670533\n",
      "Iteration 74, loss = 0.37681131\n",
      "Iteration 75, loss = 0.37558853\n",
      "Iteration 76, loss = 0.37582179\n",
      "Iteration 77, loss = 0.37536254\n",
      "Iteration 78, loss = 0.37407962\n",
      "Iteration 79, loss = 0.37409171\n",
      "Iteration 80, loss = 0.37419552\n",
      "Iteration 81, loss = 0.37330209\n",
      "Iteration 82, loss = 0.37286201\n",
      "Iteration 83, loss = 0.37265376\n",
      "Iteration 84, loss = 0.37187510\n",
      "Iteration 85, loss = 0.37156989\n",
      "Iteration 86, loss = 0.37135264\n",
      "Iteration 87, loss = 0.37082643\n",
      "Iteration 88, loss = 0.37001388\n",
      "Iteration 89, loss = 0.36939034\n",
      "Iteration 90, loss = 0.36986089\n",
      "Iteration 91, loss = 0.36974986\n",
      "Iteration 92, loss = 0.36768098\n",
      "Iteration 93, loss = 0.36818251\n",
      "Iteration 94, loss = 0.36796745\n",
      "Iteration 95, loss = 0.36712138\n",
      "Iteration 96, loss = 0.36737590\n",
      "Iteration 97, loss = 0.36644918\n",
      "Iteration 98, loss = 0.36610779\n",
      "Iteration 99, loss = 0.36624488\n",
      "Iteration 100, loss = 0.36492707\n",
      "Iteration 101, loss = 0.36451688\n",
      "Iteration 102, loss = 0.36530873\n",
      "Iteration 103, loss = 0.36410632\n",
      "Iteration 104, loss = 0.36349425\n",
      "Iteration 105, loss = 0.36388435\n",
      "Iteration 106, loss = 0.36285562\n",
      "Iteration 107, loss = 0.36354136\n",
      "Iteration 108, loss = 0.36291962\n",
      "Iteration 109, loss = 0.36210751\n",
      "Iteration 110, loss = 0.36081828\n",
      "Iteration 111, loss = 0.36200573\n",
      "Iteration 112, loss = 0.36142364\n",
      "Iteration 113, loss = 0.36084208\n",
      "Iteration 114, loss = 0.36062736\n",
      "Iteration 115, loss = 0.36029854\n",
      "Iteration 116, loss = 0.35973698\n",
      "Iteration 117, loss = 0.36019658\n",
      "Iteration 118, loss = 0.35897770\n",
      "Iteration 119, loss = 0.35958630\n",
      "Iteration 120, loss = 0.35877190\n",
      "Iteration 121, loss = 0.35792768\n",
      "Iteration 122, loss = 0.35806430\n",
      "Iteration 123, loss = 0.35710559\n",
      "Iteration 124, loss = 0.35731540\n",
      "Iteration 125, loss = 0.35732307\n",
      "Iteration 126, loss = 0.35646448\n",
      "Iteration 127, loss = 0.35649704\n",
      "Iteration 128, loss = 0.35608154\n",
      "Iteration 129, loss = 0.35603249\n",
      "Iteration 130, loss = 0.35588149\n",
      "Iteration 131, loss = 0.35584486\n",
      "Iteration 132, loss = 0.35491637\n",
      "Iteration 133, loss = 0.35468097\n",
      "Iteration 134, loss = 0.35401251\n",
      "Iteration 135, loss = 0.35440936\n",
      "Iteration 136, loss = 0.35578419\n",
      "Iteration 137, loss = 0.35449778\n",
      "Iteration 138, loss = 0.35376588\n",
      "Iteration 139, loss = 0.35315430\n",
      "Iteration 140, loss = 0.35209322\n",
      "Iteration 141, loss = 0.35331616\n",
      "Iteration 142, loss = 0.35353379\n",
      "Iteration 143, loss = 0.35198492\n",
      "Iteration 144, loss = 0.35241607\n",
      "Iteration 145, loss = 0.35142481\n",
      "Iteration 146, loss = 0.35131150\n",
      "Iteration 147, loss = 0.35113254\n",
      "Iteration 148, loss = 0.35076670\n",
      "Iteration 149, loss = 0.35167695\n",
      "Iteration 150, loss = 0.35112235\n",
      "Iteration 151, loss = 0.35076981\n",
      "Iteration 152, loss = 0.35079668\n",
      "Iteration 153, loss = 0.34953305\n",
      "Iteration 154, loss = 0.34962242\n",
      "Iteration 155, loss = 0.34978112\n",
      "Iteration 156, loss = 0.34942298\n",
      "Iteration 157, loss = 0.34859240\n",
      "Iteration 158, loss = 0.35001807\n",
      "Iteration 159, loss = 0.34812491\n",
      "Iteration 160, loss = 0.34899494\n",
      "Iteration 161, loss = 0.34875971\n",
      "Iteration 162, loss = 0.34888782\n",
      "Iteration 163, loss = 0.34856352\n",
      "Iteration 164, loss = 0.34727085\n",
      "Iteration 165, loss = 0.34736068\n",
      "Iteration 166, loss = 0.34787824\n",
      "Iteration 167, loss = 0.34744016\n",
      "Iteration 168, loss = 0.34708396\n",
      "Iteration 169, loss = 0.34595615\n",
      "Iteration 170, loss = 0.34733473\n",
      "Iteration 171, loss = 0.34568770\n",
      "Iteration 172, loss = 0.34581301\n",
      "Iteration 173, loss = 0.34576130\n",
      "Iteration 174, loss = 0.34603893\n",
      "Iteration 175, loss = 0.34574353\n",
      "Iteration 176, loss = 0.34493582\n",
      "Iteration 177, loss = 0.34496926\n",
      "Iteration 178, loss = 0.34535181\n",
      "Iteration 179, loss = 0.34450233\n",
      "Iteration 180, loss = 0.34449049\n",
      "Iteration 181, loss = 0.34420912\n",
      "Iteration 182, loss = 0.34286240\n",
      "Iteration 183, loss = 0.34483195\n",
      "Iteration 184, loss = 0.34499405\n",
      "Iteration 185, loss = 0.34365775\n",
      "Iteration 186, loss = 0.34387066\n",
      "Iteration 187, loss = 0.34323638\n",
      "Iteration 188, loss = 0.34320785\n",
      "Iteration 189, loss = 0.34297059\n",
      "Iteration 190, loss = 0.34309679\n",
      "Iteration 191, loss = 0.34342767\n",
      "Iteration 192, loss = 0.34233492\n",
      "Iteration 193, loss = 0.34215449\n",
      "Iteration 194, loss = 0.34236747\n",
      "Iteration 195, loss = 0.34168154\n",
      "Iteration 196, loss = 0.34266508\n",
      "Iteration 197, loss = 0.34109668\n",
      "Iteration 198, loss = 0.34158016\n",
      "Iteration 199, loss = 0.34120466\n",
      "Iteration 200, loss = 0.34199428\n",
      "Iteration 201, loss = 0.34153132\n",
      "Iteration 202, loss = 0.34051378\n",
      "Iteration 203, loss = 0.34129405\n",
      "Iteration 204, loss = 0.34131972\n",
      "Iteration 205, loss = 0.34040691\n",
      "Iteration 206, loss = 0.33993339\n",
      "Iteration 207, loss = 0.33993281\n",
      "Iteration 208, loss = 0.34050308\n",
      "Iteration 209, loss = 0.34029928\n",
      "Iteration 210, loss = 0.34059980\n",
      "Iteration 211, loss = 0.33927001\n",
      "Iteration 212, loss = 0.33824600\n",
      "Iteration 213, loss = 0.33938675\n",
      "Iteration 214, loss = 0.33869478\n",
      "Iteration 215, loss = 0.33944849\n",
      "Iteration 216, loss = 0.33932864\n",
      "Iteration 217, loss = 0.33813893\n",
      "Iteration 218, loss = 0.33856658\n",
      "Iteration 219, loss = 0.33862708\n",
      "Iteration 220, loss = 0.33858400\n",
      "Iteration 221, loss = 0.33832180\n",
      "Iteration 222, loss = 0.33719037\n",
      "Iteration 223, loss = 0.33710100\n",
      "Iteration 224, loss = 0.33713414\n",
      "Iteration 225, loss = 0.33735308\n",
      "Iteration 226, loss = 0.33834344\n",
      "Iteration 227, loss = 0.33742572\n",
      "Iteration 228, loss = 0.33748274\n",
      "Iteration 229, loss = 0.33681454\n",
      "Iteration 230, loss = 0.33729385\n",
      "Iteration 231, loss = 0.33651010\n",
      "Iteration 232, loss = 0.33718896\n",
      "Iteration 233, loss = 0.33619779\n",
      "Iteration 234, loss = 0.33620245\n",
      "Iteration 235, loss = 0.33594882\n",
      "Iteration 236, loss = 0.33579413\n",
      "Iteration 237, loss = 0.33589322\n",
      "Iteration 238, loss = 0.33525435\n",
      "Iteration 239, loss = 0.33527705\n",
      "Iteration 240, loss = 0.33557563\n",
      "Iteration 241, loss = 0.33463461\n",
      "Iteration 242, loss = 0.33511978\n",
      "Iteration 243, loss = 0.33625885\n",
      "Iteration 244, loss = 0.33564200\n",
      "Iteration 245, loss = 0.33511174\n",
      "Iteration 246, loss = 0.33462492\n",
      "Iteration 247, loss = 0.33481451\n",
      "Iteration 248, loss = 0.33493783\n",
      "Iteration 249, loss = 0.33458706\n",
      "Iteration 250, loss = 0.33338115\n",
      "Iteration 251, loss = 0.33443514\n",
      "Iteration 252, loss = 0.33375978\n",
      "Iteration 253, loss = 0.33410568\n",
      "Iteration 254, loss = 0.33441463\n",
      "Iteration 255, loss = 0.33353638\n",
      "Iteration 256, loss = 0.33457603\n",
      "Iteration 257, loss = 0.33460492\n",
      "Iteration 258, loss = 0.33319710\n",
      "Iteration 259, loss = 0.33297811\n",
      "Iteration 260, loss = 0.33352167\n",
      "Iteration 261, loss = 0.33348451\n",
      "Iteration 262, loss = 0.33397254\n",
      "Iteration 263, loss = 0.33276611\n",
      "Iteration 264, loss = 0.33335023\n",
      "Iteration 265, loss = 0.33184842\n",
      "Iteration 266, loss = 0.33214016\n",
      "Iteration 267, loss = 0.33247392\n",
      "Iteration 268, loss = 0.33233764\n",
      "Iteration 269, loss = 0.33286509\n",
      "Iteration 270, loss = 0.33165877\n",
      "Iteration 271, loss = 0.33177304\n",
      "Iteration 272, loss = 0.33165573\n",
      "Iteration 273, loss = 0.33259387\n",
      "Iteration 274, loss = 0.33142706\n",
      "Iteration 275, loss = 0.33252788\n",
      "Iteration 276, loss = 0.33119810\n",
      "Iteration 277, loss = 0.33152417\n",
      "Iteration 278, loss = 0.33171788\n",
      "Iteration 279, loss = 0.32950745\n",
      "Iteration 280, loss = 0.33149840\n",
      "Iteration 281, loss = 0.33096415\n",
      "Iteration 282, loss = 0.33141883\n",
      "Iteration 283, loss = 0.33022270\n",
      "Iteration 284, loss = 0.33325258\n",
      "Iteration 285, loss = 0.33039742\n",
      "Iteration 286, loss = 0.33002538\n",
      "Iteration 287, loss = 0.33118121\n",
      "Iteration 288, loss = 0.32954272\n",
      "Iteration 289, loss = 0.33071225\n",
      "Iteration 290, loss = 0.33044984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51738123\n",
      "Iteration 2, loss = 0.46743061\n",
      "Iteration 3, loss = 0.45807648\n",
      "Iteration 4, loss = 0.45183630\n",
      "Iteration 5, loss = 0.44702795\n",
      "Iteration 6, loss = 0.44301533\n",
      "Iteration 7, loss = 0.43963026\n",
      "Iteration 8, loss = 0.43676543\n",
      "Iteration 9, loss = 0.43363267\n",
      "Iteration 10, loss = 0.43158395\n",
      "Iteration 11, loss = 0.42824390\n",
      "Iteration 12, loss = 0.42618970\n",
      "Iteration 13, loss = 0.42361609\n",
      "Iteration 14, loss = 0.42168982\n",
      "Iteration 15, loss = 0.41974114\n",
      "Iteration 16, loss = 0.41726276\n",
      "Iteration 17, loss = 0.41583911\n",
      "Iteration 18, loss = 0.41399318\n",
      "Iteration 19, loss = 0.41255366\n",
      "Iteration 20, loss = 0.41067887\n",
      "Iteration 21, loss = 0.40892337\n",
      "Iteration 22, loss = 0.40798068\n",
      "Iteration 23, loss = 0.40592976\n",
      "Iteration 24, loss = 0.40551088\n",
      "Iteration 25, loss = 0.40362655\n",
      "Iteration 26, loss = 0.40247392\n",
      "Iteration 27, loss = 0.40114599\n",
      "Iteration 28, loss = 0.40000715\n",
      "Iteration 29, loss = 0.39885712\n",
      "Iteration 30, loss = 0.39831750\n",
      "Iteration 31, loss = 0.39658546\n",
      "Iteration 32, loss = 0.39626497\n",
      "Iteration 33, loss = 0.39509698\n",
      "Iteration 34, loss = 0.39422328\n",
      "Iteration 35, loss = 0.39351798\n",
      "Iteration 36, loss = 0.39193304\n",
      "Iteration 37, loss = 0.39170615\n",
      "Iteration 38, loss = 0.39068941\n",
      "Iteration 39, loss = 0.38973799\n",
      "Iteration 40, loss = 0.38873073\n",
      "Iteration 41, loss = 0.38816014\n",
      "Iteration 42, loss = 0.38784190\n",
      "Iteration 43, loss = 0.38702281\n",
      "Iteration 44, loss = 0.38633282\n",
      "Iteration 45, loss = 0.38505842\n",
      "Iteration 46, loss = 0.38457335\n",
      "Iteration 47, loss = 0.38469883\n",
      "Iteration 48, loss = 0.38319339\n",
      "Iteration 49, loss = 0.38268201\n",
      "Iteration 50, loss = 0.38166399\n",
      "Iteration 51, loss = 0.38165010\n",
      "Iteration 52, loss = 0.38090512\n",
      "Iteration 53, loss = 0.38059718\n",
      "Iteration 54, loss = 0.37973301\n",
      "Iteration 55, loss = 0.37883186\n",
      "Iteration 56, loss = 0.37830710\n",
      "Iteration 57, loss = 0.37841485\n",
      "Iteration 58, loss = 0.37760668\n",
      "Iteration 59, loss = 0.37731256\n",
      "Iteration 60, loss = 0.37637471\n",
      "Iteration 61, loss = 0.37605613\n",
      "Iteration 62, loss = 0.37610304\n",
      "Iteration 63, loss = 0.37471477\n",
      "Iteration 64, loss = 0.37372891\n",
      "Iteration 65, loss = 0.37346056\n",
      "Iteration 66, loss = 0.37334802\n",
      "Iteration 67, loss = 0.37303170\n",
      "Iteration 68, loss = 0.37238827\n",
      "Iteration 69, loss = 0.37170591\n",
      "Iteration 70, loss = 0.37197754\n",
      "Iteration 71, loss = 0.37095255\n",
      "Iteration 72, loss = 0.37077625\n",
      "Iteration 73, loss = 0.37055406\n",
      "Iteration 74, loss = 0.36902090\n",
      "Iteration 75, loss = 0.36926057\n",
      "Iteration 76, loss = 0.36821332\n",
      "Iteration 77, loss = 0.36890680\n",
      "Iteration 78, loss = 0.36865747\n",
      "Iteration 79, loss = 0.36749600\n",
      "Iteration 80, loss = 0.36709222\n",
      "Iteration 81, loss = 0.36683658\n",
      "Iteration 82, loss = 0.36602167\n",
      "Iteration 83, loss = 0.36629360\n",
      "Iteration 84, loss = 0.36556101\n",
      "Iteration 85, loss = 0.36550449\n",
      "Iteration 86, loss = 0.36427072\n",
      "Iteration 87, loss = 0.36484672\n",
      "Iteration 88, loss = 0.36348117\n",
      "Iteration 89, loss = 0.36367895\n",
      "Iteration 90, loss = 0.36306010\n",
      "Iteration 91, loss = 0.36353995\n",
      "Iteration 92, loss = 0.36276981\n",
      "Iteration 93, loss = 0.36209444\n",
      "Iteration 94, loss = 0.36212143\n",
      "Iteration 95, loss = 0.36110665\n",
      "Iteration 96, loss = 0.36090954\n",
      "Iteration 97, loss = 0.36071962\n",
      "Iteration 98, loss = 0.36069652\n",
      "Iteration 99, loss = 0.35961929\n",
      "Iteration 100, loss = 0.36003065\n",
      "Iteration 101, loss = 0.36016472\n",
      "Iteration 102, loss = 0.35886592\n",
      "Iteration 103, loss = 0.35847566\n",
      "Iteration 104, loss = 0.35820107\n",
      "Iteration 105, loss = 0.35807480\n",
      "Iteration 106, loss = 0.35754133\n",
      "Iteration 107, loss = 0.35700660\n",
      "Iteration 108, loss = 0.35697040\n",
      "Iteration 109, loss = 0.35775099\n",
      "Iteration 110, loss = 0.35615399\n",
      "Iteration 111, loss = 0.35652056\n",
      "Iteration 112, loss = 0.35579075\n",
      "Iteration 113, loss = 0.35492315\n",
      "Iteration 114, loss = 0.35448622\n",
      "Iteration 115, loss = 0.35489894\n",
      "Iteration 116, loss = 0.35383491\n",
      "Iteration 117, loss = 0.35357132\n",
      "Iteration 118, loss = 0.35473506\n",
      "Iteration 119, loss = 0.35375530\n",
      "Iteration 120, loss = 0.35254714\n",
      "Iteration 121, loss = 0.35244388\n",
      "Iteration 122, loss = 0.35210014\n",
      "Iteration 123, loss = 0.35231588\n",
      "Iteration 124, loss = 0.35269839\n",
      "Iteration 125, loss = 0.35150721\n",
      "Iteration 126, loss = 0.35228331\n",
      "Iteration 127, loss = 0.35152200\n",
      "Iteration 128, loss = 0.35184139\n",
      "Iteration 129, loss = 0.35098570\n",
      "Iteration 130, loss = 0.35065718\n",
      "Iteration 131, loss = 0.34984604\n",
      "Iteration 132, loss = 0.34937728\n",
      "Iteration 133, loss = 0.34991907\n",
      "Iteration 134, loss = 0.34976680\n",
      "Iteration 135, loss = 0.34932202\n",
      "Iteration 136, loss = 0.34876951\n",
      "Iteration 137, loss = 0.34875911\n",
      "Iteration 138, loss = 0.34851043\n",
      "Iteration 139, loss = 0.34862950\n",
      "Iteration 140, loss = 0.34842645\n",
      "Iteration 141, loss = 0.34843781\n",
      "Iteration 142, loss = 0.34661570\n",
      "Iteration 143, loss = 0.34764279\n",
      "Iteration 144, loss = 0.34738737\n",
      "Iteration 145, loss = 0.34716892\n",
      "Iteration 146, loss = 0.34720174\n",
      "Iteration 147, loss = 0.34658839\n",
      "Iteration 148, loss = 0.34703666\n",
      "Iteration 149, loss = 0.34680639\n",
      "Iteration 150, loss = 0.34641662\n",
      "Iteration 151, loss = 0.34542975\n",
      "Iteration 152, loss = 0.34505243\n",
      "Iteration 153, loss = 0.34495584\n",
      "Iteration 154, loss = 0.34534082\n",
      "Iteration 155, loss = 0.34603065\n",
      "Iteration 156, loss = 0.34484801\n",
      "Iteration 157, loss = 0.34609373\n",
      "Iteration 158, loss = 0.34522671\n",
      "Iteration 159, loss = 0.34487193\n",
      "Iteration 160, loss = 0.34542159\n",
      "Iteration 161, loss = 0.34419382\n",
      "Iteration 162, loss = 0.34395789\n",
      "Iteration 163, loss = 0.34348155\n",
      "Iteration 164, loss = 0.34362428\n",
      "Iteration 165, loss = 0.34282774\n",
      "Iteration 166, loss = 0.34355568\n",
      "Iteration 167, loss = 0.34424022\n",
      "Iteration 168, loss = 0.34299755\n",
      "Iteration 169, loss = 0.34280420\n",
      "Iteration 170, loss = 0.34215335\n",
      "Iteration 171, loss = 0.34188646\n",
      "Iteration 172, loss = 0.34215416\n",
      "Iteration 173, loss = 0.34237637\n",
      "Iteration 174, loss = 0.34258806\n",
      "Iteration 175, loss = 0.34257763\n",
      "Iteration 176, loss = 0.34209691\n",
      "Iteration 177, loss = 0.34079047\n",
      "Iteration 178, loss = 0.34192949\n",
      "Iteration 179, loss = 0.34012242\n",
      "Iteration 180, loss = 0.34046293\n",
      "Iteration 181, loss = 0.34096721\n",
      "Iteration 182, loss = 0.34101482\n",
      "Iteration 183, loss = 0.34107606\n",
      "Iteration 184, loss = 0.33978584\n",
      "Iteration 185, loss = 0.34079877\n",
      "Iteration 186, loss = 0.33977681\n",
      "Iteration 187, loss = 0.34028310\n",
      "Iteration 188, loss = 0.33928042\n",
      "Iteration 189, loss = 0.33945553\n",
      "Iteration 190, loss = 0.33935600\n",
      "Iteration 191, loss = 0.33826308\n",
      "Iteration 192, loss = 0.33948067\n",
      "Iteration 193, loss = 0.33950317\n",
      "Iteration 194, loss = 0.33919492\n",
      "Iteration 195, loss = 0.33869014\n",
      "Iteration 196, loss = 0.33831306\n",
      "Iteration 197, loss = 0.33913682\n",
      "Iteration 198, loss = 0.33825612\n",
      "Iteration 199, loss = 0.33804214\n",
      "Iteration 200, loss = 0.33784278\n",
      "Iteration 201, loss = 0.33744232\n",
      "Iteration 202, loss = 0.33851561\n",
      "Iteration 203, loss = 0.33758103\n",
      "Iteration 204, loss = 0.33758433\n",
      "Iteration 205, loss = 0.33736365\n",
      "Iteration 206, loss = 0.33777886\n",
      "Iteration 207, loss = 0.33693991\n",
      "Iteration 208, loss = 0.33657110\n",
      "Iteration 209, loss = 0.33798867\n",
      "Iteration 210, loss = 0.33752272\n",
      "Iteration 211, loss = 0.33626417\n",
      "Iteration 212, loss = 0.33736999\n",
      "Iteration 213, loss = 0.33642993\n",
      "Iteration 214, loss = 0.33664244\n",
      "Iteration 215, loss = 0.33659442\n",
      "Iteration 216, loss = 0.33547148\n",
      "Iteration 217, loss = 0.33661372\n",
      "Iteration 218, loss = 0.33636052\n",
      "Iteration 219, loss = 0.33615091\n",
      "Iteration 220, loss = 0.33504793\n",
      "Iteration 221, loss = 0.33535866\n",
      "Iteration 222, loss = 0.33515435\n",
      "Iteration 223, loss = 0.33476414\n",
      "Iteration 224, loss = 0.33420780\n",
      "Iteration 225, loss = 0.33551378\n",
      "Iteration 226, loss = 0.33472203\n",
      "Iteration 227, loss = 0.33431762\n",
      "Iteration 228, loss = 0.33495033\n",
      "Iteration 229, loss = 0.33397005\n",
      "Iteration 230, loss = 0.33452474\n",
      "Iteration 231, loss = 0.33363941\n",
      "Iteration 232, loss = 0.33396755\n",
      "Iteration 233, loss = 0.33359898\n",
      "Iteration 234, loss = 0.33405744\n",
      "Iteration 235, loss = 0.33351337\n",
      "Iteration 236, loss = 0.33408849\n",
      "Iteration 237, loss = 0.33335613\n",
      "Iteration 238, loss = 0.33330714\n",
      "Iteration 239, loss = 0.33336462\n",
      "Iteration 240, loss = 0.33298541\n",
      "Iteration 241, loss = 0.33302081\n",
      "Iteration 242, loss = 0.33298582\n",
      "Iteration 243, loss = 0.33276937\n",
      "Iteration 244, loss = 0.33320234\n",
      "Iteration 245, loss = 0.33298741\n",
      "Iteration 246, loss = 0.33223104\n",
      "Iteration 247, loss = 0.33226119\n",
      "Iteration 248, loss = 0.33233637\n",
      "Iteration 249, loss = 0.33212399\n",
      "Iteration 250, loss = 0.33211079\n",
      "Iteration 251, loss = 0.33197961\n",
      "Iteration 252, loss = 0.33245909\n",
      "Iteration 253, loss = 0.33142197\n",
      "Iteration 254, loss = 0.33208241\n",
      "Iteration 255, loss = 0.33172462\n",
      "Iteration 256, loss = 0.33128758\n",
      "Iteration 257, loss = 0.33053778\n",
      "Iteration 258, loss = 0.33165143\n",
      "Iteration 259, loss = 0.33082138\n",
      "Iteration 260, loss = 0.33143859\n",
      "Iteration 261, loss = 0.33062832\n",
      "Iteration 262, loss = 0.33126291\n",
      "Iteration 263, loss = 0.33074070\n",
      "Iteration 264, loss = 0.33117907\n",
      "Iteration 265, loss = 0.33057549\n",
      "Iteration 266, loss = 0.33028993\n",
      "Iteration 267, loss = 0.33043414\n",
      "Iteration 268, loss = 0.32945626\n",
      "Iteration 269, loss = 0.33029457\n",
      "Iteration 270, loss = 0.33066047\n",
      "Iteration 271, loss = 0.33034688\n",
      "Iteration 272, loss = 0.32969658\n",
      "Iteration 273, loss = 0.32981060\n",
      "Iteration 274, loss = 0.32926338\n",
      "Iteration 275, loss = 0.32959093\n",
      "Iteration 276, loss = 0.33035215\n",
      "Iteration 277, loss = 0.33025749\n",
      "Iteration 278, loss = 0.32980786\n",
      "Iteration 279, loss = 0.32859302\n",
      "Iteration 280, loss = 0.32961066\n",
      "Iteration 281, loss = 0.32869712\n",
      "Iteration 282, loss = 0.32897772\n",
      "Iteration 283, loss = 0.32800432\n",
      "Iteration 284, loss = 0.32936468\n",
      "Iteration 285, loss = 0.32869935\n",
      "Iteration 286, loss = 0.32856224\n",
      "Iteration 287, loss = 0.32793301\n",
      "Iteration 288, loss = 0.32805985\n",
      "Iteration 289, loss = 0.32884251\n",
      "Iteration 290, loss = 0.32862880\n",
      "Iteration 291, loss = 0.32808439\n",
      "Iteration 292, loss = 0.32806251\n",
      "Iteration 293, loss = 0.32800500\n",
      "Iteration 294, loss = 0.32753369\n",
      "Iteration 295, loss = 0.32796077\n",
      "Iteration 296, loss = 0.32785007\n",
      "Iteration 297, loss = 0.32721018\n",
      "Iteration 298, loss = 0.32782635\n",
      "Iteration 299, loss = 0.32804866\n",
      "Iteration 300, loss = 0.32762508\n",
      "Iteration 301, loss = 0.32744996\n",
      "Iteration 302, loss = 0.32772159\n",
      "Iteration 303, loss = 0.32675535\n",
      "Iteration 304, loss = 0.32745756\n",
      "Iteration 305, loss = 0.32703374\n",
      "Iteration 306, loss = 0.32626472\n",
      "Iteration 307, loss = 0.32643483\n",
      "Iteration 308, loss = 0.32600101\n",
      "Iteration 309, loss = 0.32781257\n",
      "Iteration 310, loss = 0.32754442\n",
      "Iteration 311, loss = 0.32573856\n",
      "Iteration 312, loss = 0.32653847\n",
      "Iteration 313, loss = 0.32635979\n",
      "Iteration 314, loss = 0.32573088\n",
      "Iteration 315, loss = 0.32767191\n",
      "Iteration 316, loss = 0.32698317\n",
      "Iteration 317, loss = 0.32563075\n",
      "Iteration 318, loss = 0.32627876\n",
      "Iteration 319, loss = 0.32564921\n",
      "Iteration 320, loss = 0.32597089\n",
      "Iteration 321, loss = 0.32594632\n",
      "Iteration 322, loss = 0.32556537\n",
      "Iteration 323, loss = 0.32556746\n",
      "Iteration 324, loss = 0.32543967\n",
      "Iteration 325, loss = 0.32551716\n",
      "Iteration 326, loss = 0.32532380\n",
      "Iteration 327, loss = 0.32487463\n",
      "Iteration 328, loss = 0.32462530\n",
      "Iteration 329, loss = 0.32528303\n",
      "Iteration 330, loss = 0.32484035\n",
      "Iteration 331, loss = 0.32579956\n",
      "Iteration 332, loss = 0.32510488\n",
      "Iteration 333, loss = 0.32585918\n",
      "Iteration 334, loss = 0.32387709\n",
      "Iteration 335, loss = 0.32425168\n",
      "Iteration 336, loss = 0.32428740\n",
      "Iteration 337, loss = 0.32408566\n",
      "Iteration 338, loss = 0.32508429\n",
      "Iteration 339, loss = 0.32507012\n",
      "Iteration 340, loss = 0.32354192\n",
      "Iteration 341, loss = 0.32363050\n",
      "Iteration 342, loss = 0.32422694\n",
      "Iteration 343, loss = 0.32412124\n",
      "Iteration 344, loss = 0.32427605\n",
      "Iteration 345, loss = 0.32473397\n",
      "Iteration 346, loss = 0.32336842\n",
      "Iteration 347, loss = 0.32278904\n",
      "Iteration 348, loss = 0.32332773\n",
      "Iteration 349, loss = 0.32404497\n",
      "Iteration 350, loss = 0.32377988\n",
      "Iteration 351, loss = 0.32389864\n",
      "Iteration 352, loss = 0.32277323\n",
      "Iteration 353, loss = 0.32362859\n",
      "Iteration 354, loss = 0.32258070\n",
      "Iteration 355, loss = 0.32300251\n",
      "Iteration 356, loss = 0.32186716\n",
      "Iteration 357, loss = 0.32330415\n",
      "Iteration 358, loss = 0.32304131\n",
      "Iteration 359, loss = 0.32303411\n",
      "Iteration 360, loss = 0.32229581\n",
      "Iteration 361, loss = 0.32221402\n",
      "Iteration 362, loss = 0.32270604\n",
      "Iteration 363, loss = 0.32300105\n",
      "Iteration 364, loss = 0.32332545\n",
      "Iteration 365, loss = 0.32345261\n",
      "Iteration 366, loss = 0.32192197\n",
      "Iteration 367, loss = 0.32238714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51203219\n",
      "Iteration 2, loss = 0.46195002\n",
      "Iteration 3, loss = 0.45251732\n",
      "Iteration 4, loss = 0.44724811\n",
      "Iteration 5, loss = 0.44245937\n",
      "Iteration 6, loss = 0.43858419\n",
      "Iteration 7, loss = 0.43498268\n",
      "Iteration 8, loss = 0.43165740\n",
      "Iteration 9, loss = 0.42870008\n",
      "Iteration 10, loss = 0.42606508\n",
      "Iteration 11, loss = 0.42351476\n",
      "Iteration 12, loss = 0.42161958\n",
      "Iteration 13, loss = 0.41912258\n",
      "Iteration 14, loss = 0.41700467\n",
      "Iteration 15, loss = 0.41474905\n",
      "Iteration 16, loss = 0.41353883\n",
      "Iteration 17, loss = 0.41158072\n",
      "Iteration 18, loss = 0.41033170\n",
      "Iteration 19, loss = 0.40842468\n",
      "Iteration 20, loss = 0.40668373\n",
      "Iteration 21, loss = 0.40592207\n",
      "Iteration 22, loss = 0.40378734\n",
      "Iteration 23, loss = 0.40288928\n",
      "Iteration 24, loss = 0.40172349\n",
      "Iteration 25, loss = 0.40064934\n",
      "Iteration 26, loss = 0.39944203\n",
      "Iteration 27, loss = 0.39810860\n",
      "Iteration 28, loss = 0.39675184\n",
      "Iteration 29, loss = 0.39572802\n",
      "Iteration 30, loss = 0.39435110\n",
      "Iteration 31, loss = 0.39415165\n",
      "Iteration 32, loss = 0.39292710\n",
      "Iteration 33, loss = 0.39155779\n",
      "Iteration 34, loss = 0.39053894\n",
      "Iteration 35, loss = 0.38991384\n",
      "Iteration 36, loss = 0.38924052\n",
      "Iteration 37, loss = 0.38855719\n",
      "Iteration 38, loss = 0.38791431\n",
      "Iteration 39, loss = 0.38634449\n",
      "Iteration 40, loss = 0.38525790\n",
      "Iteration 41, loss = 0.38467873\n",
      "Iteration 42, loss = 0.38360921\n",
      "Iteration 43, loss = 0.38275636\n",
      "Iteration 44, loss = 0.38200022\n",
      "Iteration 45, loss = 0.38175569\n",
      "Iteration 46, loss = 0.38081354\n",
      "Iteration 47, loss = 0.37991941\n",
      "Iteration 48, loss = 0.37903677\n",
      "Iteration 49, loss = 0.37872781\n",
      "Iteration 50, loss = 0.37765222\n",
      "Iteration 51, loss = 0.37665257\n",
      "Iteration 52, loss = 0.37681090\n",
      "Iteration 53, loss = 0.37555775\n",
      "Iteration 54, loss = 0.37476420\n",
      "Iteration 55, loss = 0.37460147\n",
      "Iteration 56, loss = 0.37476250\n",
      "Iteration 57, loss = 0.37329171\n",
      "Iteration 58, loss = 0.37270747\n",
      "Iteration 59, loss = 0.37188369\n",
      "Iteration 60, loss = 0.37044695\n",
      "Iteration 61, loss = 0.37022995\n",
      "Iteration 62, loss = 0.36966724\n",
      "Iteration 63, loss = 0.36921723\n",
      "Iteration 64, loss = 0.36927845\n",
      "Iteration 65, loss = 0.36789291\n",
      "Iteration 66, loss = 0.36796952\n",
      "Iteration 67, loss = 0.36667089\n",
      "Iteration 68, loss = 0.36623869\n",
      "Iteration 69, loss = 0.36595678\n",
      "Iteration 70, loss = 0.36546148\n",
      "Iteration 71, loss = 0.36514356\n",
      "Iteration 72, loss = 0.36384544\n",
      "Iteration 73, loss = 0.36295109\n",
      "Iteration 74, loss = 0.36216563\n",
      "Iteration 75, loss = 0.36288018\n",
      "Iteration 76, loss = 0.36190442\n",
      "Iteration 77, loss = 0.36147436\n",
      "Iteration 78, loss = 0.36119602\n",
      "Iteration 79, loss = 0.36026512\n",
      "Iteration 80, loss = 0.35984580\n",
      "Iteration 81, loss = 0.35975886\n",
      "Iteration 82, loss = 0.35886817\n",
      "Iteration 83, loss = 0.35873831\n",
      "Iteration 84, loss = 0.35829559\n",
      "Iteration 85, loss = 0.35768018\n",
      "Iteration 86, loss = 0.35679953\n",
      "Iteration 87, loss = 0.35680453\n",
      "Iteration 88, loss = 0.35597490\n",
      "Iteration 89, loss = 0.35588427\n",
      "Iteration 90, loss = 0.35542225\n",
      "Iteration 91, loss = 0.35526026\n",
      "Iteration 92, loss = 0.35464407\n",
      "Iteration 93, loss = 0.35387995\n",
      "Iteration 94, loss = 0.35349889\n",
      "Iteration 95, loss = 0.35384437\n",
      "Iteration 96, loss = 0.35344599\n",
      "Iteration 97, loss = 0.35228519\n",
      "Iteration 98, loss = 0.35228198\n",
      "Iteration 99, loss = 0.35292636\n",
      "Iteration 100, loss = 0.35190665\n",
      "Iteration 101, loss = 0.35167380\n",
      "Iteration 102, loss = 0.34994305\n",
      "Iteration 103, loss = 0.35060680\n",
      "Iteration 104, loss = 0.35040817\n",
      "Iteration 105, loss = 0.34930882\n",
      "Iteration 106, loss = 0.35032805\n",
      "Iteration 107, loss = 0.34955291\n",
      "Iteration 108, loss = 0.34830442\n",
      "Iteration 109, loss = 0.34820440\n",
      "Iteration 110, loss = 0.34814965\n",
      "Iteration 111, loss = 0.34844884\n",
      "Iteration 112, loss = 0.34843337\n",
      "Iteration 113, loss = 0.34771945\n",
      "Iteration 114, loss = 0.34706113\n",
      "Iteration 115, loss = 0.34717480\n",
      "Iteration 116, loss = 0.34585364\n",
      "Iteration 117, loss = 0.34584206\n",
      "Iteration 118, loss = 0.34623630\n",
      "Iteration 119, loss = 0.34515797\n",
      "Iteration 120, loss = 0.34521447\n",
      "Iteration 121, loss = 0.34549762\n",
      "Iteration 122, loss = 0.34592166\n",
      "Iteration 123, loss = 0.34471204\n",
      "Iteration 124, loss = 0.34429462\n",
      "Iteration 125, loss = 0.34354394\n",
      "Iteration 126, loss = 0.34425930\n",
      "Iteration 127, loss = 0.34361755\n",
      "Iteration 128, loss = 0.34264263\n",
      "Iteration 129, loss = 0.34257912\n",
      "Iteration 130, loss = 0.34286753\n",
      "Iteration 131, loss = 0.34192898\n",
      "Iteration 132, loss = 0.34193279\n",
      "Iteration 133, loss = 0.34227136\n",
      "Iteration 134, loss = 0.34201110\n",
      "Iteration 135, loss = 0.34099207\n",
      "Iteration 136, loss = 0.34087243\n",
      "Iteration 137, loss = 0.34137823\n",
      "Iteration 138, loss = 0.34014428\n",
      "Iteration 139, loss = 0.34072292\n",
      "Iteration 140, loss = 0.34042908\n",
      "Iteration 141, loss = 0.34020312\n",
      "Iteration 142, loss = 0.34010441\n",
      "Iteration 143, loss = 0.33996692\n",
      "Iteration 144, loss = 0.33919937\n",
      "Iteration 145, loss = 0.33918328\n",
      "Iteration 146, loss = 0.33865911\n",
      "Iteration 147, loss = 0.33837982\n",
      "Iteration 148, loss = 0.33817922\n",
      "Iteration 149, loss = 0.33794040\n",
      "Iteration 150, loss = 0.33756218\n",
      "Iteration 151, loss = 0.33724973\n",
      "Iteration 152, loss = 0.33803810\n",
      "Iteration 153, loss = 0.33843196\n",
      "Iteration 154, loss = 0.33645297\n",
      "Iteration 155, loss = 0.33705023\n",
      "Iteration 156, loss = 0.33651719\n",
      "Iteration 157, loss = 0.33574068\n",
      "Iteration 158, loss = 0.33644616\n",
      "Iteration 159, loss = 0.33667205\n",
      "Iteration 160, loss = 0.33622938\n",
      "Iteration 161, loss = 0.33450655\n",
      "Iteration 162, loss = 0.33485339\n",
      "Iteration 163, loss = 0.33567999\n",
      "Iteration 164, loss = 0.33463570\n",
      "Iteration 165, loss = 0.33520279\n",
      "Iteration 166, loss = 0.33404152\n",
      "Iteration 167, loss = 0.33416391\n",
      "Iteration 168, loss = 0.33454962\n",
      "Iteration 169, loss = 0.33448176\n",
      "Iteration 170, loss = 0.33383115\n",
      "Iteration 171, loss = 0.33337773\n",
      "Iteration 172, loss = 0.33364071\n",
      "Iteration 173, loss = 0.33370410\n",
      "Iteration 174, loss = 0.33227046\n",
      "Iteration 175, loss = 0.33250638\n",
      "Iteration 176, loss = 0.33296262\n",
      "Iteration 177, loss = 0.33292080\n",
      "Iteration 178, loss = 0.33207733\n",
      "Iteration 179, loss = 0.33213121\n",
      "Iteration 180, loss = 0.33173164\n",
      "Iteration 181, loss = 0.33117590\n",
      "Iteration 182, loss = 0.33152564\n",
      "Iteration 183, loss = 0.33161074\n",
      "Iteration 184, loss = 0.33106983\n",
      "Iteration 185, loss = 0.33205758\n",
      "Iteration 186, loss = 0.33039478\n",
      "Iteration 187, loss = 0.33068511\n",
      "Iteration 188, loss = 0.33032633\n",
      "Iteration 189, loss = 0.33015090\n",
      "Iteration 190, loss = 0.32930506\n",
      "Iteration 191, loss = 0.32926330\n",
      "Iteration 192, loss = 0.32954737\n",
      "Iteration 193, loss = 0.32902259\n",
      "Iteration 194, loss = 0.32881689\n",
      "Iteration 195, loss = 0.33000739\n",
      "Iteration 196, loss = 0.32824655\n",
      "Iteration 197, loss = 0.32882869\n",
      "Iteration 198, loss = 0.32892423\n",
      "Iteration 199, loss = 0.32848682\n",
      "Iteration 200, loss = 0.32777263\n",
      "Iteration 201, loss = 0.32830274\n",
      "Iteration 202, loss = 0.32827138\n",
      "Iteration 203, loss = 0.32796957\n",
      "Iteration 204, loss = 0.32827666\n",
      "Iteration 205, loss = 0.32758780\n",
      "Iteration 206, loss = 0.32700381\n",
      "Iteration 207, loss = 0.32635408\n",
      "Iteration 208, loss = 0.32622467\n",
      "Iteration 209, loss = 0.32676145\n",
      "Iteration 210, loss = 0.32687268\n",
      "Iteration 211, loss = 0.32645324\n",
      "Iteration 212, loss = 0.32611832\n",
      "Iteration 213, loss = 0.32612540\n",
      "Iteration 214, loss = 0.32556441\n",
      "Iteration 215, loss = 0.32476476\n",
      "Iteration 216, loss = 0.32560420\n",
      "Iteration 217, loss = 0.32603774\n",
      "Iteration 218, loss = 0.32493172\n",
      "Iteration 219, loss = 0.32491579\n",
      "Iteration 220, loss = 0.32503184\n",
      "Iteration 221, loss = 0.32433636\n",
      "Iteration 222, loss = 0.32527430\n",
      "Iteration 223, loss = 0.32376785\n",
      "Iteration 224, loss = 0.32446743\n",
      "Iteration 225, loss = 0.32436734\n",
      "Iteration 226, loss = 0.32374227\n",
      "Iteration 227, loss = 0.32276361\n",
      "Iteration 228, loss = 0.32417255\n",
      "Iteration 229, loss = 0.32381850\n",
      "Iteration 230, loss = 0.32325433\n",
      "Iteration 231, loss = 0.32338359\n",
      "Iteration 232, loss = 0.32304221\n",
      "Iteration 233, loss = 0.32382695\n",
      "Iteration 234, loss = 0.32318123\n",
      "Iteration 235, loss = 0.32317258\n",
      "Iteration 236, loss = 0.32290657\n",
      "Iteration 237, loss = 0.32355923\n",
      "Iteration 238, loss = 0.32314270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.49010749\n",
      "Iteration 2, loss = 0.44195885\n",
      "Iteration 3, loss = 0.43258844\n",
      "Iteration 4, loss = 0.42560898\n",
      "Iteration 5, loss = 0.42026905\n",
      "Iteration 6, loss = 0.41551830\n",
      "Iteration 7, loss = 0.41066025\n",
      "Iteration 8, loss = 0.40708696\n",
      "Iteration 9, loss = 0.40266277\n",
      "Iteration 10, loss = 0.39891822\n",
      "Iteration 11, loss = 0.39580072\n",
      "Iteration 12, loss = 0.39276249\n",
      "Iteration 13, loss = 0.38962056\n",
      "Iteration 14, loss = 0.38702829\n",
      "Iteration 15, loss = 0.38456644\n",
      "Iteration 16, loss = 0.38183707\n",
      "Iteration 17, loss = 0.37989122\n",
      "Iteration 18, loss = 0.37750008\n",
      "Iteration 19, loss = 0.37582240\n",
      "Iteration 20, loss = 0.37357792\n",
      "Iteration 21, loss = 0.37142016\n",
      "Iteration 22, loss = 0.36955713\n",
      "Iteration 23, loss = 0.36853038\n",
      "Iteration 24, loss = 0.36699543\n",
      "Iteration 25, loss = 0.36514350\n",
      "Iteration 26, loss = 0.36429943\n",
      "Iteration 27, loss = 0.36260903\n",
      "Iteration 28, loss = 0.36114288\n",
      "Iteration 29, loss = 0.35993180\n",
      "Iteration 30, loss = 0.35911153\n",
      "Iteration 31, loss = 0.35784492\n",
      "Iteration 32, loss = 0.35676702\n",
      "Iteration 33, loss = 0.35546901\n",
      "Iteration 34, loss = 0.35535866\n",
      "Iteration 35, loss = 0.35337515\n",
      "Iteration 36, loss = 0.35294757\n",
      "Iteration 37, loss = 0.35135002\n",
      "Iteration 38, loss = 0.35024374\n",
      "Iteration 39, loss = 0.35034338\n",
      "Iteration 40, loss = 0.34882598\n",
      "Iteration 41, loss = 0.34795409\n",
      "Iteration 42, loss = 0.34706242\n",
      "Iteration 43, loss = 0.34639574\n",
      "Iteration 44, loss = 0.34569093\n",
      "Iteration 45, loss = 0.34454323\n",
      "Iteration 46, loss = 0.34399069\n",
      "Iteration 47, loss = 0.34381342\n",
      "Iteration 48, loss = 0.34230331\n",
      "Iteration 49, loss = 0.34194437\n",
      "Iteration 50, loss = 0.34083911\n",
      "Iteration 51, loss = 0.33994784\n",
      "Iteration 52, loss = 0.33951259\n",
      "Iteration 53, loss = 0.33897442\n",
      "Iteration 54, loss = 0.33774393\n",
      "Iteration 55, loss = 0.33758709\n",
      "Iteration 56, loss = 0.33657441\n",
      "Iteration 57, loss = 0.33566488\n",
      "Iteration 58, loss = 0.33561888\n",
      "Iteration 59, loss = 0.33465772\n",
      "Iteration 60, loss = 0.33377322\n",
      "Iteration 61, loss = 0.33333737\n",
      "Iteration 62, loss = 0.33242195\n",
      "Iteration 63, loss = 0.33226247\n",
      "Iteration 64, loss = 0.33098453\n",
      "Iteration 65, loss = 0.33130668\n",
      "Iteration 66, loss = 0.33079393\n",
      "Iteration 67, loss = 0.32974296\n",
      "Iteration 68, loss = 0.33025156\n",
      "Iteration 69, loss = 0.32858707\n",
      "Iteration 70, loss = 0.32915103\n",
      "Iteration 71, loss = 0.32757934\n",
      "Iteration 72, loss = 0.32742773\n",
      "Iteration 73, loss = 0.32684454\n",
      "Iteration 74, loss = 0.32671393\n",
      "Iteration 75, loss = 0.32720598\n",
      "Iteration 76, loss = 0.32605348\n",
      "Iteration 77, loss = 0.32491835\n",
      "Iteration 78, loss = 0.32457125\n",
      "Iteration 79, loss = 0.32462402\n",
      "Iteration 80, loss = 0.32289004\n",
      "Iteration 81, loss = 0.32447347\n",
      "Iteration 82, loss = 0.32333359\n",
      "Iteration 83, loss = 0.32339508\n",
      "Iteration 84, loss = 0.32234686\n",
      "Iteration 85, loss = 0.32225619\n",
      "Iteration 86, loss = 0.32118783\n",
      "Iteration 87, loss = 0.32094832\n",
      "Iteration 88, loss = 0.32074754\n",
      "Iteration 89, loss = 0.31961953\n",
      "Iteration 90, loss = 0.31947114\n",
      "Iteration 91, loss = 0.31996105\n",
      "Iteration 92, loss = 0.32027446\n",
      "Iteration 93, loss = 0.31835514\n",
      "Iteration 94, loss = 0.31787765\n",
      "Iteration 95, loss = 0.31769465\n",
      "Iteration 96, loss = 0.31786911\n",
      "Iteration 97, loss = 0.31721066\n",
      "Iteration 98, loss = 0.31670929\n",
      "Iteration 99, loss = 0.31652077\n",
      "Iteration 100, loss = 0.31644739\n",
      "Iteration 101, loss = 0.31512754\n",
      "Iteration 102, loss = 0.31552120\n",
      "Iteration 103, loss = 0.31524991\n",
      "Iteration 104, loss = 0.31528569\n",
      "Iteration 105, loss = 0.31476418\n",
      "Iteration 106, loss = 0.31382326\n",
      "Iteration 107, loss = 0.31355806\n",
      "Iteration 108, loss = 0.31333453\n",
      "Iteration 109, loss = 0.31216174\n",
      "Iteration 110, loss = 0.31285943\n",
      "Iteration 111, loss = 0.31218842\n",
      "Iteration 112, loss = 0.31221826\n",
      "Iteration 113, loss = 0.31148770\n",
      "Iteration 114, loss = 0.31083913\n",
      "Iteration 115, loss = 0.31125845\n",
      "Iteration 116, loss = 0.30998455\n",
      "Iteration 117, loss = 0.31112701\n",
      "Iteration 118, loss = 0.30942813\n",
      "Iteration 119, loss = 0.30951715\n",
      "Iteration 120, loss = 0.30959954\n",
      "Iteration 121, loss = 0.30888586\n",
      "Iteration 122, loss = 0.30838904\n",
      "Iteration 123, loss = 0.30893091\n",
      "Iteration 124, loss = 0.30795432\n",
      "Iteration 125, loss = 0.30815429\n",
      "Iteration 126, loss = 0.30794589\n",
      "Iteration 127, loss = 0.30838579\n",
      "Iteration 128, loss = 0.30744800\n",
      "Iteration 129, loss = 0.30702022\n",
      "Iteration 130, loss = 0.30664818\n",
      "Iteration 131, loss = 0.30668413\n",
      "Iteration 132, loss = 0.30571528\n",
      "Iteration 133, loss = 0.30550996\n",
      "Iteration 134, loss = 0.30584243\n",
      "Iteration 135, loss = 0.30570692\n",
      "Iteration 136, loss = 0.30528776\n",
      "Iteration 137, loss = 0.30483255\n",
      "Iteration 138, loss = 0.30450049\n",
      "Iteration 139, loss = 0.30450959\n",
      "Iteration 140, loss = 0.30433376\n",
      "Iteration 141, loss = 0.30385829\n",
      "Iteration 142, loss = 0.30319762\n",
      "Iteration 143, loss = 0.30363791\n",
      "Iteration 144, loss = 0.30329668\n",
      "Iteration 145, loss = 0.30295843\n",
      "Iteration 146, loss = 0.30270335\n",
      "Iteration 147, loss = 0.30311127\n",
      "Iteration 148, loss = 0.30276473\n",
      "Iteration 149, loss = 0.30201794\n",
      "Iteration 150, loss = 0.30181548\n",
      "Iteration 151, loss = 0.30169479\n",
      "Iteration 152, loss = 0.30298955\n",
      "Iteration 153, loss = 0.30180516\n",
      "Iteration 154, loss = 0.30130406\n",
      "Iteration 155, loss = 0.29982117\n",
      "Iteration 156, loss = 0.30045950\n",
      "Iteration 157, loss = 0.29991092\n",
      "Iteration 158, loss = 0.30075308\n",
      "Iteration 159, loss = 0.29941945\n",
      "Iteration 160, loss = 0.30013538\n",
      "Iteration 161, loss = 0.29937758\n",
      "Iteration 162, loss = 0.29952500\n",
      "Iteration 163, loss = 0.29931428\n",
      "Iteration 164, loss = 0.29835964\n",
      "Iteration 165, loss = 0.29794689\n",
      "Iteration 166, loss = 0.29903263\n",
      "Iteration 167, loss = 0.29830579\n",
      "Iteration 168, loss = 0.29781034\n",
      "Iteration 169, loss = 0.29714282\n",
      "Iteration 170, loss = 0.29745330\n",
      "Iteration 171, loss = 0.29841962\n",
      "Iteration 172, loss = 0.29731322\n",
      "Iteration 173, loss = 0.29823255\n",
      "Iteration 174, loss = 0.29753646\n",
      "Iteration 175, loss = 0.29621269\n",
      "Iteration 176, loss = 0.29631484\n",
      "Iteration 177, loss = 0.29643380\n",
      "Iteration 178, loss = 0.29642789\n",
      "Iteration 179, loss = 0.29549890\n",
      "Iteration 180, loss = 0.29697885\n",
      "Iteration 181, loss = 0.29572224\n",
      "Iteration 182, loss = 0.29579697\n",
      "Iteration 183, loss = 0.29579422\n",
      "Iteration 184, loss = 0.29510615\n",
      "Iteration 185, loss = 0.29529343\n",
      "Iteration 186, loss = 0.29541279\n",
      "Iteration 187, loss = 0.29441752\n",
      "Iteration 188, loss = 0.29364475\n",
      "Iteration 189, loss = 0.29481909\n",
      "Iteration 190, loss = 0.29496008\n",
      "Iteration 191, loss = 0.29374859\n",
      "Iteration 192, loss = 0.29408523\n",
      "Iteration 193, loss = 0.29359685\n",
      "Iteration 194, loss = 0.29309886\n",
      "Iteration 195, loss = 0.29345142\n",
      "Iteration 196, loss = 0.29334189\n",
      "Iteration 197, loss = 0.29293136\n",
      "Iteration 198, loss = 0.29269619\n",
      "Iteration 199, loss = 0.29228188\n",
      "Iteration 200, loss = 0.29248790\n",
      "Iteration 201, loss = 0.29249773\n",
      "Iteration 202, loss = 0.29237458\n",
      "Iteration 203, loss = 0.29315210\n",
      "Iteration 204, loss = 0.29182873\n",
      "Iteration 205, loss = 0.29218175\n",
      "Iteration 206, loss = 0.29093793\n",
      "Iteration 207, loss = 0.29174050\n",
      "Iteration 208, loss = 0.29165292\n",
      "Iteration 209, loss = 0.29156345\n",
      "Iteration 210, loss = 0.28977602\n",
      "Iteration 211, loss = 0.29071622\n",
      "Iteration 212, loss = 0.29098573\n",
      "Iteration 213, loss = 0.29070080\n",
      "Iteration 214, loss = 0.28975858\n",
      "Iteration 215, loss = 0.28963375\n",
      "Iteration 216, loss = 0.28989813\n",
      "Iteration 217, loss = 0.28951493\n",
      "Iteration 218, loss = 0.29054149\n",
      "Iteration 219, loss = 0.28908837\n",
      "Iteration 220, loss = 0.28933987\n",
      "Iteration 221, loss = 0.28875259\n",
      "Iteration 222, loss = 0.28924829\n",
      "Iteration 223, loss = 0.28944332\n",
      "Iteration 224, loss = 0.28983627\n",
      "Iteration 225, loss = 0.28965026\n",
      "Iteration 226, loss = 0.28849764\n",
      "Iteration 227, loss = 0.28886592\n",
      "Iteration 228, loss = 0.28746201\n",
      "Iteration 229, loss = 0.28769705\n",
      "Iteration 230, loss = 0.28876482\n",
      "Iteration 231, loss = 0.28806161\n",
      "Iteration 232, loss = 0.28754951\n",
      "Iteration 233, loss = 0.28903712\n",
      "Iteration 234, loss = 0.28848797\n",
      "Iteration 235, loss = 0.28736058\n",
      "Iteration 236, loss = 0.28660270\n",
      "Iteration 237, loss = 0.28682066\n",
      "Iteration 238, loss = 0.28702786\n",
      "Iteration 239, loss = 0.28668741\n",
      "Iteration 240, loss = 0.28706650\n",
      "Iteration 241, loss = 0.28692524\n",
      "Iteration 242, loss = 0.28611864\n",
      "Iteration 243, loss = 0.28650277\n",
      "Iteration 244, loss = 0.28607141\n",
      "Iteration 245, loss = 0.28594165\n",
      "Iteration 246, loss = 0.28622809\n",
      "Iteration 247, loss = 0.28750801\n",
      "Iteration 248, loss = 0.28583685\n",
      "Iteration 249, loss = 0.28560371\n",
      "Iteration 250, loss = 0.28604618\n",
      "Iteration 251, loss = 0.28494451\n",
      "Iteration 252, loss = 0.28761409\n",
      "Iteration 253, loss = 0.28459079\n",
      "Iteration 254, loss = 0.28447636\n",
      "Iteration 255, loss = 0.28585507\n",
      "Iteration 256, loss = 0.28508832\n",
      "Iteration 257, loss = 0.28461830\n",
      "Iteration 258, loss = 0.28480733\n",
      "Iteration 259, loss = 0.28556683\n",
      "Iteration 260, loss = 0.28499085\n",
      "Iteration 261, loss = 0.28446916\n",
      "Iteration 262, loss = 0.28477878\n",
      "Iteration 263, loss = 0.28524501\n",
      "Iteration 264, loss = 0.28416646\n",
      "Iteration 265, loss = 0.28378773\n",
      "Iteration 266, loss = 0.28376242\n",
      "Iteration 267, loss = 0.28449483\n",
      "Iteration 268, loss = 0.28443446\n",
      "Iteration 269, loss = 0.28332076\n",
      "Iteration 270, loss = 0.28353662\n",
      "Iteration 271, loss = 0.28416742\n",
      "Iteration 272, loss = 0.28391496\n",
      "Iteration 273, loss = 0.28370168\n",
      "Iteration 274, loss = 0.28381748\n",
      "Iteration 275, loss = 0.28392961\n",
      "Iteration 276, loss = 0.28210211\n",
      "Iteration 277, loss = 0.28196166\n",
      "Iteration 278, loss = 0.28315110\n",
      "Iteration 279, loss = 0.28356556\n",
      "Iteration 280, loss = 0.28299910\n",
      "Iteration 281, loss = 0.28243781\n",
      "Iteration 282, loss = 0.28147773\n",
      "Iteration 283, loss = 0.28182266\n",
      "Iteration 284, loss = 0.28217939\n",
      "Iteration 285, loss = 0.28152538\n",
      "Iteration 286, loss = 0.28136592\n",
      "Iteration 287, loss = 0.28204343\n",
      "Iteration 288, loss = 0.28250633\n",
      "Iteration 289, loss = 0.28150754\n",
      "Iteration 290, loss = 0.28254037\n",
      "Iteration 291, loss = 0.28164768\n",
      "Iteration 292, loss = 0.28224311\n",
      "Iteration 293, loss = 0.28181975\n",
      "Iteration 294, loss = 0.27962405\n",
      "Iteration 295, loss = 0.28152474\n",
      "Iteration 296, loss = 0.28072976\n",
      "Iteration 297, loss = 0.28117090\n",
      "Iteration 298, loss = 0.28051961\n",
      "Iteration 299, loss = 0.28107851\n",
      "Iteration 300, loss = 0.28094053\n",
      "Iteration 301, loss = 0.28038776\n",
      "Iteration 302, loss = 0.27911943\n",
      "Iteration 303, loss = 0.28005406\n",
      "Iteration 304, loss = 0.28044900\n",
      "Iteration 305, loss = 0.28069503\n",
      "Iteration 306, loss = 0.27939016\n",
      "Iteration 307, loss = 0.27946674\n",
      "Iteration 308, loss = 0.28047157\n",
      "Iteration 309, loss = 0.27999198\n",
      "Iteration 310, loss = 0.27929481\n",
      "Iteration 311, loss = 0.27948261\n",
      "Iteration 312, loss = 0.27964878\n",
      "Iteration 313, loss = 0.27908847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51687670\n",
      "Iteration 2, loss = 0.46654783\n",
      "Iteration 3, loss = 0.45747095\n",
      "Iteration 4, loss = 0.45201575\n",
      "Iteration 5, loss = 0.44639941\n",
      "Iteration 6, loss = 0.44195467\n",
      "Iteration 7, loss = 0.43851911\n",
      "Iteration 8, loss = 0.43429872\n",
      "Iteration 9, loss = 0.43193198\n",
      "Iteration 10, loss = 0.42951686\n",
      "Iteration 11, loss = 0.42599615\n",
      "Iteration 12, loss = 0.42400963\n",
      "Iteration 13, loss = 0.42208565\n",
      "Iteration 14, loss = 0.41952335\n",
      "Iteration 15, loss = 0.41753136\n",
      "Iteration 16, loss = 0.41569104\n",
      "Iteration 17, loss = 0.41368620\n",
      "Iteration 18, loss = 0.41185828\n",
      "Iteration 19, loss = 0.41002254\n",
      "Iteration 20, loss = 0.40881844\n",
      "Iteration 21, loss = 0.40734002\n",
      "Iteration 22, loss = 0.40557959\n",
      "Iteration 23, loss = 0.40397963\n",
      "Iteration 24, loss = 0.40258182\n",
      "Iteration 25, loss = 0.40145096\n",
      "Iteration 26, loss = 0.39977890\n",
      "Iteration 27, loss = 0.39883465\n",
      "Iteration 28, loss = 0.39739125\n",
      "Iteration 29, loss = 0.39617708\n",
      "Iteration 30, loss = 0.39596232\n",
      "Iteration 31, loss = 0.39434660\n",
      "Iteration 32, loss = 0.39365653\n",
      "Iteration 33, loss = 0.39129716\n",
      "Iteration 34, loss = 0.39066720\n",
      "Iteration 35, loss = 0.38955849\n",
      "Iteration 36, loss = 0.38879625\n",
      "Iteration 37, loss = 0.38838096\n",
      "Iteration 38, loss = 0.38711787\n",
      "Iteration 39, loss = 0.38590118\n",
      "Iteration 40, loss = 0.38512524\n",
      "Iteration 41, loss = 0.38420819\n",
      "Iteration 42, loss = 0.38367231\n",
      "Iteration 43, loss = 0.38244834\n",
      "Iteration 44, loss = 0.38165417\n",
      "Iteration 45, loss = 0.38117311\n",
      "Iteration 46, loss = 0.38030092\n",
      "Iteration 47, loss = 0.37950800\n",
      "Iteration 48, loss = 0.37888347\n",
      "Iteration 49, loss = 0.37773531\n",
      "Iteration 50, loss = 0.37700687\n",
      "Iteration 51, loss = 0.37621452\n",
      "Iteration 52, loss = 0.37496322\n",
      "Iteration 53, loss = 0.37558545\n",
      "Iteration 54, loss = 0.37409009\n",
      "Iteration 55, loss = 0.37287955\n",
      "Iteration 56, loss = 0.37160098\n",
      "Iteration 57, loss = 0.37175532\n",
      "Iteration 58, loss = 0.37115330\n",
      "Iteration 59, loss = 0.37005506\n",
      "Iteration 60, loss = 0.36905022\n",
      "Iteration 61, loss = 0.36903668\n",
      "Iteration 62, loss = 0.36815087\n",
      "Iteration 63, loss = 0.36764783\n",
      "Iteration 64, loss = 0.36709815\n",
      "Iteration 65, loss = 0.36550721\n",
      "Iteration 66, loss = 0.36564109\n",
      "Iteration 67, loss = 0.36512246\n",
      "Iteration 68, loss = 0.36458434\n",
      "Iteration 69, loss = 0.36358001\n",
      "Iteration 70, loss = 0.36358415\n",
      "Iteration 71, loss = 0.36236689\n",
      "Iteration 72, loss = 0.36230543\n",
      "Iteration 73, loss = 0.36233149\n",
      "Iteration 74, loss = 0.36089588\n",
      "Iteration 75, loss = 0.36106593\n",
      "Iteration 76, loss = 0.35983556\n",
      "Iteration 77, loss = 0.35923541\n",
      "Iteration 78, loss = 0.35903826\n",
      "Iteration 79, loss = 0.35867794\n",
      "Iteration 80, loss = 0.35953619\n",
      "Iteration 81, loss = 0.35757518\n",
      "Iteration 82, loss = 0.35738494\n",
      "Iteration 83, loss = 0.35755640\n",
      "Iteration 84, loss = 0.35654343\n",
      "Iteration 85, loss = 0.35652582\n",
      "Iteration 86, loss = 0.35682809\n",
      "Iteration 87, loss = 0.35570741\n",
      "Iteration 88, loss = 0.35473136\n",
      "Iteration 89, loss = 0.35516615\n",
      "Iteration 90, loss = 0.35393465\n",
      "Iteration 91, loss = 0.35347904\n",
      "Iteration 92, loss = 0.35375332\n",
      "Iteration 93, loss = 0.35287419\n",
      "Iteration 94, loss = 0.35292797\n",
      "Iteration 95, loss = 0.35115229\n",
      "Iteration 96, loss = 0.35240669\n",
      "Iteration 97, loss = 0.35154414\n",
      "Iteration 98, loss = 0.35125101\n",
      "Iteration 99, loss = 0.35100025\n",
      "Iteration 100, loss = 0.35129421\n",
      "Iteration 101, loss = 0.34981000\n",
      "Iteration 102, loss = 0.35083600\n",
      "Iteration 103, loss = 0.35006679\n",
      "Iteration 104, loss = 0.34878148\n",
      "Iteration 105, loss = 0.34824938\n",
      "Iteration 106, loss = 0.34845474\n",
      "Iteration 107, loss = 0.34843553\n",
      "Iteration 108, loss = 0.34862716\n",
      "Iteration 109, loss = 0.34891098\n",
      "Iteration 110, loss = 0.34723811\n",
      "Iteration 111, loss = 0.34660406\n",
      "Iteration 112, loss = 0.34717960\n",
      "Iteration 113, loss = 0.34616376\n",
      "Iteration 114, loss = 0.34591094\n",
      "Iteration 115, loss = 0.34544861\n",
      "Iteration 116, loss = 0.34560697\n",
      "Iteration 117, loss = 0.34504628\n",
      "Iteration 118, loss = 0.34516826\n",
      "Iteration 119, loss = 0.34471974\n",
      "Iteration 120, loss = 0.34448648\n",
      "Iteration 121, loss = 0.34426231\n",
      "Iteration 122, loss = 0.34422136\n",
      "Iteration 123, loss = 0.34477600\n",
      "Iteration 124, loss = 0.34352624\n",
      "Iteration 125, loss = 0.34341475\n",
      "Iteration 126, loss = 0.34281714\n",
      "Iteration 127, loss = 0.34243783\n",
      "Iteration 128, loss = 0.34266751\n",
      "Iteration 129, loss = 0.34242844\n",
      "Iteration 130, loss = 0.34235528\n",
      "Iteration 131, loss = 0.34177069\n",
      "Iteration 132, loss = 0.34209876\n",
      "Iteration 133, loss = 0.34165869\n",
      "Iteration 134, loss = 0.34213156\n",
      "Iteration 135, loss = 0.34039798\n",
      "Iteration 136, loss = 0.34098015\n",
      "Iteration 137, loss = 0.33991711\n",
      "Iteration 138, loss = 0.34038028\n",
      "Iteration 139, loss = 0.33912088\n",
      "Iteration 140, loss = 0.33980814\n",
      "Iteration 141, loss = 0.33879001\n",
      "Iteration 142, loss = 0.33883515\n",
      "Iteration 143, loss = 0.33939649\n",
      "Iteration 144, loss = 0.33975325\n",
      "Iteration 145, loss = 0.33791385\n",
      "Iteration 146, loss = 0.33884398\n",
      "Iteration 147, loss = 0.33830328\n",
      "Iteration 148, loss = 0.33790242\n",
      "Iteration 149, loss = 0.33725846\n",
      "Iteration 150, loss = 0.33810124\n",
      "Iteration 151, loss = 0.33683701\n",
      "Iteration 152, loss = 0.33819028\n",
      "Iteration 153, loss = 0.33679982\n",
      "Iteration 154, loss = 0.33677413\n",
      "Iteration 155, loss = 0.33644692\n",
      "Iteration 156, loss = 0.33709608\n",
      "Iteration 157, loss = 0.33566928\n",
      "Iteration 158, loss = 0.33559603\n",
      "Iteration 159, loss = 0.33574108\n",
      "Iteration 160, loss = 0.33636825\n",
      "Iteration 161, loss = 0.33680074\n",
      "Iteration 162, loss = 0.33514006\n",
      "Iteration 163, loss = 0.33566967\n",
      "Iteration 164, loss = 0.33564567\n",
      "Iteration 165, loss = 0.33523694\n",
      "Iteration 166, loss = 0.33535567\n",
      "Iteration 167, loss = 0.33392455\n",
      "Iteration 168, loss = 0.33507695\n",
      "Iteration 169, loss = 0.33533225\n",
      "Iteration 170, loss = 0.33388440\n",
      "Iteration 171, loss = 0.33429293\n",
      "Iteration 172, loss = 0.33352385\n",
      "Iteration 173, loss = 0.33331591\n",
      "Iteration 174, loss = 0.33329867\n",
      "Iteration 175, loss = 0.33355697\n",
      "Iteration 176, loss = 0.33287045\n",
      "Iteration 177, loss = 0.33386884\n",
      "Iteration 178, loss = 0.33277441\n",
      "Iteration 179, loss = 0.33188859\n",
      "Iteration 180, loss = 0.33293028\n",
      "Iteration 181, loss = 0.33228864\n",
      "Iteration 182, loss = 0.33146470\n",
      "Iteration 183, loss = 0.33222637\n",
      "Iteration 184, loss = 0.33045183\n",
      "Iteration 185, loss = 0.33308941\n",
      "Iteration 186, loss = 0.33162639\n",
      "Iteration 187, loss = 0.33107630\n",
      "Iteration 188, loss = 0.33085026\n",
      "Iteration 189, loss = 0.33019559\n",
      "Iteration 190, loss = 0.33054096\n",
      "Iteration 191, loss = 0.32970293\n",
      "Iteration 192, loss = 0.32997823\n",
      "Iteration 193, loss = 0.33038128\n",
      "Iteration 194, loss = 0.32967467\n",
      "Iteration 195, loss = 0.33029601\n",
      "Iteration 196, loss = 0.32947090\n",
      "Iteration 197, loss = 0.32869521\n",
      "Iteration 198, loss = 0.33005855\n",
      "Iteration 199, loss = 0.33000556\n",
      "Iteration 200, loss = 0.32849087\n",
      "Iteration 201, loss = 0.32889963\n",
      "Iteration 202, loss = 0.32895020\n",
      "Iteration 203, loss = 0.32910967\n",
      "Iteration 204, loss = 0.32874845\n",
      "Iteration 205, loss = 0.32776393\n",
      "Iteration 206, loss = 0.32830280\n",
      "Iteration 207, loss = 0.32776184\n",
      "Iteration 208, loss = 0.32710153\n",
      "Iteration 209, loss = 0.32712466\n",
      "Iteration 210, loss = 0.32782880\n",
      "Iteration 211, loss = 0.32724573\n",
      "Iteration 212, loss = 0.32790344\n",
      "Iteration 213, loss = 0.32748641\n",
      "Iteration 214, loss = 0.32696232\n",
      "Iteration 215, loss = 0.32695550\n",
      "Iteration 216, loss = 0.32667173\n",
      "Iteration 217, loss = 0.32756660\n",
      "Iteration 218, loss = 0.32656731\n",
      "Iteration 219, loss = 0.32610693\n",
      "Iteration 220, loss = 0.32710735\n",
      "Iteration 221, loss = 0.32626548\n",
      "Iteration 222, loss = 0.32577095\n",
      "Iteration 223, loss = 0.32643293\n",
      "Iteration 224, loss = 0.32535217\n",
      "Iteration 225, loss = 0.32522706\n",
      "Iteration 226, loss = 0.32482581\n",
      "Iteration 227, loss = 0.32508152\n",
      "Iteration 228, loss = 0.32530220\n",
      "Iteration 229, loss = 0.32597960\n",
      "Iteration 230, loss = 0.32421176\n",
      "Iteration 231, loss = 0.32412418\n",
      "Iteration 232, loss = 0.32415344\n",
      "Iteration 233, loss = 0.32343632\n",
      "Iteration 234, loss = 0.32548558\n",
      "Iteration 235, loss = 0.32503315\n",
      "Iteration 236, loss = 0.32320871\n",
      "Iteration 237, loss = 0.32400852\n",
      "Iteration 238, loss = 0.32367529\n",
      "Iteration 239, loss = 0.32269936\n",
      "Iteration 240, loss = 0.32372157\n",
      "Iteration 241, loss = 0.32419781\n",
      "Iteration 242, loss = 0.32323498\n",
      "Iteration 243, loss = 0.32225775\n",
      "Iteration 244, loss = 0.32362454\n",
      "Iteration 245, loss = 0.32353765\n",
      "Iteration 246, loss = 0.32172065\n",
      "Iteration 247, loss = 0.32335865\n",
      "Iteration 248, loss = 0.32296570\n",
      "Iteration 249, loss = 0.32284812\n",
      "Iteration 250, loss = 0.32163121\n",
      "Iteration 251, loss = 0.32098413\n",
      "Iteration 252, loss = 0.32190748\n",
      "Iteration 253, loss = 0.32184713\n",
      "Iteration 254, loss = 0.32176119\n",
      "Iteration 255, loss = 0.32314108\n",
      "Iteration 256, loss = 0.32175101\n",
      "Iteration 257, loss = 0.32177765\n",
      "Iteration 258, loss = 0.32115613\n",
      "Iteration 259, loss = 0.32066941\n",
      "Iteration 260, loss = 0.32103568\n",
      "Iteration 261, loss = 0.31971877\n",
      "Iteration 262, loss = 0.32139784\n",
      "Iteration 263, loss = 0.31936895\n",
      "Iteration 264, loss = 0.32107203\n",
      "Iteration 265, loss = 0.32087056\n",
      "Iteration 266, loss = 0.32035705\n",
      "Iteration 267, loss = 0.31984703\n",
      "Iteration 268, loss = 0.32018507\n",
      "Iteration 269, loss = 0.31985785\n",
      "Iteration 270, loss = 0.31963252\n",
      "Iteration 271, loss = 0.32035038\n",
      "Iteration 272, loss = 0.31868298\n",
      "Iteration 273, loss = 0.32071612\n",
      "Iteration 274, loss = 0.32002365\n",
      "Iteration 275, loss = 0.31968227\n",
      "Iteration 276, loss = 0.31843603\n",
      "Iteration 277, loss = 0.31975043\n",
      "Iteration 278, loss = 0.31904882\n",
      "Iteration 279, loss = 0.31779860\n",
      "Iteration 280, loss = 0.31856511\n",
      "Iteration 281, loss = 0.31864150\n",
      "Iteration 282, loss = 0.31793531\n",
      "Iteration 283, loss = 0.31966204\n",
      "Iteration 284, loss = 0.31811886\n",
      "Iteration 285, loss = 0.31885350\n",
      "Iteration 286, loss = 0.31883289\n",
      "Iteration 287, loss = 0.31958986\n",
      "Iteration 288, loss = 0.31787001\n",
      "Iteration 289, loss = 0.31663572\n",
      "Iteration 290, loss = 0.31876768\n",
      "Iteration 291, loss = 0.31773169\n",
      "Iteration 292, loss = 0.31773205\n",
      "Iteration 293, loss = 0.31717209\n",
      "Iteration 294, loss = 0.31797569\n",
      "Iteration 295, loss = 0.31915617\n",
      "Iteration 296, loss = 0.31698798\n",
      "Iteration 297, loss = 0.31889491\n",
      "Iteration 298, loss = 0.31637772\n",
      "Iteration 299, loss = 0.31669844\n",
      "Iteration 300, loss = 0.31661130\n",
      "Iteration 301, loss = 0.31787761\n",
      "Iteration 302, loss = 0.31633075\n",
      "Iteration 303, loss = 0.31731791\n",
      "Iteration 304, loss = 0.31788842\n",
      "Iteration 305, loss = 0.31613693\n",
      "Iteration 306, loss = 0.31591376\n",
      "Iteration 307, loss = 0.31510611\n",
      "Iteration 308, loss = 0.31560378\n",
      "Iteration 309, loss = 0.31597446\n",
      "Iteration 310, loss = 0.31693775\n",
      "Iteration 311, loss = 0.31556503\n",
      "Iteration 312, loss = 0.31570586\n",
      "Iteration 313, loss = 0.31573674\n",
      "Iteration 314, loss = 0.31544323\n",
      "Iteration 315, loss = 0.31453565\n",
      "Iteration 316, loss = 0.31521346\n",
      "Iteration 317, loss = 0.31577426\n",
      "Iteration 318, loss = 0.31493478\n",
      "Iteration 319, loss = 0.31632771\n",
      "Iteration 320, loss = 0.31430417\n",
      "Iteration 321, loss = 0.31496813\n",
      "Iteration 322, loss = 0.31503092\n",
      "Iteration 323, loss = 0.31544119\n",
      "Iteration 324, loss = 0.31501171\n",
      "Iteration 325, loss = 0.31444595\n",
      "Iteration 326, loss = 0.31443918\n",
      "Iteration 327, loss = 0.31453501\n",
      "Iteration 328, loss = 0.31436175\n",
      "Iteration 329, loss = 0.31482187\n",
      "Iteration 330, loss = 0.31447458\n",
      "Iteration 331, loss = 0.31344595\n",
      "Iteration 332, loss = 0.31454318\n",
      "Iteration 333, loss = 0.31492130\n",
      "Iteration 334, loss = 0.31328488\n",
      "Iteration 335, loss = 0.31400022\n",
      "Iteration 336, loss = 0.31390617\n",
      "Iteration 337, loss = 0.31463801\n",
      "Iteration 338, loss = 0.31308362\n",
      "Iteration 339, loss = 0.31380232\n",
      "Iteration 340, loss = 0.31213559\n",
      "Iteration 341, loss = 0.31350708\n",
      "Iteration 342, loss = 0.31264924\n",
      "Iteration 343, loss = 0.31419385\n",
      "Iteration 344, loss = 0.31307731\n",
      "Iteration 345, loss = 0.31361370\n",
      "Iteration 346, loss = 0.31244189\n",
      "Iteration 347, loss = 0.31396258\n",
      "Iteration 348, loss = 0.31272854\n",
      "Iteration 349, loss = 0.31289699\n",
      "Iteration 350, loss = 0.31270594\n",
      "Iteration 351, loss = 0.31228299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51422648\n",
      "Iteration 2, loss = 0.46644387\n",
      "Iteration 3, loss = 0.45899911\n",
      "Iteration 4, loss = 0.45336335\n",
      "Iteration 5, loss = 0.44953014\n",
      "Iteration 6, loss = 0.44518090\n",
      "Iteration 7, loss = 0.44174151\n",
      "Iteration 8, loss = 0.43824813\n",
      "Iteration 9, loss = 0.43452933\n",
      "Iteration 10, loss = 0.43170095\n",
      "Iteration 11, loss = 0.42925360\n",
      "Iteration 12, loss = 0.42631983\n",
      "Iteration 13, loss = 0.42451201\n",
      "Iteration 14, loss = 0.42182172\n",
      "Iteration 15, loss = 0.41950875\n",
      "Iteration 16, loss = 0.41805872\n",
      "Iteration 17, loss = 0.41568644\n",
      "Iteration 18, loss = 0.41417264\n",
      "Iteration 19, loss = 0.41245141\n",
      "Iteration 20, loss = 0.41071957\n",
      "Iteration 21, loss = 0.40971744\n",
      "Iteration 22, loss = 0.40740890\n",
      "Iteration 23, loss = 0.40673622\n",
      "Iteration 24, loss = 0.40425192\n",
      "Iteration 25, loss = 0.40394692\n",
      "Iteration 26, loss = 0.40186891\n",
      "Iteration 27, loss = 0.40075087\n",
      "Iteration 28, loss = 0.39963570\n",
      "Iteration 29, loss = 0.39815935\n",
      "Iteration 30, loss = 0.39716894\n",
      "Iteration 31, loss = 0.39641031\n",
      "Iteration 32, loss = 0.39460511\n",
      "Iteration 33, loss = 0.39367424\n",
      "Iteration 34, loss = 0.39278404\n",
      "Iteration 35, loss = 0.39307122\n",
      "Iteration 36, loss = 0.39071402\n",
      "Iteration 37, loss = 0.39023214\n",
      "Iteration 38, loss = 0.38879795\n",
      "Iteration 39, loss = 0.38839045\n",
      "Iteration 40, loss = 0.38816948\n",
      "Iteration 41, loss = 0.38616972\n",
      "Iteration 42, loss = 0.38553362\n",
      "Iteration 43, loss = 0.38495518\n",
      "Iteration 44, loss = 0.38430442\n",
      "Iteration 45, loss = 0.38290776\n",
      "Iteration 46, loss = 0.38291852\n",
      "Iteration 47, loss = 0.38132294\n",
      "Iteration 48, loss = 0.38125885\n",
      "Iteration 49, loss = 0.38021051\n",
      "Iteration 50, loss = 0.37836738\n",
      "Iteration 51, loss = 0.37817904\n",
      "Iteration 52, loss = 0.37807979\n",
      "Iteration 53, loss = 0.37682477\n",
      "Iteration 54, loss = 0.37618935\n",
      "Iteration 55, loss = 0.37521525\n",
      "Iteration 56, loss = 0.37509876\n",
      "Iteration 57, loss = 0.37388582\n",
      "Iteration 58, loss = 0.37386063\n",
      "Iteration 59, loss = 0.37260048\n",
      "Iteration 60, loss = 0.37209745\n",
      "Iteration 61, loss = 0.37179170\n",
      "Iteration 62, loss = 0.37061994\n",
      "Iteration 63, loss = 0.37045472\n",
      "Iteration 64, loss = 0.36899702\n",
      "Iteration 65, loss = 0.36917797\n",
      "Iteration 66, loss = 0.36761993\n",
      "Iteration 67, loss = 0.36743493\n",
      "Iteration 68, loss = 0.36744145\n",
      "Iteration 69, loss = 0.36647883\n",
      "Iteration 70, loss = 0.36576281\n",
      "Iteration 71, loss = 0.36615352\n",
      "Iteration 72, loss = 0.36436773\n",
      "Iteration 73, loss = 0.36505319\n",
      "Iteration 74, loss = 0.36355325\n",
      "Iteration 75, loss = 0.36306421\n",
      "Iteration 76, loss = 0.36275064\n",
      "Iteration 77, loss = 0.36207130\n",
      "Iteration 78, loss = 0.36131588\n",
      "Iteration 79, loss = 0.36107503\n",
      "Iteration 80, loss = 0.36093679\n",
      "Iteration 81, loss = 0.35971714\n",
      "Iteration 82, loss = 0.35944079\n",
      "Iteration 83, loss = 0.35906527\n",
      "Iteration 84, loss = 0.35897877\n",
      "Iteration 85, loss = 0.35905534\n",
      "Iteration 86, loss = 0.35857896\n",
      "Iteration 87, loss = 0.35721946\n",
      "Iteration 88, loss = 0.35778572\n",
      "Iteration 89, loss = 0.35678096\n",
      "Iteration 90, loss = 0.35679220\n",
      "Iteration 91, loss = 0.35580306\n",
      "Iteration 92, loss = 0.35643410\n",
      "Iteration 93, loss = 0.35429553\n",
      "Iteration 94, loss = 0.35411987\n",
      "Iteration 95, loss = 0.35392128\n",
      "Iteration 96, loss = 0.35433110\n",
      "Iteration 97, loss = 0.35347510\n",
      "Iteration 98, loss = 0.35254946\n",
      "Iteration 99, loss = 0.35291081\n",
      "Iteration 100, loss = 0.35240871\n",
      "Iteration 101, loss = 0.35123566\n",
      "Iteration 102, loss = 0.35195405\n",
      "Iteration 103, loss = 0.35177697\n",
      "Iteration 104, loss = 0.35070597\n",
      "Iteration 105, loss = 0.35067538\n",
      "Iteration 106, loss = 0.35033885\n",
      "Iteration 107, loss = 0.34995235\n",
      "Iteration 108, loss = 0.35021423\n",
      "Iteration 109, loss = 0.34916654\n",
      "Iteration 110, loss = 0.34966155\n",
      "Iteration 111, loss = 0.34872678\n",
      "Iteration 112, loss = 0.34856784\n",
      "Iteration 113, loss = 0.34811029\n",
      "Iteration 114, loss = 0.34766797\n",
      "Iteration 115, loss = 0.34836541\n",
      "Iteration 116, loss = 0.34707967\n",
      "Iteration 117, loss = 0.34711238\n",
      "Iteration 118, loss = 0.34606104\n",
      "Iteration 119, loss = 0.34708895\n",
      "Iteration 120, loss = 0.34594915\n",
      "Iteration 121, loss = 0.34581104\n",
      "Iteration 122, loss = 0.34498159\n",
      "Iteration 123, loss = 0.34478089\n",
      "Iteration 124, loss = 0.34524260\n",
      "Iteration 125, loss = 0.34444046\n",
      "Iteration 126, loss = 0.34493312\n",
      "Iteration 127, loss = 0.34418243\n",
      "Iteration 128, loss = 0.34450515\n",
      "Iteration 129, loss = 0.34338766\n",
      "Iteration 130, loss = 0.34363738\n",
      "Iteration 131, loss = 0.34319413\n",
      "Iteration 132, loss = 0.34317101\n",
      "Iteration 133, loss = 0.34311504\n",
      "Iteration 134, loss = 0.34222501\n",
      "Iteration 135, loss = 0.34280058\n",
      "Iteration 136, loss = 0.34180616\n",
      "Iteration 137, loss = 0.34084360\n",
      "Iteration 138, loss = 0.34180191\n",
      "Iteration 139, loss = 0.34077968\n",
      "Iteration 140, loss = 0.34117456\n",
      "Iteration 141, loss = 0.34080544\n",
      "Iteration 142, loss = 0.33998325\n",
      "Iteration 143, loss = 0.34099728\n",
      "Iteration 144, loss = 0.33959909\n",
      "Iteration 145, loss = 0.33972709\n",
      "Iteration 146, loss = 0.33849927\n",
      "Iteration 147, loss = 0.33892627\n",
      "Iteration 148, loss = 0.33868635\n",
      "Iteration 149, loss = 0.33759638\n",
      "Iteration 150, loss = 0.33812051\n",
      "Iteration 151, loss = 0.33816357\n",
      "Iteration 152, loss = 0.33729212\n",
      "Iteration 153, loss = 0.33831667\n",
      "Iteration 154, loss = 0.33755005\n",
      "Iteration 155, loss = 0.33629500\n",
      "Iteration 156, loss = 0.33780146\n",
      "Iteration 157, loss = 0.33643416\n",
      "Iteration 158, loss = 0.33643218\n",
      "Iteration 159, loss = 0.33566508\n",
      "Iteration 160, loss = 0.33727747\n",
      "Iteration 161, loss = 0.33522879\n",
      "Iteration 162, loss = 0.33614026\n",
      "Iteration 163, loss = 0.33520145\n",
      "Iteration 164, loss = 0.33535187\n",
      "Iteration 165, loss = 0.33402272\n",
      "Iteration 166, loss = 0.33538093\n",
      "Iteration 167, loss = 0.33403021\n",
      "Iteration 168, loss = 0.33400769\n",
      "Iteration 169, loss = 0.33422071\n",
      "Iteration 170, loss = 0.33436739\n",
      "Iteration 171, loss = 0.33313909\n",
      "Iteration 172, loss = 0.33374367\n",
      "Iteration 173, loss = 0.33304889\n",
      "Iteration 174, loss = 0.33343913\n",
      "Iteration 175, loss = 0.33265905\n",
      "Iteration 176, loss = 0.33363694\n",
      "Iteration 177, loss = 0.33307355\n",
      "Iteration 178, loss = 0.33307533\n",
      "Iteration 179, loss = 0.33189177\n",
      "Iteration 180, loss = 0.33170431\n",
      "Iteration 181, loss = 0.33271599\n",
      "Iteration 182, loss = 0.33192751\n",
      "Iteration 183, loss = 0.33198967\n",
      "Iteration 184, loss = 0.33033940\n",
      "Iteration 185, loss = 0.33102536\n",
      "Iteration 186, loss = 0.33091851\n",
      "Iteration 187, loss = 0.33047160\n",
      "Iteration 188, loss = 0.33068099\n",
      "Iteration 189, loss = 0.33036626\n",
      "Iteration 190, loss = 0.33024952\n",
      "Iteration 191, loss = 0.32970574\n",
      "Iteration 192, loss = 0.32983257\n",
      "Iteration 193, loss = 0.33042015\n",
      "Iteration 194, loss = 0.32967924\n",
      "Iteration 195, loss = 0.32969778\n",
      "Iteration 196, loss = 0.32923872\n",
      "Iteration 197, loss = 0.32866073\n",
      "Iteration 198, loss = 0.32963381\n",
      "Iteration 199, loss = 0.32894911\n",
      "Iteration 200, loss = 0.32843448\n",
      "Iteration 201, loss = 0.32800725\n",
      "Iteration 202, loss = 0.32814085\n",
      "Iteration 203, loss = 0.32818551\n",
      "Iteration 204, loss = 0.32855223\n",
      "Iteration 205, loss = 0.32778615\n",
      "Iteration 206, loss = 0.32705505\n",
      "Iteration 207, loss = 0.32648570\n",
      "Iteration 208, loss = 0.32797896\n",
      "Iteration 209, loss = 0.32660449\n",
      "Iteration 210, loss = 0.32760091\n",
      "Iteration 211, loss = 0.32726335\n",
      "Iteration 212, loss = 0.32601299\n",
      "Iteration 213, loss = 0.32648640\n",
      "Iteration 214, loss = 0.32704760\n",
      "Iteration 215, loss = 0.32638734\n",
      "Iteration 216, loss = 0.32640678\n",
      "Iteration 217, loss = 0.32611958\n",
      "Iteration 218, loss = 0.32589137\n",
      "Iteration 219, loss = 0.32631142\n",
      "Iteration 220, loss = 0.32514969\n",
      "Iteration 221, loss = 0.32536119\n",
      "Iteration 222, loss = 0.32457322\n",
      "Iteration 223, loss = 0.32527418\n",
      "Iteration 224, loss = 0.32539126\n",
      "Iteration 225, loss = 0.32429549\n",
      "Iteration 226, loss = 0.32472737\n",
      "Iteration 227, loss = 0.32412176\n",
      "Iteration 228, loss = 0.32439959\n",
      "Iteration 229, loss = 0.32413728\n",
      "Iteration 230, loss = 0.32424636\n",
      "Iteration 231, loss = 0.32352986\n",
      "Iteration 232, loss = 0.32466302\n",
      "Iteration 233, loss = 0.32414178\n",
      "Iteration 234, loss = 0.32281613\n",
      "Iteration 235, loss = 0.32356941\n",
      "Iteration 236, loss = 0.32272770\n",
      "Iteration 237, loss = 0.32387479\n",
      "Iteration 238, loss = 0.32316131\n",
      "Iteration 239, loss = 0.32481377\n",
      "Iteration 240, loss = 0.32311362\n",
      "Iteration 241, loss = 0.32399932\n",
      "Iteration 242, loss = 0.32320301\n",
      "Iteration 243, loss = 0.32220685\n",
      "Iteration 244, loss = 0.32111655\n",
      "Iteration 245, loss = 0.32339820\n",
      "Iteration 246, loss = 0.32198519\n",
      "Iteration 247, loss = 0.32267552\n",
      "Iteration 248, loss = 0.32314086\n",
      "Iteration 249, loss = 0.32124965\n",
      "Iteration 250, loss = 0.32142857\n",
      "Iteration 251, loss = 0.32109516\n",
      "Iteration 252, loss = 0.32066768\n",
      "Iteration 253, loss = 0.32156736\n",
      "Iteration 254, loss = 0.32203355\n",
      "Iteration 255, loss = 0.32053248\n",
      "Iteration 256, loss = 0.32121701\n",
      "Iteration 257, loss = 0.32091157\n",
      "Iteration 258, loss = 0.32155262\n",
      "Iteration 259, loss = 0.32065023\n",
      "Iteration 260, loss = 0.32064897\n",
      "Iteration 261, loss = 0.32099395\n",
      "Iteration 262, loss = 0.31963100\n",
      "Iteration 263, loss = 0.32152445\n",
      "Iteration 264, loss = 0.31972876\n",
      "Iteration 265, loss = 0.32074070\n",
      "Iteration 266, loss = 0.31982606\n",
      "Iteration 267, loss = 0.31991628\n",
      "Iteration 268, loss = 0.31925579\n",
      "Iteration 269, loss = 0.31902348\n",
      "Iteration 270, loss = 0.31874987\n",
      "Iteration 271, loss = 0.31882287\n",
      "Iteration 272, loss = 0.31905216\n",
      "Iteration 273, loss = 0.31931821\n",
      "Iteration 274, loss = 0.31854348\n",
      "Iteration 275, loss = 0.32038644\n",
      "Iteration 276, loss = 0.31916483\n",
      "Iteration 277, loss = 0.31889576\n",
      "Iteration 278, loss = 0.31834186\n",
      "Iteration 279, loss = 0.31818173\n",
      "Iteration 280, loss = 0.31772099\n",
      "Iteration 281, loss = 0.31984866\n",
      "Iteration 282, loss = 0.31891457\n",
      "Iteration 283, loss = 0.31803899\n",
      "Iteration 284, loss = 0.31841345\n",
      "Iteration 285, loss = 0.31733731\n",
      "Iteration 286, loss = 0.31666398\n",
      "Iteration 287, loss = 0.31804426\n",
      "Iteration 288, loss = 0.31715090\n",
      "Iteration 289, loss = 0.31614272\n",
      "Iteration 290, loss = 0.31717135\n",
      "Iteration 291, loss = 0.31782145\n",
      "Iteration 292, loss = 0.31821611\n",
      "Iteration 293, loss = 0.31780820\n",
      "Iteration 294, loss = 0.31656583\n",
      "Iteration 295, loss = 0.31580973\n",
      "Iteration 296, loss = 0.31588324\n",
      "Iteration 297, loss = 0.31628986\n",
      "Iteration 298, loss = 0.31638758\n",
      "Iteration 299, loss = 0.31666692\n",
      "Iteration 300, loss = 0.31779094\n",
      "Iteration 301, loss = 0.31605128\n",
      "Iteration 302, loss = 0.31527331\n",
      "Iteration 303, loss = 0.31571248\n",
      "Iteration 304, loss = 0.31560021\n",
      "Iteration 305, loss = 0.31567576\n",
      "Iteration 306, loss = 0.31645821\n",
      "Iteration 307, loss = 0.31576391\n",
      "Iteration 308, loss = 0.31544935\n",
      "Iteration 309, loss = 0.31551867\n",
      "Iteration 310, loss = 0.31508297\n",
      "Iteration 311, loss = 0.31470897\n",
      "Iteration 312, loss = 0.31577369\n",
      "Iteration 313, loss = 0.31524865\n",
      "Iteration 314, loss = 0.31457247\n",
      "Iteration 315, loss = 0.31538842\n",
      "Iteration 316, loss = 0.31560823\n",
      "Iteration 317, loss = 0.31402054\n",
      "Iteration 318, loss = 0.31478803\n",
      "Iteration 319, loss = 0.31467394\n",
      "Iteration 320, loss = 0.31388218\n",
      "Iteration 321, loss = 0.31445154\n",
      "Iteration 322, loss = 0.31450173\n",
      "Iteration 323, loss = 0.31459861\n",
      "Iteration 324, loss = 0.31355853\n",
      "Iteration 325, loss = 0.31292234\n",
      "Iteration 326, loss = 0.31413824\n",
      "Iteration 327, loss = 0.31336959\n",
      "Iteration 328, loss = 0.31395169\n",
      "Iteration 329, loss = 0.31303297\n",
      "Iteration 330, loss = 0.31379780\n",
      "Iteration 331, loss = 0.31223406\n",
      "Iteration 332, loss = 0.31253062\n",
      "Iteration 333, loss = 0.31366190\n",
      "Iteration 334, loss = 0.31295270\n",
      "Iteration 335, loss = 0.31429813\n",
      "Iteration 336, loss = 0.31208650\n",
      "Iteration 337, loss = 0.31314249\n",
      "Iteration 338, loss = 0.31263033\n",
      "Iteration 339, loss = 0.31341147\n",
      "Iteration 340, loss = 0.31206551\n",
      "Iteration 341, loss = 0.31138864\n",
      "Iteration 342, loss = 0.31305612\n",
      "Iteration 343, loss = 0.31246224\n",
      "Iteration 344, loss = 0.31183239\n",
      "Iteration 345, loss = 0.31284612\n",
      "Iteration 346, loss = 0.31171256\n",
      "Iteration 347, loss = 0.31202921\n",
      "Iteration 348, loss = 0.31157300\n",
      "Iteration 349, loss = 0.31139723\n",
      "Iteration 350, loss = 0.31215903\n",
      "Iteration 351, loss = 0.31292822\n",
      "Iteration 352, loss = 0.31130177\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51878829\n",
      "Iteration 2, loss = 0.46879543\n",
      "Iteration 3, loss = 0.46044549\n",
      "Iteration 4, loss = 0.45478423\n",
      "Iteration 5, loss = 0.45135995\n",
      "Iteration 6, loss = 0.44738894\n",
      "Iteration 7, loss = 0.44472471\n",
      "Iteration 8, loss = 0.44122440\n",
      "Iteration 9, loss = 0.43878626\n",
      "Iteration 10, loss = 0.43596039\n",
      "Iteration 11, loss = 0.43310585\n",
      "Iteration 12, loss = 0.43065778\n",
      "Iteration 13, loss = 0.42849296\n",
      "Iteration 14, loss = 0.42722472\n",
      "Iteration 15, loss = 0.42432528\n",
      "Iteration 16, loss = 0.42255407\n",
      "Iteration 17, loss = 0.42074796\n",
      "Iteration 18, loss = 0.41927414\n",
      "Iteration 19, loss = 0.41751029\n",
      "Iteration 20, loss = 0.41612679\n",
      "Iteration 21, loss = 0.41459598\n",
      "Iteration 22, loss = 0.41383066\n",
      "Iteration 23, loss = 0.41212355\n",
      "Iteration 24, loss = 0.41112834\n",
      "Iteration 25, loss = 0.40985620\n",
      "Iteration 26, loss = 0.40874234\n",
      "Iteration 27, loss = 0.40776279\n",
      "Iteration 28, loss = 0.40654708\n",
      "Iteration 29, loss = 0.40568412\n",
      "Iteration 30, loss = 0.40428524\n",
      "Iteration 31, loss = 0.40361692\n",
      "Iteration 32, loss = 0.40249674\n",
      "Iteration 33, loss = 0.40120867\n",
      "Iteration 34, loss = 0.39980206\n",
      "Iteration 35, loss = 0.39917418\n",
      "Iteration 36, loss = 0.39811560\n",
      "Iteration 37, loss = 0.39700624\n",
      "Iteration 38, loss = 0.39631526\n",
      "Iteration 39, loss = 0.39569888\n",
      "Iteration 40, loss = 0.39381977\n",
      "Iteration 41, loss = 0.39399171\n",
      "Iteration 42, loss = 0.39251811\n",
      "Iteration 43, loss = 0.39115757\n",
      "Iteration 44, loss = 0.39069592\n",
      "Iteration 45, loss = 0.38981389\n",
      "Iteration 46, loss = 0.38881248\n",
      "Iteration 47, loss = 0.38878982\n",
      "Iteration 48, loss = 0.38765363\n",
      "Iteration 49, loss = 0.38612513\n",
      "Iteration 50, loss = 0.38555338\n",
      "Iteration 51, loss = 0.38463345\n",
      "Iteration 52, loss = 0.38405492\n",
      "Iteration 53, loss = 0.38349857\n",
      "Iteration 54, loss = 0.38346227\n",
      "Iteration 55, loss = 0.38204506\n",
      "Iteration 56, loss = 0.38111629\n",
      "Iteration 57, loss = 0.38081657\n",
      "Iteration 58, loss = 0.38087263\n",
      "Iteration 59, loss = 0.37955510\n",
      "Iteration 60, loss = 0.37860104\n",
      "Iteration 61, loss = 0.37904656\n",
      "Iteration 62, loss = 0.37769211\n",
      "Iteration 63, loss = 0.37756946\n",
      "Iteration 64, loss = 0.37607004\n",
      "Iteration 65, loss = 0.37649433\n",
      "Iteration 66, loss = 0.37515166\n",
      "Iteration 67, loss = 0.37407134\n",
      "Iteration 68, loss = 0.37410622\n",
      "Iteration 69, loss = 0.37445958\n",
      "Iteration 70, loss = 0.37309919\n",
      "Iteration 71, loss = 0.37255226\n",
      "Iteration 72, loss = 0.37190110\n",
      "Iteration 73, loss = 0.37081830\n",
      "Iteration 74, loss = 0.37135711\n",
      "Iteration 75, loss = 0.37039204\n",
      "Iteration 76, loss = 0.36973983\n",
      "Iteration 77, loss = 0.36907900\n",
      "Iteration 78, loss = 0.36878095\n",
      "Iteration 79, loss = 0.36798777\n",
      "Iteration 80, loss = 0.36831509\n",
      "Iteration 81, loss = 0.36830172\n",
      "Iteration 82, loss = 0.36718078\n",
      "Iteration 83, loss = 0.36692053\n",
      "Iteration 84, loss = 0.36689917\n",
      "Iteration 85, loss = 0.36576815\n",
      "Iteration 86, loss = 0.36583095\n",
      "Iteration 87, loss = 0.36531154\n",
      "Iteration 88, loss = 0.36519651\n",
      "Iteration 89, loss = 0.36382027\n",
      "Iteration 90, loss = 0.36376737\n",
      "Iteration 91, loss = 0.36307494\n",
      "Iteration 92, loss = 0.36346929\n",
      "Iteration 93, loss = 0.36263643\n",
      "Iteration 94, loss = 0.36173072\n",
      "Iteration 95, loss = 0.36202555\n",
      "Iteration 96, loss = 0.36263232\n",
      "Iteration 97, loss = 0.36106002\n",
      "Iteration 98, loss = 0.36201138\n",
      "Iteration 99, loss = 0.36106499\n",
      "Iteration 100, loss = 0.36004150\n",
      "Iteration 101, loss = 0.36073714\n",
      "Iteration 102, loss = 0.36021782\n",
      "Iteration 103, loss = 0.35899858\n",
      "Iteration 104, loss = 0.35839679\n",
      "Iteration 105, loss = 0.35843755\n",
      "Iteration 106, loss = 0.35837791\n",
      "Iteration 107, loss = 0.35835886\n",
      "Iteration 108, loss = 0.35782121\n",
      "Iteration 109, loss = 0.35803883\n",
      "Iteration 110, loss = 0.35774331\n",
      "Iteration 111, loss = 0.35689905\n",
      "Iteration 112, loss = 0.35661318\n",
      "Iteration 113, loss = 0.35629476\n",
      "Iteration 114, loss = 0.35587522\n",
      "Iteration 115, loss = 0.35639347\n",
      "Iteration 116, loss = 0.35498788\n",
      "Iteration 117, loss = 0.35579365\n",
      "Iteration 118, loss = 0.35512752\n",
      "Iteration 119, loss = 0.35414052\n",
      "Iteration 120, loss = 0.35398596\n",
      "Iteration 121, loss = 0.35379039\n",
      "Iteration 122, loss = 0.35362156\n",
      "Iteration 123, loss = 0.35374860\n",
      "Iteration 124, loss = 0.35389773\n",
      "Iteration 125, loss = 0.35265056\n",
      "Iteration 126, loss = 0.35282477\n",
      "Iteration 127, loss = 0.35291914\n",
      "Iteration 128, loss = 0.35220265\n",
      "Iteration 129, loss = 0.35220734\n",
      "Iteration 130, loss = 0.35147958\n",
      "Iteration 131, loss = 0.35151068\n",
      "Iteration 132, loss = 0.35061627\n",
      "Iteration 133, loss = 0.35065554\n",
      "Iteration 134, loss = 0.35088068\n",
      "Iteration 135, loss = 0.35035967\n",
      "Iteration 136, loss = 0.35058205\n",
      "Iteration 137, loss = 0.35018507\n",
      "Iteration 138, loss = 0.35007295\n",
      "Iteration 139, loss = 0.34957277\n",
      "Iteration 140, loss = 0.34892760\n",
      "Iteration 141, loss = 0.34842630\n",
      "Iteration 142, loss = 0.34869313\n",
      "Iteration 143, loss = 0.34863776\n",
      "Iteration 144, loss = 0.34802589\n",
      "Iteration 145, loss = 0.34796088\n",
      "Iteration 146, loss = 0.34749414\n",
      "Iteration 147, loss = 0.34857346\n",
      "Iteration 148, loss = 0.34830407\n",
      "Iteration 149, loss = 0.34707993\n",
      "Iteration 150, loss = 0.34736975\n",
      "Iteration 151, loss = 0.34678719\n",
      "Iteration 152, loss = 0.34623669\n",
      "Iteration 153, loss = 0.34712295\n",
      "Iteration 154, loss = 0.34662290\n",
      "Iteration 155, loss = 0.34579874\n",
      "Iteration 156, loss = 0.34609482\n",
      "Iteration 157, loss = 0.34670899\n",
      "Iteration 158, loss = 0.34587767\n",
      "Iteration 159, loss = 0.34539257\n",
      "Iteration 160, loss = 0.34482905\n",
      "Iteration 161, loss = 0.34491813\n",
      "Iteration 162, loss = 0.34496353\n",
      "Iteration 163, loss = 0.34486355\n",
      "Iteration 164, loss = 0.34528459\n",
      "Iteration 165, loss = 0.34304342\n",
      "Iteration 166, loss = 0.34348714\n",
      "Iteration 167, loss = 0.34432458\n",
      "Iteration 168, loss = 0.34439458\n",
      "Iteration 169, loss = 0.34239438\n",
      "Iteration 170, loss = 0.34217831\n",
      "Iteration 171, loss = 0.34227491\n",
      "Iteration 172, loss = 0.34272820\n",
      "Iteration 173, loss = 0.34277025\n",
      "Iteration 174, loss = 0.34234318\n",
      "Iteration 175, loss = 0.34197633\n",
      "Iteration 176, loss = 0.34265666\n",
      "Iteration 177, loss = 0.34156788\n",
      "Iteration 178, loss = 0.34140728\n",
      "Iteration 179, loss = 0.34125047\n",
      "Iteration 180, loss = 0.34019953\n",
      "Iteration 181, loss = 0.34061605\n",
      "Iteration 182, loss = 0.34105942\n",
      "Iteration 183, loss = 0.34145228\n",
      "Iteration 184, loss = 0.34081813\n",
      "Iteration 185, loss = 0.33985572\n",
      "Iteration 186, loss = 0.34073332\n",
      "Iteration 187, loss = 0.34074613\n",
      "Iteration 188, loss = 0.33968019\n",
      "Iteration 189, loss = 0.34077730\n",
      "Iteration 190, loss = 0.34012537\n",
      "Iteration 191, loss = 0.33917385\n",
      "Iteration 192, loss = 0.33923225\n",
      "Iteration 193, loss = 0.33976246\n",
      "Iteration 194, loss = 0.33897541\n",
      "Iteration 195, loss = 0.33929674\n",
      "Iteration 196, loss = 0.33923710\n",
      "Iteration 197, loss = 0.33926510\n",
      "Iteration 198, loss = 0.33788586\n",
      "Iteration 199, loss = 0.33776008\n",
      "Iteration 200, loss = 0.33973162\n",
      "Iteration 201, loss = 0.33782591\n",
      "Iteration 202, loss = 0.33836581\n",
      "Iteration 203, loss = 0.33782985\n",
      "Iteration 204, loss = 0.33712856\n",
      "Iteration 205, loss = 0.33758556\n",
      "Iteration 206, loss = 0.33686302\n",
      "Iteration 207, loss = 0.33698644\n",
      "Iteration 208, loss = 0.33759938\n",
      "Iteration 209, loss = 0.33734195\n",
      "Iteration 210, loss = 0.33700877\n",
      "Iteration 211, loss = 0.33526375\n",
      "Iteration 212, loss = 0.33665924\n",
      "Iteration 213, loss = 0.33520458\n",
      "Iteration 214, loss = 0.33620834\n",
      "Iteration 215, loss = 0.33697343\n",
      "Iteration 216, loss = 0.33554676\n",
      "Iteration 217, loss = 0.33607633\n",
      "Iteration 218, loss = 0.33567124\n",
      "Iteration 219, loss = 0.33604308\n",
      "Iteration 220, loss = 0.33560059\n",
      "Iteration 221, loss = 0.33515922\n",
      "Iteration 222, loss = 0.33476885\n",
      "Iteration 223, loss = 0.33566688\n",
      "Iteration 224, loss = 0.33506138\n",
      "Iteration 225, loss = 0.33456053\n",
      "Iteration 226, loss = 0.33501048\n",
      "Iteration 227, loss = 0.33468701\n",
      "Iteration 228, loss = 0.33424342\n",
      "Iteration 229, loss = 0.33435491\n",
      "Iteration 230, loss = 0.33473620\n",
      "Iteration 231, loss = 0.33366421\n",
      "Iteration 232, loss = 0.33362066\n",
      "Iteration 233, loss = 0.33469488\n",
      "Iteration 234, loss = 0.33405015\n",
      "Iteration 235, loss = 0.33412149\n",
      "Iteration 236, loss = 0.33383224\n",
      "Iteration 237, loss = 0.33378637\n",
      "Iteration 238, loss = 0.33372839\n",
      "Iteration 239, loss = 0.33336387\n",
      "Iteration 240, loss = 0.33255170\n",
      "Iteration 241, loss = 0.33327612\n",
      "Iteration 242, loss = 0.33351921\n",
      "Iteration 243, loss = 0.33286389\n",
      "Iteration 244, loss = 0.33288014\n",
      "Iteration 245, loss = 0.33243697\n",
      "Iteration 246, loss = 0.33194573\n",
      "Iteration 247, loss = 0.33210893\n",
      "Iteration 248, loss = 0.33173109\n",
      "Iteration 249, loss = 0.33119792\n",
      "Iteration 250, loss = 0.33101295\n",
      "Iteration 251, loss = 0.33243010\n",
      "Iteration 252, loss = 0.33199676\n",
      "Iteration 253, loss = 0.33104777\n",
      "Iteration 254, loss = 0.33205089\n",
      "Iteration 255, loss = 0.33040881\n",
      "Iteration 256, loss = 0.33084746\n",
      "Iteration 257, loss = 0.33215412\n",
      "Iteration 258, loss = 0.33182368\n",
      "Iteration 259, loss = 0.33166869\n",
      "Iteration 260, loss = 0.33182204\n",
      "Iteration 261, loss = 0.33167254\n",
      "Iteration 262, loss = 0.33037782\n",
      "Iteration 263, loss = 0.33039955\n",
      "Iteration 264, loss = 0.32974561\n",
      "Iteration 265, loss = 0.33015785\n",
      "Iteration 266, loss = 0.32969248\n",
      "Iteration 267, loss = 0.33101893\n",
      "Iteration 268, loss = 0.33029438\n",
      "Iteration 269, loss = 0.32959608\n",
      "Iteration 270, loss = 0.32933467\n",
      "Iteration 271, loss = 0.33016945\n",
      "Iteration 272, loss = 0.33016841\n",
      "Iteration 273, loss = 0.32889706\n",
      "Iteration 274, loss = 0.32972716\n",
      "Iteration 275, loss = 0.33005509\n",
      "Iteration 276, loss = 0.32885131\n",
      "Iteration 277, loss = 0.33017597\n",
      "Iteration 278, loss = 0.32900206\n",
      "Iteration 279, loss = 0.32933130\n",
      "Iteration 280, loss = 0.32933809\n",
      "Iteration 281, loss = 0.32829938\n",
      "Iteration 282, loss = 0.32899900\n",
      "Iteration 283, loss = 0.32843593\n",
      "Iteration 284, loss = 0.32802591\n",
      "Iteration 285, loss = 0.32921811\n",
      "Iteration 286, loss = 0.32813320\n",
      "Iteration 287, loss = 0.32786406\n",
      "Iteration 288, loss = 0.32794094\n",
      "Iteration 289, loss = 0.32892232\n",
      "Iteration 290, loss = 0.32822413\n",
      "Iteration 291, loss = 0.32820370\n",
      "Iteration 292, loss = 0.32804206\n",
      "Iteration 293, loss = 0.32746728\n",
      "Iteration 294, loss = 0.32689297\n",
      "Iteration 295, loss = 0.32769841\n",
      "Iteration 296, loss = 0.32819935\n",
      "Iteration 297, loss = 0.32619375\n",
      "Iteration 298, loss = 0.32804655\n",
      "Iteration 299, loss = 0.32726865\n",
      "Iteration 300, loss = 0.32725689\n",
      "Iteration 301, loss = 0.32688397\n",
      "Iteration 302, loss = 0.32761424\n",
      "Iteration 303, loss = 0.32717715\n",
      "Iteration 304, loss = 0.32654557\n",
      "Iteration 305, loss = 0.32818875\n",
      "Iteration 306, loss = 0.32628272\n",
      "Iteration 307, loss = 0.32746713\n",
      "Iteration 308, loss = 0.32598169\n",
      "Iteration 309, loss = 0.32642220\n",
      "Iteration 310, loss = 0.32661127\n",
      "Iteration 311, loss = 0.32566396\n",
      "Iteration 312, loss = 0.32640995\n",
      "Iteration 313, loss = 0.32626826\n",
      "Iteration 314, loss = 0.32509983\n",
      "Iteration 315, loss = 0.32541642\n",
      "Iteration 316, loss = 0.32565730\n",
      "Iteration 317, loss = 0.32683428\n",
      "Iteration 318, loss = 0.32581667\n",
      "Iteration 319, loss = 0.32576593\n",
      "Iteration 320, loss = 0.32572020\n",
      "Iteration 321, loss = 0.32618396\n",
      "Iteration 322, loss = 0.32580207\n",
      "Iteration 323, loss = 0.32516669\n",
      "Iteration 324, loss = 0.32479703\n",
      "Iteration 325, loss = 0.32534793\n",
      "Iteration 326, loss = 0.32422388\n",
      "Iteration 327, loss = 0.32483285\n",
      "Iteration 328, loss = 0.32439586\n",
      "Iteration 329, loss = 0.32514871\n",
      "Iteration 330, loss = 0.32422277\n",
      "Iteration 331, loss = 0.32532679\n",
      "Iteration 332, loss = 0.32454959\n",
      "Iteration 333, loss = 0.32380248\n",
      "Iteration 334, loss = 0.32438144\n",
      "Iteration 335, loss = 0.32440940\n",
      "Iteration 336, loss = 0.32458480\n",
      "Iteration 337, loss = 0.32370205\n",
      "Iteration 338, loss = 0.32435766\n",
      "Iteration 339, loss = 0.32394736\n",
      "Iteration 340, loss = 0.32518071\n",
      "Iteration 341, loss = 0.32367150\n",
      "Iteration 342, loss = 0.32296285\n",
      "Iteration 343, loss = 0.32319015\n",
      "Iteration 344, loss = 0.32364130\n",
      "Iteration 345, loss = 0.32396164\n",
      "Iteration 346, loss = 0.32506735\n",
      "Iteration 347, loss = 0.32284391\n",
      "Iteration 348, loss = 0.32292078\n",
      "Iteration 349, loss = 0.32429241\n",
      "Iteration 350, loss = 0.32318715\n",
      "Iteration 351, loss = 0.32274253\n",
      "Iteration 352, loss = 0.32337235\n",
      "Iteration 353, loss = 0.32312226\n",
      "Iteration 354, loss = 0.32351683\n",
      "Iteration 355, loss = 0.32319325\n",
      "Iteration 356, loss = 0.32218520\n",
      "Iteration 357, loss = 0.32270320\n",
      "Iteration 358, loss = 0.32346826\n",
      "Iteration 359, loss = 0.32299667\n",
      "Iteration 360, loss = 0.32311750\n",
      "Iteration 361, loss = 0.32272674\n",
      "Iteration 362, loss = 0.32208895\n",
      "Iteration 363, loss = 0.32275228\n",
      "Iteration 364, loss = 0.32209786\n",
      "Iteration 365, loss = 0.32265127\n",
      "Iteration 366, loss = 0.32401664\n",
      "Iteration 367, loss = 0.32229029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51276833\n",
      "Iteration 2, loss = 0.46591827\n",
      "Iteration 3, loss = 0.45690513\n",
      "Iteration 4, loss = 0.45078628\n",
      "Iteration 5, loss = 0.44543562\n",
      "Iteration 6, loss = 0.44088596\n",
      "Iteration 7, loss = 0.43661710\n",
      "Iteration 8, loss = 0.43275257\n",
      "Iteration 9, loss = 0.42914190\n",
      "Iteration 10, loss = 0.42618008\n",
      "Iteration 11, loss = 0.42320515\n",
      "Iteration 12, loss = 0.42007646\n",
      "Iteration 13, loss = 0.41796591\n",
      "Iteration 14, loss = 0.41650245\n",
      "Iteration 15, loss = 0.41321256\n",
      "Iteration 16, loss = 0.41227498\n",
      "Iteration 17, loss = 0.41013930\n",
      "Iteration 18, loss = 0.40852049\n",
      "Iteration 19, loss = 0.40690146\n",
      "Iteration 20, loss = 0.40540355\n",
      "Iteration 21, loss = 0.40375849\n",
      "Iteration 22, loss = 0.40298509\n",
      "Iteration 23, loss = 0.40146386\n",
      "Iteration 24, loss = 0.39983389\n",
      "Iteration 25, loss = 0.39897225\n",
      "Iteration 26, loss = 0.39828442\n",
      "Iteration 27, loss = 0.39718000\n",
      "Iteration 28, loss = 0.39579072\n",
      "Iteration 29, loss = 0.39478194\n",
      "Iteration 30, loss = 0.39363971\n",
      "Iteration 31, loss = 0.39253407\n",
      "Iteration 32, loss = 0.39125670\n",
      "Iteration 33, loss = 0.38994839\n",
      "Iteration 34, loss = 0.38947130\n",
      "Iteration 35, loss = 0.38940966\n",
      "Iteration 36, loss = 0.38771159\n",
      "Iteration 37, loss = 0.38650987\n",
      "Iteration 38, loss = 0.38515311\n",
      "Iteration 39, loss = 0.38497407\n",
      "Iteration 40, loss = 0.38415342\n",
      "Iteration 41, loss = 0.38366194\n",
      "Iteration 42, loss = 0.38239403\n",
      "Iteration 43, loss = 0.38096635\n",
      "Iteration 44, loss = 0.38027178\n",
      "Iteration 45, loss = 0.37969584\n",
      "Iteration 46, loss = 0.37869452\n",
      "Iteration 47, loss = 0.37885647\n",
      "Iteration 48, loss = 0.37717559\n",
      "Iteration 49, loss = 0.37619908\n",
      "Iteration 50, loss = 0.37509800\n",
      "Iteration 51, loss = 0.37445814\n",
      "Iteration 52, loss = 0.37358222\n",
      "Iteration 53, loss = 0.37310510\n",
      "Iteration 54, loss = 0.37267130\n",
      "Iteration 55, loss = 0.37149951\n",
      "Iteration 56, loss = 0.37086537\n",
      "Iteration 57, loss = 0.37001622\n",
      "Iteration 58, loss = 0.36969270\n",
      "Iteration 59, loss = 0.36921714\n",
      "Iteration 60, loss = 0.36752774\n",
      "Iteration 61, loss = 0.36749088\n",
      "Iteration 62, loss = 0.36710318\n",
      "Iteration 63, loss = 0.36556857\n",
      "Iteration 64, loss = 0.36557335\n",
      "Iteration 65, loss = 0.36596798\n",
      "Iteration 66, loss = 0.36444339\n",
      "Iteration 67, loss = 0.36411578\n",
      "Iteration 68, loss = 0.36414112\n",
      "Iteration 69, loss = 0.36283870\n",
      "Iteration 70, loss = 0.36265972\n",
      "Iteration 71, loss = 0.36177727\n",
      "Iteration 72, loss = 0.36150022\n",
      "Iteration 73, loss = 0.36150920\n",
      "Iteration 74, loss = 0.36020351\n",
      "Iteration 75, loss = 0.36086160\n",
      "Iteration 76, loss = 0.35941532\n",
      "Iteration 77, loss = 0.35874937\n",
      "Iteration 78, loss = 0.35807311\n",
      "Iteration 79, loss = 0.35824830\n",
      "Iteration 80, loss = 0.35829685\n",
      "Iteration 81, loss = 0.35715092\n",
      "Iteration 82, loss = 0.35712248\n",
      "Iteration 83, loss = 0.35590731\n",
      "Iteration 84, loss = 0.35686007\n",
      "Iteration 85, loss = 0.35454773\n",
      "Iteration 86, loss = 0.35480883\n",
      "Iteration 87, loss = 0.35416786\n",
      "Iteration 88, loss = 0.35436416\n",
      "Iteration 89, loss = 0.35433254\n",
      "Iteration 90, loss = 0.35325387\n",
      "Iteration 91, loss = 0.35250415\n",
      "Iteration 92, loss = 0.35267562\n",
      "Iteration 93, loss = 0.35249752\n",
      "Iteration 94, loss = 0.35196707\n",
      "Iteration 95, loss = 0.35197909\n",
      "Iteration 96, loss = 0.35133823\n",
      "Iteration 97, loss = 0.35026756\n",
      "Iteration 98, loss = 0.35089755\n",
      "Iteration 99, loss = 0.34960221\n",
      "Iteration 100, loss = 0.34973314\n",
      "Iteration 101, loss = 0.34957309\n",
      "Iteration 102, loss = 0.34879185\n",
      "Iteration 103, loss = 0.34785831\n",
      "Iteration 104, loss = 0.34848174\n",
      "Iteration 105, loss = 0.34855250\n",
      "Iteration 106, loss = 0.34817960\n",
      "Iteration 107, loss = 0.34684227\n",
      "Iteration 108, loss = 0.34750402\n",
      "Iteration 109, loss = 0.34605452\n",
      "Iteration 110, loss = 0.34667270\n",
      "Iteration 111, loss = 0.34637753\n",
      "Iteration 112, loss = 0.34663246\n",
      "Iteration 113, loss = 0.34706604\n",
      "Iteration 114, loss = 0.34463646\n",
      "Iteration 115, loss = 0.34450809\n",
      "Iteration 116, loss = 0.34478775\n",
      "Iteration 117, loss = 0.34410735\n",
      "Iteration 118, loss = 0.34426566\n",
      "Iteration 119, loss = 0.34436435\n",
      "Iteration 120, loss = 0.34375553\n",
      "Iteration 121, loss = 0.34447029\n",
      "Iteration 122, loss = 0.34296272\n",
      "Iteration 123, loss = 0.34411016\n",
      "Iteration 124, loss = 0.34320190\n",
      "Iteration 125, loss = 0.34171812\n",
      "Iteration 126, loss = 0.34203944\n",
      "Iteration 127, loss = 0.34098551\n",
      "Iteration 128, loss = 0.34203906\n",
      "Iteration 129, loss = 0.34118833\n",
      "Iteration 130, loss = 0.34061198\n",
      "Iteration 131, loss = 0.34065994\n",
      "Iteration 132, loss = 0.34038854\n",
      "Iteration 133, loss = 0.33981580\n",
      "Iteration 134, loss = 0.33928180\n",
      "Iteration 135, loss = 0.33992125\n",
      "Iteration 136, loss = 0.33952568\n",
      "Iteration 137, loss = 0.33844492\n",
      "Iteration 138, loss = 0.33911203\n",
      "Iteration 139, loss = 0.33961434\n",
      "Iteration 140, loss = 0.33780085\n",
      "Iteration 141, loss = 0.33860460\n",
      "Iteration 142, loss = 0.33731347\n",
      "Iteration 143, loss = 0.33762820\n",
      "Iteration 144, loss = 0.33704729\n",
      "Iteration 145, loss = 0.33753122\n",
      "Iteration 146, loss = 0.33723193\n",
      "Iteration 147, loss = 0.33670333\n",
      "Iteration 148, loss = 0.33549439\n",
      "Iteration 149, loss = 0.33665757\n",
      "Iteration 150, loss = 0.33504096\n",
      "Iteration 151, loss = 0.33488735\n",
      "Iteration 152, loss = 0.33582367\n",
      "Iteration 153, loss = 0.33561844\n",
      "Iteration 154, loss = 0.33470094\n",
      "Iteration 155, loss = 0.33503305\n",
      "Iteration 156, loss = 0.33529564\n",
      "Iteration 157, loss = 0.33364338\n",
      "Iteration 158, loss = 0.33344649\n",
      "Iteration 159, loss = 0.33404136\n",
      "Iteration 160, loss = 0.33421916\n",
      "Iteration 161, loss = 0.33372211\n",
      "Iteration 162, loss = 0.33302072\n",
      "Iteration 163, loss = 0.33396319\n",
      "Iteration 164, loss = 0.33266340\n",
      "Iteration 165, loss = 0.33279722\n",
      "Iteration 166, loss = 0.33236931\n",
      "Iteration 167, loss = 0.33266739\n",
      "Iteration 168, loss = 0.33183131\n",
      "Iteration 169, loss = 0.33386247\n",
      "Iteration 170, loss = 0.33103167\n",
      "Iteration 171, loss = 0.33168552\n",
      "Iteration 172, loss = 0.33223923\n",
      "Iteration 173, loss = 0.33105115\n",
      "Iteration 174, loss = 0.33056121\n",
      "Iteration 175, loss = 0.33033414\n",
      "Iteration 176, loss = 0.32999332\n",
      "Iteration 177, loss = 0.33052794\n",
      "Iteration 178, loss = 0.33028476\n",
      "Iteration 179, loss = 0.33057731\n",
      "Iteration 180, loss = 0.32943210\n",
      "Iteration 181, loss = 0.33069020\n",
      "Iteration 182, loss = 0.32961153\n",
      "Iteration 183, loss = 0.32934283\n",
      "Iteration 184, loss = 0.32930515\n",
      "Iteration 185, loss = 0.32905941\n",
      "Iteration 186, loss = 0.32844144\n",
      "Iteration 187, loss = 0.32935613\n",
      "Iteration 188, loss = 0.32863065\n",
      "Iteration 189, loss = 0.32821721\n",
      "Iteration 190, loss = 0.32848127\n",
      "Iteration 191, loss = 0.32752674\n",
      "Iteration 192, loss = 0.32779609\n",
      "Iteration 193, loss = 0.32816200\n",
      "Iteration 194, loss = 0.32788932\n",
      "Iteration 195, loss = 0.32780409\n",
      "Iteration 196, loss = 0.32715518\n",
      "Iteration 197, loss = 0.32755939\n",
      "Iteration 198, loss = 0.32627688\n",
      "Iteration 199, loss = 0.32656591\n",
      "Iteration 200, loss = 0.32654077\n",
      "Iteration 201, loss = 0.32635972\n",
      "Iteration 202, loss = 0.32761671\n",
      "Iteration 203, loss = 0.32642667\n",
      "Iteration 204, loss = 0.32684093\n",
      "Iteration 205, loss = 0.32561160\n",
      "Iteration 206, loss = 0.32544702\n",
      "Iteration 207, loss = 0.32583305\n",
      "Iteration 208, loss = 0.32454672\n",
      "Iteration 209, loss = 0.32512179\n",
      "Iteration 210, loss = 0.32547265\n",
      "Iteration 211, loss = 0.32555370\n",
      "Iteration 212, loss = 0.32577775\n",
      "Iteration 213, loss = 0.32554858\n",
      "Iteration 214, loss = 0.32491627\n",
      "Iteration 215, loss = 0.32448691\n",
      "Iteration 216, loss = 0.32379700\n",
      "Iteration 217, loss = 0.32356466\n",
      "Iteration 218, loss = 0.32312843\n",
      "Iteration 219, loss = 0.32437257\n",
      "Iteration 220, loss = 0.32416797\n",
      "Iteration 221, loss = 0.32401958\n",
      "Iteration 222, loss = 0.32252863\n",
      "Iteration 223, loss = 0.32320657\n",
      "Iteration 224, loss = 0.32328740\n",
      "Iteration 225, loss = 0.32235595\n",
      "Iteration 226, loss = 0.32313895\n",
      "Iteration 227, loss = 0.32307634\n",
      "Iteration 228, loss = 0.32298853\n",
      "Iteration 229, loss = 0.32145202\n",
      "Iteration 230, loss = 0.32169834\n",
      "Iteration 231, loss = 0.32183545\n",
      "Iteration 232, loss = 0.32212601\n",
      "Iteration 233, loss = 0.32198896\n",
      "Iteration 234, loss = 0.32144092\n",
      "Iteration 235, loss = 0.32180734\n",
      "Iteration 236, loss = 0.32170454\n",
      "Iteration 237, loss = 0.32137750\n",
      "Iteration 238, loss = 0.32147488\n",
      "Iteration 239, loss = 0.32064451\n",
      "Iteration 240, loss = 0.32157309\n",
      "Iteration 241, loss = 0.32003875\n",
      "Iteration 242, loss = 0.32018897\n",
      "Iteration 243, loss = 0.32037444\n",
      "Iteration 244, loss = 0.32041236\n",
      "Iteration 245, loss = 0.32111795\n",
      "Iteration 246, loss = 0.32033956\n",
      "Iteration 247, loss = 0.31947620\n",
      "Iteration 248, loss = 0.31999487\n",
      "Iteration 249, loss = 0.32004905\n",
      "Iteration 250, loss = 0.32052154\n",
      "Iteration 251, loss = 0.32076204\n",
      "Iteration 252, loss = 0.31922639\n",
      "Iteration 253, loss = 0.31913134\n",
      "Iteration 254, loss = 0.31886774\n",
      "Iteration 255, loss = 0.31940197\n",
      "Iteration 256, loss = 0.32060152\n",
      "Iteration 257, loss = 0.31910790\n",
      "Iteration 258, loss = 0.31935411\n",
      "Iteration 259, loss = 0.31908926\n",
      "Iteration 260, loss = 0.31782899\n",
      "Iteration 261, loss = 0.31916373\n",
      "Iteration 262, loss = 0.31756955\n",
      "Iteration 263, loss = 0.31787050\n",
      "Iteration 264, loss = 0.31804444\n",
      "Iteration 265, loss = 0.31895999\n",
      "Iteration 266, loss = 0.31783641\n",
      "Iteration 267, loss = 0.31897047\n",
      "Iteration 268, loss = 0.31648099\n",
      "Iteration 269, loss = 0.31705018\n",
      "Iteration 270, loss = 0.31839901\n",
      "Iteration 271, loss = 0.31757926\n",
      "Iteration 272, loss = 0.31778009\n",
      "Iteration 273, loss = 0.31715943\n",
      "Iteration 274, loss = 0.31890660\n",
      "Iteration 275, loss = 0.31658396\n",
      "Iteration 276, loss = 0.31809468\n",
      "Iteration 277, loss = 0.31689195\n",
      "Iteration 278, loss = 0.31647880\n",
      "Iteration 279, loss = 0.31674436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51576260\n",
      "Iteration 2, loss = 0.46808973\n",
      "Iteration 3, loss = 0.45976374\n",
      "Iteration 4, loss = 0.45394073\n",
      "Iteration 5, loss = 0.44919093\n",
      "Iteration 6, loss = 0.44475857\n",
      "Iteration 7, loss = 0.44111412\n",
      "Iteration 8, loss = 0.43767534\n",
      "Iteration 9, loss = 0.43528421\n",
      "Iteration 10, loss = 0.43193307\n",
      "Iteration 11, loss = 0.42910212\n",
      "Iteration 12, loss = 0.42641363\n",
      "Iteration 13, loss = 0.42422971\n",
      "Iteration 14, loss = 0.42236618\n",
      "Iteration 15, loss = 0.41973277\n",
      "Iteration 16, loss = 0.41814107\n",
      "Iteration 17, loss = 0.41602775\n",
      "Iteration 18, loss = 0.41426142\n",
      "Iteration 19, loss = 0.41263140\n",
      "Iteration 20, loss = 0.41082220\n",
      "Iteration 21, loss = 0.40925907\n",
      "Iteration 22, loss = 0.40809837\n",
      "Iteration 23, loss = 0.40650889\n",
      "Iteration 24, loss = 0.40572645\n",
      "Iteration 25, loss = 0.40463205\n",
      "Iteration 26, loss = 0.40269063\n",
      "Iteration 27, loss = 0.40163756\n",
      "Iteration 28, loss = 0.40043056\n",
      "Iteration 29, loss = 0.39895473\n",
      "Iteration 30, loss = 0.39830201\n",
      "Iteration 31, loss = 0.39786119\n",
      "Iteration 32, loss = 0.39661787\n",
      "Iteration 33, loss = 0.39607211\n",
      "Iteration 34, loss = 0.39472098\n",
      "Iteration 35, loss = 0.39346198\n",
      "Iteration 36, loss = 0.39335782\n",
      "Iteration 37, loss = 0.39189999\n",
      "Iteration 38, loss = 0.39116438\n",
      "Iteration 39, loss = 0.39019869\n",
      "Iteration 40, loss = 0.38903223\n",
      "Iteration 41, loss = 0.38853767\n",
      "Iteration 42, loss = 0.38801357\n",
      "Iteration 43, loss = 0.38755029\n",
      "Iteration 44, loss = 0.38640974\n",
      "Iteration 45, loss = 0.38577253\n",
      "Iteration 46, loss = 0.38484808\n",
      "Iteration 47, loss = 0.38529221\n",
      "Iteration 48, loss = 0.38355479\n",
      "Iteration 49, loss = 0.38325593\n",
      "Iteration 50, loss = 0.38192713\n",
      "Iteration 51, loss = 0.38180199\n",
      "Iteration 52, loss = 0.38130102\n",
      "Iteration 53, loss = 0.38072542\n",
      "Iteration 54, loss = 0.37952879\n",
      "Iteration 55, loss = 0.37901677\n",
      "Iteration 56, loss = 0.37822249\n",
      "Iteration 57, loss = 0.37776834\n",
      "Iteration 58, loss = 0.37803708\n",
      "Iteration 59, loss = 0.37663047\n",
      "Iteration 60, loss = 0.37654499\n",
      "Iteration 61, loss = 0.37597422\n",
      "Iteration 62, loss = 0.37445628\n",
      "Iteration 63, loss = 0.37444884\n",
      "Iteration 64, loss = 0.37373236\n",
      "Iteration 65, loss = 0.37366742\n",
      "Iteration 66, loss = 0.37268310\n",
      "Iteration 67, loss = 0.37286785\n",
      "Iteration 68, loss = 0.37214435\n",
      "Iteration 69, loss = 0.37165286\n",
      "Iteration 70, loss = 0.37168678\n",
      "Iteration 71, loss = 0.37025578\n",
      "Iteration 72, loss = 0.37006346\n",
      "Iteration 73, loss = 0.37001932\n",
      "Iteration 74, loss = 0.36853954\n",
      "Iteration 75, loss = 0.36892815\n",
      "Iteration 76, loss = 0.36833083\n",
      "Iteration 77, loss = 0.36748219\n",
      "Iteration 78, loss = 0.36732370\n",
      "Iteration 79, loss = 0.36677381\n",
      "Iteration 80, loss = 0.36592508\n",
      "Iteration 81, loss = 0.36590724\n",
      "Iteration 82, loss = 0.36528612\n",
      "Iteration 83, loss = 0.36480922\n",
      "Iteration 84, loss = 0.36467835\n",
      "Iteration 85, loss = 0.36392624\n",
      "Iteration 86, loss = 0.36274549\n",
      "Iteration 87, loss = 0.36258911\n",
      "Iteration 88, loss = 0.36241766\n",
      "Iteration 89, loss = 0.36223873\n",
      "Iteration 90, loss = 0.36180430\n",
      "Iteration 91, loss = 0.36096747\n",
      "Iteration 92, loss = 0.36273544\n",
      "Iteration 93, loss = 0.36064972\n",
      "Iteration 94, loss = 0.36067206\n",
      "Iteration 95, loss = 0.35905924\n",
      "Iteration 96, loss = 0.35970122\n",
      "Iteration 97, loss = 0.35855665\n",
      "Iteration 98, loss = 0.35916994\n",
      "Iteration 99, loss = 0.35881955\n",
      "Iteration 100, loss = 0.35788228\n",
      "Iteration 101, loss = 0.35705881\n",
      "Iteration 102, loss = 0.35694963\n",
      "Iteration 103, loss = 0.35691287\n",
      "Iteration 104, loss = 0.35631502\n",
      "Iteration 105, loss = 0.35632390\n",
      "Iteration 106, loss = 0.35536521\n",
      "Iteration 107, loss = 0.35535159\n",
      "Iteration 108, loss = 0.35498134\n",
      "Iteration 109, loss = 0.35392563\n",
      "Iteration 110, loss = 0.35415332\n",
      "Iteration 111, loss = 0.35422814\n",
      "Iteration 112, loss = 0.35335369\n",
      "Iteration 113, loss = 0.35332205\n",
      "Iteration 114, loss = 0.35288684\n",
      "Iteration 115, loss = 0.35256815\n",
      "Iteration 116, loss = 0.35238517\n",
      "Iteration 117, loss = 0.35160972\n",
      "Iteration 118, loss = 0.35156575\n",
      "Iteration 119, loss = 0.35220558\n",
      "Iteration 120, loss = 0.35068263\n",
      "Iteration 121, loss = 0.35107579\n",
      "Iteration 122, loss = 0.35119866\n",
      "Iteration 123, loss = 0.35075784\n",
      "Iteration 124, loss = 0.35015421\n",
      "Iteration 125, loss = 0.35038716\n",
      "Iteration 126, loss = 0.34979412\n",
      "Iteration 127, loss = 0.34845510\n",
      "Iteration 128, loss = 0.34893998\n",
      "Iteration 129, loss = 0.34824363\n",
      "Iteration 130, loss = 0.34850015\n",
      "Iteration 131, loss = 0.34843146\n",
      "Iteration 132, loss = 0.34755702\n",
      "Iteration 133, loss = 0.34760022\n",
      "Iteration 134, loss = 0.34651752\n",
      "Iteration 135, loss = 0.34758734\n",
      "Iteration 136, loss = 0.34678027\n",
      "Iteration 137, loss = 0.34647369\n",
      "Iteration 138, loss = 0.34550303\n",
      "Iteration 139, loss = 0.34579840\n",
      "Iteration 140, loss = 0.34675015\n",
      "Iteration 141, loss = 0.34542929\n",
      "Iteration 142, loss = 0.34516831\n",
      "Iteration 143, loss = 0.34484704\n",
      "Iteration 144, loss = 0.34437955\n",
      "Iteration 145, loss = 0.34444588\n",
      "Iteration 146, loss = 0.34421041\n",
      "Iteration 147, loss = 0.34364080\n",
      "Iteration 148, loss = 0.34336068\n",
      "Iteration 149, loss = 0.34434350\n",
      "Iteration 150, loss = 0.34345521\n",
      "Iteration 151, loss = 0.34321582\n",
      "Iteration 152, loss = 0.34271908\n",
      "Iteration 153, loss = 0.34290512\n",
      "Iteration 154, loss = 0.34168681\n",
      "Iteration 155, loss = 0.34284171\n",
      "Iteration 156, loss = 0.34195335\n",
      "Iteration 157, loss = 0.34224913\n",
      "Iteration 158, loss = 0.34177764\n",
      "Iteration 159, loss = 0.34125888\n",
      "Iteration 160, loss = 0.34183582\n",
      "Iteration 161, loss = 0.34155402\n",
      "Iteration 162, loss = 0.34116430\n",
      "Iteration 163, loss = 0.34042947\n",
      "Iteration 164, loss = 0.34005421\n",
      "Iteration 165, loss = 0.34052796\n",
      "Iteration 166, loss = 0.33998045\n",
      "Iteration 167, loss = 0.34005110\n",
      "Iteration 168, loss = 0.34011479\n",
      "Iteration 169, loss = 0.33918897\n",
      "Iteration 170, loss = 0.33882827\n",
      "Iteration 171, loss = 0.33921274\n",
      "Iteration 172, loss = 0.33907884\n",
      "Iteration 173, loss = 0.33879334\n",
      "Iteration 174, loss = 0.33848084\n",
      "Iteration 175, loss = 0.33838491\n",
      "Iteration 176, loss = 0.33895071\n",
      "Iteration 177, loss = 0.33827852\n",
      "Iteration 178, loss = 0.33911024\n",
      "Iteration 179, loss = 0.33793575\n",
      "Iteration 180, loss = 0.33704187\n",
      "Iteration 181, loss = 0.33858639\n",
      "Iteration 182, loss = 0.33743621\n",
      "Iteration 183, loss = 0.33645833\n",
      "Iteration 184, loss = 0.33700049\n",
      "Iteration 185, loss = 0.33713230\n",
      "Iteration 186, loss = 0.33661759\n",
      "Iteration 187, loss = 0.33606387\n",
      "Iteration 188, loss = 0.33678717\n",
      "Iteration 189, loss = 0.33710238\n",
      "Iteration 190, loss = 0.33584116\n",
      "Iteration 191, loss = 0.33730757\n",
      "Iteration 192, loss = 0.33547216\n",
      "Iteration 193, loss = 0.33524639\n",
      "Iteration 194, loss = 0.33545751\n",
      "Iteration 195, loss = 0.33617441\n",
      "Iteration 196, loss = 0.33461264\n",
      "Iteration 197, loss = 0.33512357\n",
      "Iteration 198, loss = 0.33486780\n",
      "Iteration 199, loss = 0.33519030\n",
      "Iteration 200, loss = 0.33493326\n",
      "Iteration 201, loss = 0.33363545\n",
      "Iteration 202, loss = 0.33442798\n",
      "Iteration 203, loss = 0.33385679\n",
      "Iteration 204, loss = 0.33414201\n",
      "Iteration 205, loss = 0.33388438\n",
      "Iteration 206, loss = 0.33447320\n",
      "Iteration 207, loss = 0.33331646\n",
      "Iteration 208, loss = 0.33392042\n",
      "Iteration 209, loss = 0.33364026\n",
      "Iteration 210, loss = 0.33228511\n",
      "Iteration 211, loss = 0.33285419\n",
      "Iteration 212, loss = 0.33338716\n",
      "Iteration 213, loss = 0.33152086\n",
      "Iteration 214, loss = 0.33212211\n",
      "Iteration 215, loss = 0.33211440\n",
      "Iteration 216, loss = 0.33193510\n",
      "Iteration 217, loss = 0.33225992\n",
      "Iteration 218, loss = 0.33220317\n",
      "Iteration 219, loss = 0.33223139\n",
      "Iteration 220, loss = 0.33199476\n",
      "Iteration 221, loss = 0.33370417\n",
      "Iteration 222, loss = 0.33125515\n",
      "Iteration 223, loss = 0.33330947\n",
      "Iteration 224, loss = 0.33099725\n",
      "Iteration 225, loss = 0.33134647\n",
      "Iteration 226, loss = 0.33104810\n",
      "Iteration 227, loss = 0.33321615\n",
      "Iteration 228, loss = 0.33074600\n",
      "Iteration 229, loss = 0.33086948\n",
      "Iteration 230, loss = 0.33039860\n",
      "Iteration 231, loss = 0.32966600\n",
      "Iteration 232, loss = 0.33019867\n",
      "Iteration 233, loss = 0.33054398\n",
      "Iteration 234, loss = 0.33073469\n",
      "Iteration 235, loss = 0.32978001\n",
      "Iteration 236, loss = 0.32980541\n",
      "Iteration 237, loss = 0.32966303\n",
      "Iteration 238, loss = 0.33015793\n",
      "Iteration 239, loss = 0.32986287\n",
      "Iteration 240, loss = 0.33118814\n",
      "Iteration 241, loss = 0.33037233\n",
      "Iteration 242, loss = 0.32898140\n",
      "Iteration 243, loss = 0.33007776\n",
      "Iteration 244, loss = 0.32852657\n",
      "Iteration 245, loss = 0.32923370\n",
      "Iteration 246, loss = 0.32825855\n",
      "Iteration 247, loss = 0.32997285\n",
      "Iteration 248, loss = 0.32826897\n",
      "Iteration 249, loss = 0.32856855\n",
      "Iteration 250, loss = 0.32774878\n",
      "Iteration 251, loss = 0.32842866\n",
      "Iteration 252, loss = 0.32897782\n",
      "Iteration 253, loss = 0.32808804\n",
      "Iteration 254, loss = 0.32754576\n",
      "Iteration 255, loss = 0.32759033\n",
      "Iteration 256, loss = 0.32732130\n",
      "Iteration 257, loss = 0.32810919\n",
      "Iteration 258, loss = 0.32729870\n",
      "Iteration 259, loss = 0.32753634\n",
      "Iteration 260, loss = 0.32678882\n",
      "Iteration 261, loss = 0.32683727\n",
      "Iteration 262, loss = 0.32798690\n",
      "Iteration 263, loss = 0.32677926\n",
      "Iteration 264, loss = 0.32657520\n",
      "Iteration 265, loss = 0.32569111\n",
      "Iteration 266, loss = 0.32836038\n",
      "Iteration 267, loss = 0.32656779\n",
      "Iteration 268, loss = 0.32761342\n",
      "Iteration 269, loss = 0.32669920\n",
      "Iteration 270, loss = 0.32723438\n",
      "Iteration 271, loss = 0.32566280\n",
      "Iteration 272, loss = 0.32613796\n",
      "Iteration 273, loss = 0.32608468\n",
      "Iteration 274, loss = 0.32689459\n",
      "Iteration 275, loss = 0.32652288\n",
      "Iteration 276, loss = 0.32517195\n",
      "Iteration 277, loss = 0.32590968\n",
      "Iteration 278, loss = 0.32533473\n",
      "Iteration 279, loss = 0.32539097\n",
      "Iteration 280, loss = 0.32460049\n",
      "Iteration 281, loss = 0.32582715\n",
      "Iteration 282, loss = 0.32617033\n",
      "Iteration 283, loss = 0.32467332\n",
      "Iteration 284, loss = 0.32416332\n",
      "Iteration 285, loss = 0.32535714\n",
      "Iteration 286, loss = 0.32437410\n",
      "Iteration 287, loss = 0.32512404\n",
      "Iteration 288, loss = 0.32367663\n",
      "Iteration 289, loss = 0.32405367\n",
      "Iteration 290, loss = 0.32454032\n",
      "Iteration 291, loss = 0.32512098\n",
      "Iteration 292, loss = 0.32456543\n",
      "Iteration 293, loss = 0.32364863\n",
      "Iteration 294, loss = 0.32295506\n",
      "Iteration 295, loss = 0.32387937\n",
      "Iteration 296, loss = 0.32503012\n",
      "Iteration 297, loss = 0.32362674\n",
      "Iteration 298, loss = 0.32285672\n",
      "Iteration 299, loss = 0.32288888\n",
      "Iteration 300, loss = 0.32422686\n",
      "Iteration 301, loss = 0.32375133\n",
      "Iteration 302, loss = 0.32286303\n",
      "Iteration 303, loss = 0.32251502\n",
      "Iteration 304, loss = 0.32455348\n",
      "Iteration 305, loss = 0.32338532\n",
      "Iteration 306, loss = 0.32191386\n",
      "Iteration 307, loss = 0.32318320\n",
      "Iteration 308, loss = 0.32221016\n",
      "Iteration 309, loss = 0.32260752\n",
      "Iteration 310, loss = 0.32383512\n",
      "Iteration 311, loss = 0.32221843\n",
      "Iteration 312, loss = 0.32152866\n",
      "Iteration 313, loss = 0.32224710\n",
      "Iteration 314, loss = 0.32284420\n",
      "Iteration 315, loss = 0.32129956\n",
      "Iteration 316, loss = 0.32151876\n",
      "Iteration 317, loss = 0.32269679\n",
      "Iteration 318, loss = 0.32273472\n",
      "Iteration 319, loss = 0.32202328\n",
      "Iteration 320, loss = 0.32117569\n",
      "Iteration 321, loss = 0.32309553\n",
      "Iteration 322, loss = 0.32222963\n",
      "Iteration 323, loss = 0.32085294\n",
      "Iteration 324, loss = 0.32212992\n",
      "Iteration 325, loss = 0.32170409\n",
      "Iteration 326, loss = 0.32153782\n",
      "Iteration 327, loss = 0.32066619\n",
      "Iteration 328, loss = 0.32170032\n",
      "Iteration 329, loss = 0.32084453\n",
      "Iteration 330, loss = 0.32096479\n",
      "Iteration 331, loss = 0.32061324\n",
      "Iteration 332, loss = 0.32187998\n",
      "Iteration 333, loss = 0.32066860\n",
      "Iteration 334, loss = 0.32000734\n",
      "Iteration 335, loss = 0.32093636\n",
      "Iteration 336, loss = 0.32031169\n",
      "Iteration 337, loss = 0.31957559\n",
      "Iteration 338, loss = 0.32109302\n",
      "Iteration 339, loss = 0.32102907\n",
      "Iteration 340, loss = 0.32135779\n",
      "Iteration 341, loss = 0.32034561\n",
      "Iteration 342, loss = 0.32056118\n",
      "Iteration 343, loss = 0.31957058\n",
      "Iteration 344, loss = 0.31969543\n",
      "Iteration 345, loss = 0.31999066\n",
      "Iteration 346, loss = 0.31984030\n",
      "Iteration 347, loss = 0.31990692\n",
      "Iteration 348, loss = 0.32008387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51356503\n",
      "Iteration 2, loss = 0.46674283\n",
      "Iteration 3, loss = 0.45764341\n",
      "Iteration 4, loss = 0.45229533\n",
      "Iteration 5, loss = 0.44810781\n",
      "Iteration 6, loss = 0.44440276\n",
      "Iteration 7, loss = 0.44114200\n",
      "Iteration 8, loss = 0.43826708\n",
      "Iteration 9, loss = 0.43524099\n",
      "Iteration 10, loss = 0.43242680\n",
      "Iteration 11, loss = 0.42993359\n",
      "Iteration 12, loss = 0.42780400\n",
      "Iteration 13, loss = 0.42587121\n",
      "Iteration 14, loss = 0.42452945\n",
      "Iteration 15, loss = 0.42181619\n",
      "Iteration 16, loss = 0.41975112\n",
      "Iteration 17, loss = 0.41781619\n",
      "Iteration 18, loss = 0.41671883\n",
      "Iteration 19, loss = 0.41557597\n",
      "Iteration 20, loss = 0.41348549\n",
      "Iteration 21, loss = 0.41185477\n",
      "Iteration 22, loss = 0.41064776\n",
      "Iteration 23, loss = 0.40966203\n",
      "Iteration 24, loss = 0.40838443\n",
      "Iteration 25, loss = 0.40676094\n",
      "Iteration 26, loss = 0.40529290\n",
      "Iteration 27, loss = 0.40499301\n",
      "Iteration 28, loss = 0.40315800\n",
      "Iteration 29, loss = 0.40272227\n",
      "Iteration 30, loss = 0.40216988\n",
      "Iteration 31, loss = 0.40046644\n",
      "Iteration 32, loss = 0.39912281\n",
      "Iteration 33, loss = 0.39824252\n",
      "Iteration 34, loss = 0.39801922\n",
      "Iteration 35, loss = 0.39706405\n",
      "Iteration 36, loss = 0.39570168\n",
      "Iteration 37, loss = 0.39466241\n",
      "Iteration 38, loss = 0.39356799\n",
      "Iteration 39, loss = 0.39281965\n",
      "Iteration 40, loss = 0.39264366\n",
      "Iteration 41, loss = 0.39085811\n",
      "Iteration 42, loss = 0.39064671\n",
      "Iteration 43, loss = 0.38977960\n",
      "Iteration 44, loss = 0.38896524\n",
      "Iteration 45, loss = 0.38838685\n",
      "Iteration 46, loss = 0.38793292\n",
      "Iteration 47, loss = 0.38675986\n",
      "Iteration 48, loss = 0.38562277\n",
      "Iteration 49, loss = 0.38554727\n",
      "Iteration 50, loss = 0.38444561\n",
      "Iteration 51, loss = 0.38403038\n",
      "Iteration 52, loss = 0.38356033\n",
      "Iteration 53, loss = 0.38368022\n",
      "Iteration 54, loss = 0.38255510\n",
      "Iteration 55, loss = 0.38142124\n",
      "Iteration 56, loss = 0.38094916\n",
      "Iteration 57, loss = 0.38046512\n",
      "Iteration 58, loss = 0.37989608\n",
      "Iteration 59, loss = 0.37887874\n",
      "Iteration 60, loss = 0.37854751\n",
      "Iteration 61, loss = 0.37799161\n",
      "Iteration 62, loss = 0.37844575\n",
      "Iteration 63, loss = 0.37686629\n",
      "Iteration 64, loss = 0.37662085\n",
      "Iteration 65, loss = 0.37599922\n",
      "Iteration 66, loss = 0.37499232\n",
      "Iteration 67, loss = 0.37545197\n",
      "Iteration 68, loss = 0.37467634\n",
      "Iteration 69, loss = 0.37390438\n",
      "Iteration 70, loss = 0.37395437\n",
      "Iteration 71, loss = 0.37304285\n",
      "Iteration 72, loss = 0.37296266\n",
      "Iteration 73, loss = 0.37251903\n",
      "Iteration 74, loss = 0.37106215\n",
      "Iteration 75, loss = 0.37058142\n",
      "Iteration 76, loss = 0.37112633\n",
      "Iteration 77, loss = 0.37094430\n",
      "Iteration 78, loss = 0.37005212\n",
      "Iteration 79, loss = 0.36928100\n",
      "Iteration 80, loss = 0.36873731\n",
      "Iteration 81, loss = 0.36906423\n",
      "Iteration 82, loss = 0.36826480\n",
      "Iteration 83, loss = 0.36832856\n",
      "Iteration 84, loss = 0.36770831\n",
      "Iteration 85, loss = 0.36702369\n",
      "Iteration 86, loss = 0.36705826\n",
      "Iteration 87, loss = 0.36675938\n",
      "Iteration 88, loss = 0.36648566\n",
      "Iteration 89, loss = 0.36596440\n",
      "Iteration 90, loss = 0.36528682\n",
      "Iteration 91, loss = 0.36462799\n",
      "Iteration 92, loss = 0.36494177\n",
      "Iteration 93, loss = 0.36418530\n",
      "Iteration 94, loss = 0.36467927\n",
      "Iteration 95, loss = 0.36438358\n",
      "Iteration 96, loss = 0.36386725\n",
      "Iteration 97, loss = 0.36388836\n",
      "Iteration 98, loss = 0.36289687\n",
      "Iteration 99, loss = 0.36216578\n",
      "Iteration 100, loss = 0.36278907\n",
      "Iteration 101, loss = 0.36268522\n",
      "Iteration 102, loss = 0.36181243\n",
      "Iteration 103, loss = 0.36149270\n",
      "Iteration 104, loss = 0.36108932\n",
      "Iteration 105, loss = 0.36032949\n",
      "Iteration 106, loss = 0.36118234\n",
      "Iteration 107, loss = 0.36159599\n",
      "Iteration 108, loss = 0.35961103\n",
      "Iteration 109, loss = 0.35929550\n",
      "Iteration 110, loss = 0.35940504\n",
      "Iteration 111, loss = 0.35937319\n",
      "Iteration 112, loss = 0.36002598\n",
      "Iteration 113, loss = 0.35899816\n",
      "Iteration 114, loss = 0.35836932\n",
      "Iteration 115, loss = 0.35805180\n",
      "Iteration 116, loss = 0.35748299\n",
      "Iteration 117, loss = 0.35711488\n",
      "Iteration 118, loss = 0.35856706\n",
      "Iteration 119, loss = 0.35743905\n",
      "Iteration 120, loss = 0.35610593\n",
      "Iteration 121, loss = 0.35735067\n",
      "Iteration 122, loss = 0.35614551\n",
      "Iteration 123, loss = 0.35641924\n",
      "Iteration 124, loss = 0.35670934\n",
      "Iteration 125, loss = 0.35519902\n",
      "Iteration 126, loss = 0.35633371\n",
      "Iteration 127, loss = 0.35588073\n",
      "Iteration 128, loss = 0.35458052\n",
      "Iteration 129, loss = 0.35448103\n",
      "Iteration 130, loss = 0.35533809\n",
      "Iteration 131, loss = 0.35473850\n",
      "Iteration 132, loss = 0.35414412\n",
      "Iteration 133, loss = 0.35298540\n",
      "Iteration 134, loss = 0.35476368\n",
      "Iteration 135, loss = 0.35411737\n",
      "Iteration 136, loss = 0.35297417\n",
      "Iteration 137, loss = 0.35299221\n",
      "Iteration 138, loss = 0.35216707\n",
      "Iteration 139, loss = 0.35274478\n",
      "Iteration 140, loss = 0.35207216\n",
      "Iteration 141, loss = 0.35145412\n",
      "Iteration 142, loss = 0.35159365\n",
      "Iteration 143, loss = 0.35219127\n",
      "Iteration 144, loss = 0.35100845\n",
      "Iteration 145, loss = 0.35156692\n",
      "Iteration 146, loss = 0.35007852\n",
      "Iteration 147, loss = 0.35070483\n",
      "Iteration 148, loss = 0.35100717\n",
      "Iteration 149, loss = 0.34980504\n",
      "Iteration 150, loss = 0.35027276\n",
      "Iteration 151, loss = 0.34927697\n",
      "Iteration 152, loss = 0.34905536\n",
      "Iteration 153, loss = 0.34971386\n",
      "Iteration 154, loss = 0.34935048\n",
      "Iteration 155, loss = 0.34890105\n",
      "Iteration 156, loss = 0.34850851\n",
      "Iteration 157, loss = 0.34884125\n",
      "Iteration 158, loss = 0.34907963\n",
      "Iteration 159, loss = 0.34767070\n",
      "Iteration 160, loss = 0.34781352\n",
      "Iteration 161, loss = 0.34907622\n",
      "Iteration 162, loss = 0.34764788\n",
      "Iteration 163, loss = 0.34702207\n",
      "Iteration 164, loss = 0.34540923\n",
      "Iteration 165, loss = 0.34800868\n",
      "Iteration 166, loss = 0.34657635\n",
      "Iteration 167, loss = 0.34709708\n",
      "Iteration 168, loss = 0.34654809\n",
      "Iteration 169, loss = 0.34616049\n",
      "Iteration 170, loss = 0.34656648\n",
      "Iteration 171, loss = 0.34632904\n",
      "Iteration 172, loss = 0.34537728\n",
      "Iteration 173, loss = 0.34463956\n",
      "Iteration 174, loss = 0.34569430\n",
      "Iteration 175, loss = 0.34562016\n",
      "Iteration 176, loss = 0.34457929\n",
      "Iteration 177, loss = 0.34474796\n",
      "Iteration 178, loss = 0.34498621\n",
      "Iteration 179, loss = 0.34449769\n",
      "Iteration 180, loss = 0.34488963\n",
      "Iteration 181, loss = 0.34363447\n",
      "Iteration 182, loss = 0.34381195\n",
      "Iteration 183, loss = 0.34380780\n",
      "Iteration 184, loss = 0.34246592\n",
      "Iteration 185, loss = 0.34421730\n",
      "Iteration 186, loss = 0.34398893\n",
      "Iteration 187, loss = 0.34242403\n",
      "Iteration 188, loss = 0.34303196\n",
      "Iteration 189, loss = 0.34359551\n",
      "Iteration 190, loss = 0.34238220\n",
      "Iteration 191, loss = 0.34332871\n",
      "Iteration 192, loss = 0.34263898\n",
      "Iteration 193, loss = 0.34180200\n",
      "Iteration 194, loss = 0.34182357\n",
      "Iteration 195, loss = 0.34224795\n",
      "Iteration 196, loss = 0.34115389\n",
      "Iteration 197, loss = 0.34179139\n",
      "Iteration 198, loss = 0.34143945\n",
      "Iteration 199, loss = 0.34159206\n",
      "Iteration 200, loss = 0.34120139\n",
      "Iteration 201, loss = 0.34139449\n",
      "Iteration 202, loss = 0.34107860\n",
      "Iteration 203, loss = 0.34040097\n",
      "Iteration 204, loss = 0.34038720\n",
      "Iteration 205, loss = 0.33973029\n",
      "Iteration 206, loss = 0.33926120\n",
      "Iteration 207, loss = 0.33983565\n",
      "Iteration 208, loss = 0.34001409\n",
      "Iteration 209, loss = 0.33963204\n",
      "Iteration 210, loss = 0.33896148\n",
      "Iteration 211, loss = 0.33963742\n",
      "Iteration 212, loss = 0.33912016\n",
      "Iteration 213, loss = 0.33880001\n",
      "Iteration 214, loss = 0.33867986\n",
      "Iteration 215, loss = 0.33910167\n",
      "Iteration 216, loss = 0.33836574\n",
      "Iteration 217, loss = 0.33814353\n",
      "Iteration 218, loss = 0.33766371\n",
      "Iteration 219, loss = 0.33835584\n",
      "Iteration 220, loss = 0.33710543\n",
      "Iteration 221, loss = 0.33755333\n",
      "Iteration 222, loss = 0.33659407\n",
      "Iteration 223, loss = 0.33906455\n",
      "Iteration 224, loss = 0.33788625\n",
      "Iteration 225, loss = 0.33650924\n",
      "Iteration 226, loss = 0.33622921\n",
      "Iteration 227, loss = 0.33672945\n",
      "Iteration 228, loss = 0.33685276\n",
      "Iteration 229, loss = 0.33772661\n",
      "Iteration 230, loss = 0.33559847\n",
      "Iteration 231, loss = 0.33612395\n",
      "Iteration 232, loss = 0.33690731\n",
      "Iteration 233, loss = 0.33664733\n",
      "Iteration 234, loss = 0.33580298\n",
      "Iteration 235, loss = 0.33609393\n",
      "Iteration 236, loss = 0.33648101\n",
      "Iteration 237, loss = 0.33563481\n",
      "Iteration 238, loss = 0.33520312\n",
      "Iteration 239, loss = 0.33469016\n",
      "Iteration 240, loss = 0.33555007\n",
      "Iteration 241, loss = 0.33562549\n",
      "Iteration 242, loss = 0.33494599\n",
      "Iteration 243, loss = 0.33442290\n",
      "Iteration 244, loss = 0.33506150\n",
      "Iteration 245, loss = 0.33454971\n",
      "Iteration 246, loss = 0.33402910\n",
      "Iteration 247, loss = 0.33380458\n",
      "Iteration 248, loss = 0.33543097\n",
      "Iteration 249, loss = 0.33327206\n",
      "Iteration 250, loss = 0.33350880\n",
      "Iteration 251, loss = 0.33476598\n",
      "Iteration 252, loss = 0.33388071\n",
      "Iteration 253, loss = 0.33386592\n",
      "Iteration 254, loss = 0.33344048\n",
      "Iteration 255, loss = 0.33317680\n",
      "Iteration 256, loss = 0.33283778\n",
      "Iteration 257, loss = 0.33278149\n",
      "Iteration 258, loss = 0.33294768\n",
      "Iteration 259, loss = 0.33261288\n",
      "Iteration 260, loss = 0.33276480\n",
      "Iteration 261, loss = 0.33290214\n",
      "Iteration 262, loss = 0.33193611\n",
      "Iteration 263, loss = 0.33149275\n",
      "Iteration 264, loss = 0.33249878\n",
      "Iteration 265, loss = 0.33200618\n",
      "Iteration 266, loss = 0.33174378\n",
      "Iteration 267, loss = 0.33171044\n",
      "Iteration 268, loss = 0.33211857\n",
      "Iteration 269, loss = 0.33116267\n",
      "Iteration 270, loss = 0.33143208\n",
      "Iteration 271, loss = 0.33115348\n",
      "Iteration 272, loss = 0.33197513\n",
      "Iteration 273, loss = 0.33122842\n",
      "Iteration 274, loss = 0.33114268\n",
      "Iteration 275, loss = 0.33048544\n",
      "Iteration 276, loss = 0.33135976\n",
      "Iteration 277, loss = 0.33091135\n",
      "Iteration 278, loss = 0.33083328\n",
      "Iteration 279, loss = 0.33034556\n",
      "Iteration 280, loss = 0.33130400\n",
      "Iteration 281, loss = 0.32985848\n",
      "Iteration 282, loss = 0.33070379\n",
      "Iteration 283, loss = 0.33145466\n",
      "Iteration 284, loss = 0.32999483\n",
      "Iteration 285, loss = 0.32988463\n",
      "Iteration 286, loss = 0.33101101\n",
      "Iteration 287, loss = 0.32966922\n",
      "Iteration 288, loss = 0.32972685\n",
      "Iteration 289, loss = 0.32955363\n",
      "Iteration 290, loss = 0.33054676\n",
      "Iteration 291, loss = 0.32906167\n",
      "Iteration 292, loss = 0.32878089\n",
      "Iteration 293, loss = 0.33103509\n",
      "Iteration 294, loss = 0.32891058\n",
      "Iteration 295, loss = 0.32923295\n",
      "Iteration 296, loss = 0.32930169\n",
      "Iteration 297, loss = 0.32858739\n",
      "Iteration 298, loss = 0.32949505\n",
      "Iteration 299, loss = 0.32964092\n",
      "Iteration 300, loss = 0.32805627\n",
      "Iteration 301, loss = 0.33019484\n",
      "Iteration 302, loss = 0.32903273\n",
      "Iteration 303, loss = 0.32744205\n",
      "Iteration 304, loss = 0.32841059\n",
      "Iteration 305, loss = 0.32799010\n",
      "Iteration 306, loss = 0.32847466\n",
      "Iteration 307, loss = 0.32961523\n",
      "Iteration 308, loss = 0.32929089\n",
      "Iteration 309, loss = 0.32899571\n",
      "Iteration 310, loss = 0.32758062\n",
      "Iteration 311, loss = 0.32832276\n",
      "Iteration 312, loss = 0.32722782\n",
      "Iteration 313, loss = 0.32716105\n",
      "Iteration 314, loss = 0.32664969\n",
      "Iteration 315, loss = 0.32764164\n",
      "Iteration 316, loss = 0.32746134\n",
      "Iteration 317, loss = 0.32708468\n",
      "Iteration 318, loss = 0.32654939\n",
      "Iteration 319, loss = 0.32694871\n",
      "Iteration 320, loss = 0.32670963\n",
      "Iteration 321, loss = 0.32776882\n",
      "Iteration 322, loss = 0.32653859\n",
      "Iteration 323, loss = 0.32599037\n",
      "Iteration 324, loss = 0.32627834\n",
      "Iteration 325, loss = 0.32745653\n",
      "Iteration 326, loss = 0.32610047\n",
      "Iteration 327, loss = 0.32664084\n",
      "Iteration 328, loss = 0.32677817\n",
      "Iteration 329, loss = 0.32647157\n",
      "Iteration 330, loss = 0.32619427\n",
      "Iteration 331, loss = 0.32748922\n",
      "Iteration 332, loss = 0.32500089\n",
      "Iteration 333, loss = 0.32496044\n",
      "Iteration 334, loss = 0.32630850\n",
      "Iteration 335, loss = 0.32594808\n",
      "Iteration 336, loss = 0.32657963\n",
      "Iteration 337, loss = 0.32427327\n",
      "Iteration 338, loss = 0.32670361\n",
      "Iteration 339, loss = 0.32496723\n",
      "Iteration 340, loss = 0.32576117\n",
      "Iteration 341, loss = 0.32554598\n",
      "Iteration 342, loss = 0.32492685\n",
      "Iteration 343, loss = 0.32478127\n",
      "Iteration 344, loss = 0.32545751\n",
      "Iteration 345, loss = 0.32566619\n",
      "Iteration 346, loss = 0.32598485\n",
      "Iteration 347, loss = 0.32455891\n",
      "Iteration 348, loss = 0.32500281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51114431\n",
      "Iteration 2, loss = 0.46081458\n",
      "Iteration 3, loss = 0.45374333\n",
      "Iteration 4, loss = 0.44875104\n",
      "Iteration 5, loss = 0.44461280\n",
      "Iteration 6, loss = 0.44085509\n",
      "Iteration 7, loss = 0.43798210\n",
      "Iteration 8, loss = 0.43470439\n",
      "Iteration 9, loss = 0.43242281\n",
      "Iteration 10, loss = 0.42882138\n",
      "Iteration 11, loss = 0.42695809\n",
      "Iteration 12, loss = 0.42410668\n",
      "Iteration 13, loss = 0.42281784\n",
      "Iteration 14, loss = 0.42045835\n",
      "Iteration 15, loss = 0.41863384\n",
      "Iteration 16, loss = 0.41700375\n",
      "Iteration 17, loss = 0.41476639\n",
      "Iteration 18, loss = 0.41391859\n",
      "Iteration 19, loss = 0.41247758\n",
      "Iteration 20, loss = 0.41028906\n",
      "Iteration 21, loss = 0.40918818\n",
      "Iteration 22, loss = 0.40748717\n",
      "Iteration 23, loss = 0.40664301\n",
      "Iteration 24, loss = 0.40503184\n",
      "Iteration 25, loss = 0.40376180\n",
      "Iteration 26, loss = 0.40272385\n",
      "Iteration 27, loss = 0.40126061\n",
      "Iteration 28, loss = 0.40028409\n",
      "Iteration 29, loss = 0.39896001\n",
      "Iteration 30, loss = 0.39834727\n",
      "Iteration 31, loss = 0.39697200\n",
      "Iteration 32, loss = 0.39620760\n",
      "Iteration 33, loss = 0.39521739\n",
      "Iteration 34, loss = 0.39370516\n",
      "Iteration 35, loss = 0.39320653\n",
      "Iteration 36, loss = 0.39224361\n",
      "Iteration 37, loss = 0.39092212\n",
      "Iteration 38, loss = 0.39048822\n",
      "Iteration 39, loss = 0.38911373\n",
      "Iteration 40, loss = 0.38868836\n",
      "Iteration 41, loss = 0.38793531\n",
      "Iteration 42, loss = 0.38664654\n",
      "Iteration 43, loss = 0.38634925\n",
      "Iteration 44, loss = 0.38528880\n",
      "Iteration 45, loss = 0.38402218\n",
      "Iteration 46, loss = 0.38369017\n",
      "Iteration 47, loss = 0.38324912\n",
      "Iteration 48, loss = 0.38218900\n",
      "Iteration 49, loss = 0.38177831\n",
      "Iteration 50, loss = 0.38033545\n",
      "Iteration 51, loss = 0.38016523\n",
      "Iteration 52, loss = 0.37954728\n",
      "Iteration 53, loss = 0.37899347\n",
      "Iteration 54, loss = 0.37887633\n",
      "Iteration 55, loss = 0.37768839\n",
      "Iteration 56, loss = 0.37764376\n",
      "Iteration 57, loss = 0.37662961\n",
      "Iteration 58, loss = 0.37648062\n",
      "Iteration 59, loss = 0.37605140\n",
      "Iteration 60, loss = 0.37472705\n",
      "Iteration 61, loss = 0.37406449\n",
      "Iteration 62, loss = 0.37288898\n",
      "Iteration 63, loss = 0.37232853\n",
      "Iteration 64, loss = 0.37265887\n",
      "Iteration 65, loss = 0.37256639\n",
      "Iteration 66, loss = 0.37075889\n",
      "Iteration 67, loss = 0.37119327\n",
      "Iteration 68, loss = 0.37061103\n",
      "Iteration 69, loss = 0.36969792\n",
      "Iteration 70, loss = 0.37040185\n",
      "Iteration 71, loss = 0.36844735\n",
      "Iteration 72, loss = 0.36884843\n",
      "Iteration 73, loss = 0.36845791\n",
      "Iteration 74, loss = 0.36744155\n",
      "Iteration 75, loss = 0.36652081\n",
      "Iteration 76, loss = 0.36618032\n",
      "Iteration 77, loss = 0.36557910\n",
      "Iteration 78, loss = 0.36558080\n",
      "Iteration 79, loss = 0.36538080\n",
      "Iteration 80, loss = 0.36406812\n",
      "Iteration 81, loss = 0.36443827\n",
      "Iteration 82, loss = 0.36342995\n",
      "Iteration 83, loss = 0.36409742\n",
      "Iteration 84, loss = 0.36329925\n",
      "Iteration 85, loss = 0.36259843\n",
      "Iteration 86, loss = 0.36178748\n",
      "Iteration 87, loss = 0.36181461\n",
      "Iteration 88, loss = 0.36058140\n",
      "Iteration 89, loss = 0.36105524\n",
      "Iteration 90, loss = 0.36073810\n",
      "Iteration 91, loss = 0.36081019\n",
      "Iteration 92, loss = 0.35936636\n",
      "Iteration 93, loss = 0.35905370\n",
      "Iteration 94, loss = 0.35862767\n",
      "Iteration 95, loss = 0.35912957\n",
      "Iteration 96, loss = 0.35826149\n",
      "Iteration 97, loss = 0.35842744\n",
      "Iteration 98, loss = 0.35671714\n",
      "Iteration 99, loss = 0.35853076\n",
      "Iteration 100, loss = 0.35633358\n",
      "Iteration 101, loss = 0.35707724\n",
      "Iteration 102, loss = 0.35631227\n",
      "Iteration 103, loss = 0.35600958\n",
      "Iteration 104, loss = 0.35514440\n",
      "Iteration 105, loss = 0.35531263\n",
      "Iteration 106, loss = 0.35469970\n",
      "Iteration 107, loss = 0.35438263\n",
      "Iteration 108, loss = 0.35431504\n",
      "Iteration 109, loss = 0.35372742\n",
      "Iteration 110, loss = 0.35334078\n",
      "Iteration 111, loss = 0.35353429\n",
      "Iteration 112, loss = 0.35256393\n",
      "Iteration 113, loss = 0.35176310\n",
      "Iteration 114, loss = 0.35223687\n",
      "Iteration 115, loss = 0.35169312\n",
      "Iteration 116, loss = 0.35097478\n",
      "Iteration 117, loss = 0.35099449\n",
      "Iteration 118, loss = 0.35094426\n",
      "Iteration 119, loss = 0.35027732\n",
      "Iteration 120, loss = 0.35052844\n",
      "Iteration 121, loss = 0.34970673\n",
      "Iteration 122, loss = 0.34866117\n",
      "Iteration 123, loss = 0.34958441\n",
      "Iteration 124, loss = 0.34933722\n",
      "Iteration 125, loss = 0.34826203\n",
      "Iteration 126, loss = 0.34801081\n",
      "Iteration 127, loss = 0.34919341\n",
      "Iteration 128, loss = 0.34779713\n",
      "Iteration 129, loss = 0.34748308\n",
      "Iteration 130, loss = 0.34719205\n",
      "Iteration 131, loss = 0.34672743\n",
      "Iteration 132, loss = 0.34662139\n",
      "Iteration 133, loss = 0.34619862\n",
      "Iteration 134, loss = 0.34611113\n",
      "Iteration 135, loss = 0.34595985\n",
      "Iteration 136, loss = 0.34655194\n",
      "Iteration 137, loss = 0.34507643\n",
      "Iteration 138, loss = 0.34525564\n",
      "Iteration 139, loss = 0.34512552\n",
      "Iteration 140, loss = 0.34496015\n",
      "Iteration 141, loss = 0.34448918\n",
      "Iteration 142, loss = 0.34465384\n",
      "Iteration 143, loss = 0.34421974\n",
      "Iteration 144, loss = 0.34499592\n",
      "Iteration 145, loss = 0.34422436\n",
      "Iteration 146, loss = 0.34333382\n",
      "Iteration 147, loss = 0.34279126\n",
      "Iteration 148, loss = 0.34208804\n",
      "Iteration 149, loss = 0.34244182\n",
      "Iteration 150, loss = 0.34228994\n",
      "Iteration 151, loss = 0.34270913\n",
      "Iteration 152, loss = 0.34241194\n",
      "Iteration 153, loss = 0.34256857\n",
      "Iteration 154, loss = 0.34117452\n",
      "Iteration 155, loss = 0.34100706\n",
      "Iteration 156, loss = 0.34036972\n",
      "Iteration 157, loss = 0.34081356\n",
      "Iteration 158, loss = 0.34044979\n",
      "Iteration 159, loss = 0.34090353\n",
      "Iteration 160, loss = 0.34021856\n",
      "Iteration 161, loss = 0.33983831\n",
      "Iteration 162, loss = 0.33992546\n",
      "Iteration 163, loss = 0.33936443\n",
      "Iteration 164, loss = 0.33979914\n",
      "Iteration 165, loss = 0.33877749\n",
      "Iteration 166, loss = 0.33890155\n",
      "Iteration 167, loss = 0.33879479\n",
      "Iteration 168, loss = 0.33847460\n",
      "Iteration 169, loss = 0.33894946\n",
      "Iteration 170, loss = 0.33933674\n",
      "Iteration 171, loss = 0.33774055\n",
      "Iteration 172, loss = 0.33783467\n",
      "Iteration 173, loss = 0.33711992\n",
      "Iteration 174, loss = 0.33681419\n",
      "Iteration 175, loss = 0.33692152\n",
      "Iteration 176, loss = 0.33618372\n",
      "Iteration 177, loss = 0.33696287\n",
      "Iteration 178, loss = 0.33696400\n",
      "Iteration 179, loss = 0.33694990\n",
      "Iteration 180, loss = 0.33712170\n",
      "Iteration 181, loss = 0.33625334\n",
      "Iteration 182, loss = 0.33605150\n",
      "Iteration 183, loss = 0.33682315\n",
      "Iteration 184, loss = 0.33656220\n",
      "Iteration 185, loss = 0.33563660\n",
      "Iteration 186, loss = 0.33496510\n",
      "Iteration 187, loss = 0.33503785\n",
      "Iteration 188, loss = 0.33544118\n",
      "Iteration 189, loss = 0.33563994\n",
      "Iteration 190, loss = 0.33537220\n",
      "Iteration 191, loss = 0.33442514\n",
      "Iteration 192, loss = 0.33396471\n",
      "Iteration 193, loss = 0.33448241\n",
      "Iteration 194, loss = 0.33388120\n",
      "Iteration 195, loss = 0.33438477\n",
      "Iteration 196, loss = 0.33364858\n",
      "Iteration 197, loss = 0.33485071\n",
      "Iteration 198, loss = 0.33256118\n",
      "Iteration 199, loss = 0.33385191\n",
      "Iteration 200, loss = 0.33312783\n",
      "Iteration 201, loss = 0.33280032\n",
      "Iteration 202, loss = 0.33409814\n",
      "Iteration 203, loss = 0.33242349\n",
      "Iteration 204, loss = 0.33120066\n",
      "Iteration 205, loss = 0.33241983\n",
      "Iteration 206, loss = 0.33154432\n",
      "Iteration 207, loss = 0.33231993\n",
      "Iteration 208, loss = 0.33191200\n",
      "Iteration 209, loss = 0.33209675\n",
      "Iteration 210, loss = 0.33210401\n",
      "Iteration 211, loss = 0.33085651\n",
      "Iteration 212, loss = 0.33007663\n",
      "Iteration 213, loss = 0.33146488\n",
      "Iteration 214, loss = 0.33202851\n",
      "Iteration 215, loss = 0.33063465\n",
      "Iteration 216, loss = 0.33043606\n",
      "Iteration 217, loss = 0.32982210\n",
      "Iteration 218, loss = 0.32974128\n",
      "Iteration 219, loss = 0.33113604\n",
      "Iteration 220, loss = 0.32943048\n",
      "Iteration 221, loss = 0.32983544\n",
      "Iteration 222, loss = 0.32939180\n",
      "Iteration 223, loss = 0.33131713\n",
      "Iteration 224, loss = 0.32988137\n",
      "Iteration 225, loss = 0.32997125\n",
      "Iteration 226, loss = 0.32888030\n",
      "Iteration 227, loss = 0.32827896\n",
      "Iteration 228, loss = 0.32959722\n",
      "Iteration 229, loss = 0.32929835\n",
      "Iteration 230, loss = 0.32971162\n",
      "Iteration 231, loss = 0.32837961\n",
      "Iteration 232, loss = 0.32832950\n",
      "Iteration 233, loss = 0.32752097\n",
      "Iteration 234, loss = 0.32834875\n",
      "Iteration 235, loss = 0.32921454\n",
      "Iteration 236, loss = 0.32737350\n",
      "Iteration 237, loss = 0.32793390\n",
      "Iteration 238, loss = 0.32701050\n",
      "Iteration 239, loss = 0.32670457\n",
      "Iteration 240, loss = 0.32799670\n",
      "Iteration 241, loss = 0.32773914\n",
      "Iteration 242, loss = 0.32690181\n",
      "Iteration 243, loss = 0.32787336\n",
      "Iteration 244, loss = 0.32843682\n",
      "Iteration 245, loss = 0.32708558\n",
      "Iteration 246, loss = 0.32704290\n",
      "Iteration 247, loss = 0.32640846\n",
      "Iteration 248, loss = 0.32661016\n",
      "Iteration 249, loss = 0.32743814\n",
      "Iteration 250, loss = 0.32530406\n",
      "Iteration 251, loss = 0.32740791\n",
      "Iteration 252, loss = 0.32620303\n",
      "Iteration 253, loss = 0.32737238\n",
      "Iteration 254, loss = 0.32550161\n",
      "Iteration 255, loss = 0.32607175\n",
      "Iteration 256, loss = 0.32593070\n",
      "Iteration 257, loss = 0.32522107\n",
      "Iteration 258, loss = 0.32552829\n",
      "Iteration 259, loss = 0.32591173\n",
      "Iteration 260, loss = 0.32680461\n",
      "Iteration 261, loss = 0.32448386\n",
      "Iteration 262, loss = 0.32459789\n",
      "Iteration 263, loss = 0.32485769\n",
      "Iteration 264, loss = 0.32364448\n",
      "Iteration 265, loss = 0.32558301\n",
      "Iteration 266, loss = 0.32382177\n",
      "Iteration 267, loss = 0.32474723\n",
      "Iteration 268, loss = 0.32392094\n",
      "Iteration 269, loss = 0.32379429\n",
      "Iteration 270, loss = 0.32486033\n",
      "Iteration 271, loss = 0.32575423\n",
      "Iteration 272, loss = 0.32379666\n",
      "Iteration 273, loss = 0.32393305\n",
      "Iteration 274, loss = 0.32473408\n",
      "Iteration 275, loss = 0.32341541\n",
      "Iteration 276, loss = 0.32331197\n",
      "Iteration 277, loss = 0.32378998\n",
      "Iteration 278, loss = 0.32258475\n",
      "Iteration 279, loss = 0.32335723\n",
      "Iteration 280, loss = 0.32407747\n",
      "Iteration 281, loss = 0.32306470\n",
      "Iteration 282, loss = 0.32334612\n",
      "Iteration 283, loss = 0.32289667\n",
      "Iteration 284, loss = 0.32372496\n",
      "Iteration 285, loss = 0.32285084\n",
      "Iteration 286, loss = 0.32213687\n",
      "Iteration 287, loss = 0.32209715\n",
      "Iteration 288, loss = 0.32146442\n",
      "Iteration 289, loss = 0.32230184\n",
      "Iteration 290, loss = 0.32260130\n",
      "Iteration 291, loss = 0.32256720\n",
      "Iteration 292, loss = 0.32339080\n",
      "Iteration 293, loss = 0.32240868\n",
      "Iteration 294, loss = 0.32134797\n",
      "Iteration 295, loss = 0.32154756\n",
      "Iteration 296, loss = 0.32123066\n",
      "Iteration 297, loss = 0.32250513\n",
      "Iteration 298, loss = 0.32116912\n",
      "Iteration 299, loss = 0.32122301\n",
      "Iteration 300, loss = 0.32103112\n",
      "Iteration 301, loss = 0.32170398\n",
      "Iteration 302, loss = 0.32170417\n",
      "Iteration 303, loss = 0.32030200\n",
      "Iteration 304, loss = 0.32100201\n",
      "Iteration 305, loss = 0.32053432\n",
      "Iteration 306, loss = 0.32099077\n",
      "Iteration 307, loss = 0.32182857\n",
      "Iteration 308, loss = 0.31988223\n",
      "Iteration 309, loss = 0.32158479\n",
      "Iteration 310, loss = 0.31992830\n",
      "Iteration 311, loss = 0.31994813\n",
      "Iteration 312, loss = 0.31967431\n",
      "Iteration 313, loss = 0.32139144\n",
      "Iteration 314, loss = 0.32017065\n",
      "Iteration 315, loss = 0.31986830\n",
      "Iteration 316, loss = 0.31966383\n",
      "Iteration 317, loss = 0.32117102\n",
      "Iteration 318, loss = 0.31963282\n",
      "Iteration 319, loss = 0.31903760\n",
      "Iteration 320, loss = 0.32064425\n",
      "Iteration 321, loss = 0.31935846\n",
      "Iteration 322, loss = 0.31917377\n",
      "Iteration 323, loss = 0.31894355\n",
      "Iteration 324, loss = 0.31922720\n",
      "Iteration 325, loss = 0.31882793\n",
      "Iteration 326, loss = 0.31888491\n",
      "Iteration 327, loss = 0.32146151\n",
      "Iteration 328, loss = 0.31904383\n",
      "Iteration 329, loss = 0.31853969\n",
      "Iteration 330, loss = 0.31815502\n",
      "Iteration 331, loss = 0.31814670\n",
      "Iteration 332, loss = 0.31819220\n",
      "Iteration 333, loss = 0.31945567\n",
      "Iteration 334, loss = 0.31963031\n",
      "Iteration 335, loss = 0.31920372\n",
      "Iteration 336, loss = 0.31726519\n",
      "Iteration 337, loss = 0.31845253\n",
      "Iteration 338, loss = 0.31839071\n",
      "Iteration 339, loss = 0.31870198\n",
      "Iteration 340, loss = 0.31819964\n",
      "Iteration 341, loss = 0.31710733\n",
      "Iteration 342, loss = 0.31675670\n",
      "Iteration 343, loss = 0.31822602\n",
      "Iteration 344, loss = 0.31742557\n",
      "Iteration 345, loss = 0.31686599\n",
      "Iteration 346, loss = 0.31854951\n",
      "Iteration 347, loss = 0.31785435\n",
      "Iteration 348, loss = 0.31657639\n",
      "Iteration 349, loss = 0.31750700\n",
      "Iteration 350, loss = 0.31731632\n",
      "Iteration 351, loss = 0.31745055\n",
      "Iteration 352, loss = 0.31806112\n",
      "Iteration 353, loss = 0.31711759\n",
      "Iteration 354, loss = 0.31648882\n",
      "Iteration 355, loss = 0.31706374\n",
      "Iteration 356, loss = 0.31715180\n",
      "Iteration 357, loss = 0.31651577\n",
      "Iteration 358, loss = 0.31696501\n",
      "Iteration 359, loss = 0.31739660\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51780161\n",
      "Iteration 2, loss = 0.47080148\n",
      "Iteration 3, loss = 0.46270561\n",
      "Iteration 4, loss = 0.45689429\n",
      "Iteration 5, loss = 0.45169734\n",
      "Iteration 6, loss = 0.44733817\n",
      "Iteration 7, loss = 0.44383501\n",
      "Iteration 8, loss = 0.43941724\n",
      "Iteration 9, loss = 0.43687515\n",
      "Iteration 10, loss = 0.43309742\n",
      "Iteration 11, loss = 0.43021734\n",
      "Iteration 12, loss = 0.42722635\n",
      "Iteration 13, loss = 0.42438868\n",
      "Iteration 14, loss = 0.42216776\n",
      "Iteration 15, loss = 0.42001526\n",
      "Iteration 16, loss = 0.41801589\n",
      "Iteration 17, loss = 0.41610664\n",
      "Iteration 18, loss = 0.41414240\n",
      "Iteration 19, loss = 0.41213789\n",
      "Iteration 20, loss = 0.41043158\n",
      "Iteration 21, loss = 0.40925577\n",
      "Iteration 22, loss = 0.40737456\n",
      "Iteration 23, loss = 0.40608036\n",
      "Iteration 24, loss = 0.40492621\n",
      "Iteration 25, loss = 0.40302936\n",
      "Iteration 26, loss = 0.40185199\n",
      "Iteration 27, loss = 0.40077706\n",
      "Iteration 28, loss = 0.39973591\n",
      "Iteration 29, loss = 0.39800435\n",
      "Iteration 30, loss = 0.39788772\n",
      "Iteration 31, loss = 0.39594445\n",
      "Iteration 32, loss = 0.39498030\n",
      "Iteration 33, loss = 0.39389244\n",
      "Iteration 34, loss = 0.39249624\n",
      "Iteration 35, loss = 0.39139318\n",
      "Iteration 36, loss = 0.39030068\n",
      "Iteration 37, loss = 0.38958645\n",
      "Iteration 38, loss = 0.38850758\n",
      "Iteration 39, loss = 0.38715765\n",
      "Iteration 40, loss = 0.38660727\n",
      "Iteration 41, loss = 0.38557591\n",
      "Iteration 42, loss = 0.38453648\n",
      "Iteration 43, loss = 0.38351746\n",
      "Iteration 44, loss = 0.38267746\n",
      "Iteration 45, loss = 0.38137203\n",
      "Iteration 46, loss = 0.38155218\n",
      "Iteration 47, loss = 0.37961346\n",
      "Iteration 48, loss = 0.37846426\n",
      "Iteration 49, loss = 0.37847307\n",
      "Iteration 50, loss = 0.37735250\n",
      "Iteration 51, loss = 0.37596981\n",
      "Iteration 52, loss = 0.37621540\n",
      "Iteration 53, loss = 0.37480733\n",
      "Iteration 54, loss = 0.37436869\n",
      "Iteration 55, loss = 0.37376646\n",
      "Iteration 56, loss = 0.37309218\n",
      "Iteration 57, loss = 0.37176337\n",
      "Iteration 58, loss = 0.37188517\n",
      "Iteration 59, loss = 0.37072996\n",
      "Iteration 60, loss = 0.37018734\n",
      "Iteration 61, loss = 0.36952305\n",
      "Iteration 62, loss = 0.36901108\n",
      "Iteration 63, loss = 0.36820694\n",
      "Iteration 64, loss = 0.36782501\n",
      "Iteration 65, loss = 0.36736663\n",
      "Iteration 66, loss = 0.36749715\n",
      "Iteration 67, loss = 0.36633014\n",
      "Iteration 68, loss = 0.36546215\n",
      "Iteration 69, loss = 0.36454383\n",
      "Iteration 70, loss = 0.36398431\n",
      "Iteration 71, loss = 0.36331212\n",
      "Iteration 72, loss = 0.36380507\n",
      "Iteration 73, loss = 0.36239766\n",
      "Iteration 74, loss = 0.36220133\n",
      "Iteration 75, loss = 0.36077136\n",
      "Iteration 76, loss = 0.36027854\n",
      "Iteration 77, loss = 0.36090247\n",
      "Iteration 78, loss = 0.36044592\n",
      "Iteration 79, loss = 0.35949196\n",
      "Iteration 80, loss = 0.35848185\n",
      "Iteration 81, loss = 0.35857244\n",
      "Iteration 82, loss = 0.35835585\n",
      "Iteration 83, loss = 0.35791482\n",
      "Iteration 84, loss = 0.35732637\n",
      "Iteration 85, loss = 0.35611324\n",
      "Iteration 86, loss = 0.35653610\n",
      "Iteration 87, loss = 0.35601936\n",
      "Iteration 88, loss = 0.35579076\n",
      "Iteration 89, loss = 0.35429616\n",
      "Iteration 90, loss = 0.35487224\n",
      "Iteration 91, loss = 0.35471018\n",
      "Iteration 92, loss = 0.35390737\n",
      "Iteration 93, loss = 0.35270426\n",
      "Iteration 94, loss = 0.35322420\n",
      "Iteration 95, loss = 0.35281372\n",
      "Iteration 96, loss = 0.35249025\n",
      "Iteration 97, loss = 0.35123520\n",
      "Iteration 98, loss = 0.35218502\n",
      "Iteration 99, loss = 0.35239549\n",
      "Iteration 100, loss = 0.35167767\n",
      "Iteration 101, loss = 0.35076877\n",
      "Iteration 102, loss = 0.34961752\n",
      "Iteration 103, loss = 0.35029813\n",
      "Iteration 104, loss = 0.34929014\n",
      "Iteration 105, loss = 0.34917893\n",
      "Iteration 106, loss = 0.34876196\n",
      "Iteration 107, loss = 0.34811793\n",
      "Iteration 108, loss = 0.34837288\n",
      "Iteration 109, loss = 0.34809552\n",
      "Iteration 110, loss = 0.34728014\n",
      "Iteration 111, loss = 0.34779686\n",
      "Iteration 112, loss = 0.34665342\n",
      "Iteration 113, loss = 0.34682654\n",
      "Iteration 114, loss = 0.34633853\n",
      "Iteration 115, loss = 0.34510717\n",
      "Iteration 116, loss = 0.34569687\n",
      "Iteration 117, loss = 0.34556778\n",
      "Iteration 118, loss = 0.34487198\n",
      "Iteration 119, loss = 0.34566736\n",
      "Iteration 120, loss = 0.34495272\n",
      "Iteration 121, loss = 0.34391405\n",
      "Iteration 122, loss = 0.34310999\n",
      "Iteration 123, loss = 0.34448141\n",
      "Iteration 124, loss = 0.34347381\n",
      "Iteration 125, loss = 0.34286253\n",
      "Iteration 126, loss = 0.34348476\n",
      "Iteration 127, loss = 0.34319740\n",
      "Iteration 128, loss = 0.34321616\n",
      "Iteration 129, loss = 0.34154058\n",
      "Iteration 130, loss = 0.34189198\n",
      "Iteration 131, loss = 0.34300021\n",
      "Iteration 132, loss = 0.34169992\n",
      "Iteration 133, loss = 0.34109731\n",
      "Iteration 134, loss = 0.34136830\n",
      "Iteration 135, loss = 0.34155950\n",
      "Iteration 136, loss = 0.33989490\n",
      "Iteration 137, loss = 0.34061205\n",
      "Iteration 138, loss = 0.34061849\n",
      "Iteration 139, loss = 0.34000714\n",
      "Iteration 140, loss = 0.34049594\n",
      "Iteration 141, loss = 0.33909073\n",
      "Iteration 142, loss = 0.33901283\n",
      "Iteration 143, loss = 0.33934121\n",
      "Iteration 144, loss = 0.33863262\n",
      "Iteration 145, loss = 0.33849966\n",
      "Iteration 146, loss = 0.33784630\n",
      "Iteration 147, loss = 0.33800038\n",
      "Iteration 148, loss = 0.33759181\n",
      "Iteration 149, loss = 0.33778761\n",
      "Iteration 150, loss = 0.33794236\n",
      "Iteration 151, loss = 0.33753114\n",
      "Iteration 152, loss = 0.33694162\n",
      "Iteration 153, loss = 0.33754284\n",
      "Iteration 154, loss = 0.33663183\n",
      "Iteration 155, loss = 0.33626016\n",
      "Iteration 156, loss = 0.33637358\n",
      "Iteration 157, loss = 0.33548130\n",
      "Iteration 158, loss = 0.33634808\n",
      "Iteration 159, loss = 0.33581758\n",
      "Iteration 160, loss = 0.33450193\n",
      "Iteration 161, loss = 0.33513101\n",
      "Iteration 162, loss = 0.33408130\n",
      "Iteration 163, loss = 0.33491897\n",
      "Iteration 164, loss = 0.33446624\n",
      "Iteration 165, loss = 0.33402613\n",
      "Iteration 166, loss = 0.33391801\n",
      "Iteration 167, loss = 0.33431818\n",
      "Iteration 168, loss = 0.33368653\n",
      "Iteration 169, loss = 0.33345622\n",
      "Iteration 170, loss = 0.33301273\n",
      "Iteration 171, loss = 0.33329387\n",
      "Iteration 172, loss = 0.33251548\n",
      "Iteration 173, loss = 0.33306798\n",
      "Iteration 174, loss = 0.33217709\n",
      "Iteration 175, loss = 0.33202850\n",
      "Iteration 176, loss = 0.33215589\n",
      "Iteration 177, loss = 0.33076462\n",
      "Iteration 178, loss = 0.33194309\n",
      "Iteration 179, loss = 0.33257438\n",
      "Iteration 180, loss = 0.33113948\n",
      "Iteration 181, loss = 0.33081610\n",
      "Iteration 182, loss = 0.33134567\n",
      "Iteration 183, loss = 0.33166682\n",
      "Iteration 184, loss = 0.33145261\n",
      "Iteration 185, loss = 0.33013310\n",
      "Iteration 186, loss = 0.32960861\n",
      "Iteration 187, loss = 0.32930966\n",
      "Iteration 188, loss = 0.33097985\n",
      "Iteration 189, loss = 0.33057918\n",
      "Iteration 190, loss = 0.32966849\n",
      "Iteration 191, loss = 0.32929113\n",
      "Iteration 192, loss = 0.32998892\n",
      "Iteration 193, loss = 0.32892331\n",
      "Iteration 194, loss = 0.32863747\n",
      "Iteration 195, loss = 0.33030092\n",
      "Iteration 196, loss = 0.32928740\n",
      "Iteration 197, loss = 0.32813301\n",
      "Iteration 198, loss = 0.32922010\n",
      "Iteration 199, loss = 0.32760675\n",
      "Iteration 200, loss = 0.32837745\n",
      "Iteration 201, loss = 0.32887526\n",
      "Iteration 202, loss = 0.32781635\n",
      "Iteration 203, loss = 0.32726590\n",
      "Iteration 204, loss = 0.32877458\n",
      "Iteration 205, loss = 0.32699215\n",
      "Iteration 206, loss = 0.32794832\n",
      "Iteration 207, loss = 0.32748050\n",
      "Iteration 208, loss = 0.32803773\n",
      "Iteration 209, loss = 0.32695242\n",
      "Iteration 210, loss = 0.32714239\n",
      "Iteration 211, loss = 0.32603134\n",
      "Iteration 212, loss = 0.32654893\n",
      "Iteration 213, loss = 0.32592798\n",
      "Iteration 214, loss = 0.32535676\n",
      "Iteration 215, loss = 0.32636014\n",
      "Iteration 216, loss = 0.32615228\n",
      "Iteration 217, loss = 0.32613683\n",
      "Iteration 218, loss = 0.32597558\n",
      "Iteration 219, loss = 0.32576218\n",
      "Iteration 220, loss = 0.32489241\n",
      "Iteration 221, loss = 0.32585538\n",
      "Iteration 222, loss = 0.32533268\n",
      "Iteration 223, loss = 0.32569762\n",
      "Iteration 224, loss = 0.32517161\n",
      "Iteration 225, loss = 0.32449954\n",
      "Iteration 226, loss = 0.32550125\n",
      "Iteration 227, loss = 0.32448243\n",
      "Iteration 228, loss = 0.32484205\n",
      "Iteration 229, loss = 0.32368104\n",
      "Iteration 230, loss = 0.32460073\n",
      "Iteration 231, loss = 0.32371206\n",
      "Iteration 232, loss = 0.32419618\n",
      "Iteration 233, loss = 0.32363135\n",
      "Iteration 234, loss = 0.32529901\n",
      "Iteration 235, loss = 0.32313648\n",
      "Iteration 236, loss = 0.32386907\n",
      "Iteration 237, loss = 0.32311890\n",
      "Iteration 238, loss = 0.32324966\n",
      "Iteration 239, loss = 0.32348813\n",
      "Iteration 240, loss = 0.32297748\n",
      "Iteration 241, loss = 0.32291188\n",
      "Iteration 242, loss = 0.32308137\n",
      "Iteration 243, loss = 0.32240661\n",
      "Iteration 244, loss = 0.32285263\n",
      "Iteration 245, loss = 0.32243545\n",
      "Iteration 246, loss = 0.32274512\n",
      "Iteration 247, loss = 0.32237658\n",
      "Iteration 248, loss = 0.32157142\n",
      "Iteration 249, loss = 0.32262132\n",
      "Iteration 250, loss = 0.32142153\n",
      "Iteration 251, loss = 0.32270961\n",
      "Iteration 252, loss = 0.32232255\n",
      "Iteration 253, loss = 0.32155505\n",
      "Iteration 254, loss = 0.32095816\n",
      "Iteration 255, loss = 0.32255878\n",
      "Iteration 256, loss = 0.32093370\n",
      "Iteration 257, loss = 0.32140361\n",
      "Iteration 258, loss = 0.32124981\n",
      "Iteration 259, loss = 0.32089808\n",
      "Iteration 260, loss = 0.32114858\n",
      "Iteration 261, loss = 0.32121806\n",
      "Iteration 262, loss = 0.32051090\n",
      "Iteration 263, loss = 0.32128064\n",
      "Iteration 264, loss = 0.32001182\n",
      "Iteration 265, loss = 0.31978732\n",
      "Iteration 266, loss = 0.32095175\n",
      "Iteration 267, loss = 0.32033369\n",
      "Iteration 268, loss = 0.31981898\n",
      "Iteration 269, loss = 0.31979227\n",
      "Iteration 270, loss = 0.31868214\n",
      "Iteration 271, loss = 0.31980136\n",
      "Iteration 272, loss = 0.32041483\n",
      "Iteration 273, loss = 0.31877588\n",
      "Iteration 274, loss = 0.31948341\n",
      "Iteration 275, loss = 0.31999973\n",
      "Iteration 276, loss = 0.31955392\n",
      "Iteration 277, loss = 0.31992795\n",
      "Iteration 278, loss = 0.31837936\n",
      "Iteration 279, loss = 0.31975711\n",
      "Iteration 280, loss = 0.31853555\n",
      "Iteration 281, loss = 0.31884986\n",
      "Iteration 282, loss = 0.31998870\n",
      "Iteration 283, loss = 0.31879015\n",
      "Iteration 284, loss = 0.31946787\n",
      "Iteration 285, loss = 0.31786686\n",
      "Iteration 286, loss = 0.31861554\n",
      "Iteration 287, loss = 0.31761420\n",
      "Iteration 288, loss = 0.31739092\n",
      "Iteration 289, loss = 0.31900304\n",
      "Iteration 290, loss = 0.31828833\n",
      "Iteration 291, loss = 0.31758277\n",
      "Iteration 292, loss = 0.31707770\n",
      "Iteration 293, loss = 0.31808773\n",
      "Iteration 294, loss = 0.31716173\n",
      "Iteration 295, loss = 0.31799676\n",
      "Iteration 296, loss = 0.31676860\n",
      "Iteration 297, loss = 0.31728062\n",
      "Iteration 298, loss = 0.31862630\n",
      "Iteration 299, loss = 0.31785713\n",
      "Iteration 300, loss = 0.31587393\n",
      "Iteration 301, loss = 0.31728719\n",
      "Iteration 302, loss = 0.31796360\n",
      "Iteration 303, loss = 0.31658461\n",
      "Iteration 304, loss = 0.31775423\n",
      "Iteration 305, loss = 0.31644370\n",
      "Iteration 306, loss = 0.31611150\n",
      "Iteration 307, loss = 0.31559560\n",
      "Iteration 308, loss = 0.31712967\n",
      "Iteration 309, loss = 0.31677843\n",
      "Iteration 310, loss = 0.31667814\n",
      "Iteration 311, loss = 0.31705742\n",
      "Iteration 312, loss = 0.31644629\n",
      "Iteration 313, loss = 0.31492500\n",
      "Iteration 314, loss = 0.31604933\n",
      "Iteration 315, loss = 0.31654513\n",
      "Iteration 316, loss = 0.31627055\n",
      "Iteration 317, loss = 0.31635463\n",
      "Iteration 318, loss = 0.31501648\n",
      "Iteration 319, loss = 0.31571070\n",
      "Iteration 320, loss = 0.31534693\n",
      "Iteration 321, loss = 0.31608178\n",
      "Iteration 322, loss = 0.31513430\n",
      "Iteration 323, loss = 0.31649953\n",
      "Iteration 324, loss = 0.31437492\n",
      "Iteration 325, loss = 0.31563761\n",
      "Iteration 326, loss = 0.31479698\n",
      "Iteration 327, loss = 0.31515175\n",
      "Iteration 328, loss = 0.31510259\n",
      "Iteration 329, loss = 0.31422610\n",
      "Iteration 330, loss = 0.31432545\n",
      "Iteration 331, loss = 0.31400661\n",
      "Iteration 332, loss = 0.31491062\n",
      "Iteration 333, loss = 0.31443508\n",
      "Iteration 334, loss = 0.31494606\n",
      "Iteration 335, loss = 0.31651482\n",
      "Iteration 336, loss = 0.31364080\n",
      "Iteration 337, loss = 0.31330716\n",
      "Iteration 338, loss = 0.31392945\n",
      "Iteration 339, loss = 0.31447923\n",
      "Iteration 340, loss = 0.31301763\n",
      "Iteration 341, loss = 0.31300428\n",
      "Iteration 342, loss = 0.31483825\n",
      "Iteration 343, loss = 0.31224361\n",
      "Iteration 344, loss = 0.31432854\n",
      "Iteration 345, loss = 0.31354252\n",
      "Iteration 346, loss = 0.31332397\n",
      "Iteration 347, loss = 0.31294155\n",
      "Iteration 348, loss = 0.31347190\n",
      "Iteration 349, loss = 0.31307049\n",
      "Iteration 350, loss = 0.31369527\n",
      "Iteration 351, loss = 0.31371358\n",
      "Iteration 352, loss = 0.31284487\n",
      "Iteration 353, loss = 0.31384573\n",
      "Iteration 354, loss = 0.31322591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51261647\n",
      "Iteration 2, loss = 0.46701673\n",
      "Iteration 3, loss = 0.45832813\n",
      "Iteration 4, loss = 0.45355935\n",
      "Iteration 5, loss = 0.44947936\n",
      "Iteration 6, loss = 0.44559264\n",
      "Iteration 7, loss = 0.44236658\n",
      "Iteration 8, loss = 0.43924249\n",
      "Iteration 9, loss = 0.43711351\n",
      "Iteration 10, loss = 0.43418403\n",
      "Iteration 11, loss = 0.43188423\n",
      "Iteration 12, loss = 0.42978469\n",
      "Iteration 13, loss = 0.42737791\n",
      "Iteration 14, loss = 0.42554199\n",
      "Iteration 15, loss = 0.42370997\n",
      "Iteration 16, loss = 0.42222251\n",
      "Iteration 17, loss = 0.42046697\n",
      "Iteration 18, loss = 0.41873711\n",
      "Iteration 19, loss = 0.41731326\n",
      "Iteration 20, loss = 0.41625314\n",
      "Iteration 21, loss = 0.41439943\n",
      "Iteration 22, loss = 0.41282398\n",
      "Iteration 23, loss = 0.41200315\n",
      "Iteration 24, loss = 0.41038483\n",
      "Iteration 25, loss = 0.40958615\n",
      "Iteration 26, loss = 0.40799373\n",
      "Iteration 27, loss = 0.40678657\n",
      "Iteration 28, loss = 0.40616208\n",
      "Iteration 29, loss = 0.40456256\n",
      "Iteration 30, loss = 0.40423578\n",
      "Iteration 31, loss = 0.40353033\n",
      "Iteration 32, loss = 0.40192298\n",
      "Iteration 33, loss = 0.40084191\n",
      "Iteration 34, loss = 0.40013764\n",
      "Iteration 35, loss = 0.39885082\n",
      "Iteration 36, loss = 0.39887515\n",
      "Iteration 37, loss = 0.39738581\n",
      "Iteration 38, loss = 0.39550679\n",
      "Iteration 39, loss = 0.39501081\n",
      "Iteration 40, loss = 0.39459809\n",
      "Iteration 41, loss = 0.39330471\n",
      "Iteration 42, loss = 0.39202327\n",
      "Iteration 43, loss = 0.39137909\n",
      "Iteration 44, loss = 0.39088143\n",
      "Iteration 45, loss = 0.39022357\n",
      "Iteration 46, loss = 0.38875019\n",
      "Iteration 47, loss = 0.38804634\n",
      "Iteration 48, loss = 0.38767649\n",
      "Iteration 49, loss = 0.38625256\n",
      "Iteration 50, loss = 0.38624589\n",
      "Iteration 51, loss = 0.38577172\n",
      "Iteration 52, loss = 0.38425584\n",
      "Iteration 53, loss = 0.38332595\n",
      "Iteration 54, loss = 0.38252524\n",
      "Iteration 55, loss = 0.38210249\n",
      "Iteration 56, loss = 0.38212713\n",
      "Iteration 57, loss = 0.38135112\n",
      "Iteration 58, loss = 0.38091709\n",
      "Iteration 59, loss = 0.38025868\n",
      "Iteration 60, loss = 0.37928461\n",
      "Iteration 61, loss = 0.37885000\n",
      "Iteration 62, loss = 0.37932247\n",
      "Iteration 63, loss = 0.37715256\n",
      "Iteration 64, loss = 0.37716881\n",
      "Iteration 65, loss = 0.37618526\n",
      "Iteration 66, loss = 0.37653428\n",
      "Iteration 67, loss = 0.37510023\n",
      "Iteration 68, loss = 0.37558001\n",
      "Iteration 69, loss = 0.37514186\n",
      "Iteration 70, loss = 0.37491318\n",
      "Iteration 71, loss = 0.37350598\n",
      "Iteration 72, loss = 0.37286268\n",
      "Iteration 73, loss = 0.37254037\n",
      "Iteration 74, loss = 0.37224577\n",
      "Iteration 75, loss = 0.37132705\n",
      "Iteration 76, loss = 0.37086229\n",
      "Iteration 77, loss = 0.37038874\n",
      "Iteration 78, loss = 0.37049508\n",
      "Iteration 79, loss = 0.37062011\n",
      "Iteration 80, loss = 0.36985957\n",
      "Iteration 81, loss = 0.36921125\n",
      "Iteration 82, loss = 0.36851692\n",
      "Iteration 83, loss = 0.36788553\n",
      "Iteration 84, loss = 0.36710099\n",
      "Iteration 85, loss = 0.36771463\n",
      "Iteration 86, loss = 0.36587527\n",
      "Iteration 87, loss = 0.36597429\n",
      "Iteration 88, loss = 0.36622977\n",
      "Iteration 89, loss = 0.36625939\n",
      "Iteration 90, loss = 0.36472892\n",
      "Iteration 91, loss = 0.36490213\n",
      "Iteration 92, loss = 0.36347731\n",
      "Iteration 93, loss = 0.36442120\n",
      "Iteration 94, loss = 0.36321980\n",
      "Iteration 95, loss = 0.36326737\n",
      "Iteration 96, loss = 0.36282578\n",
      "Iteration 97, loss = 0.36215017\n",
      "Iteration 98, loss = 0.36193154\n",
      "Iteration 99, loss = 0.36220003\n",
      "Iteration 100, loss = 0.36129845\n",
      "Iteration 101, loss = 0.36042636\n",
      "Iteration 102, loss = 0.36129072\n",
      "Iteration 103, loss = 0.35965521\n",
      "Iteration 104, loss = 0.36007432\n",
      "Iteration 105, loss = 0.35927994\n",
      "Iteration 106, loss = 0.35918197\n",
      "Iteration 107, loss = 0.35910994\n",
      "Iteration 108, loss = 0.35855379\n",
      "Iteration 109, loss = 0.35933043\n",
      "Iteration 110, loss = 0.35816397\n",
      "Iteration 111, loss = 0.35820364\n",
      "Iteration 112, loss = 0.35849293\n",
      "Iteration 113, loss = 0.35677685\n",
      "Iteration 114, loss = 0.35653951\n",
      "Iteration 115, loss = 0.35633641\n",
      "Iteration 116, loss = 0.35639928\n",
      "Iteration 117, loss = 0.35601807\n",
      "Iteration 118, loss = 0.35658873\n",
      "Iteration 119, loss = 0.35555214\n",
      "Iteration 120, loss = 0.35551275\n",
      "Iteration 121, loss = 0.35433190\n",
      "Iteration 122, loss = 0.35476001\n",
      "Iteration 123, loss = 0.35493736\n",
      "Iteration 124, loss = 0.35450713\n",
      "Iteration 125, loss = 0.35372950\n",
      "Iteration 126, loss = 0.35371958\n",
      "Iteration 127, loss = 0.35368547\n",
      "Iteration 128, loss = 0.35333108\n",
      "Iteration 129, loss = 0.35336562\n",
      "Iteration 130, loss = 0.35218109\n",
      "Iteration 131, loss = 0.35261077\n",
      "Iteration 132, loss = 0.35267339\n",
      "Iteration 133, loss = 0.35159827\n",
      "Iteration 134, loss = 0.35246418\n",
      "Iteration 135, loss = 0.35257251\n",
      "Iteration 136, loss = 0.35124529\n",
      "Iteration 137, loss = 0.35088733\n",
      "Iteration 138, loss = 0.35101447\n",
      "Iteration 139, loss = 0.34996496\n",
      "Iteration 140, loss = 0.35102207\n",
      "Iteration 141, loss = 0.34985087\n",
      "Iteration 142, loss = 0.34966203\n",
      "Iteration 143, loss = 0.34914356\n",
      "Iteration 144, loss = 0.34985549\n",
      "Iteration 145, loss = 0.34941151\n",
      "Iteration 146, loss = 0.34930558\n",
      "Iteration 147, loss = 0.34902580\n",
      "Iteration 148, loss = 0.34825522\n",
      "Iteration 149, loss = 0.34965602\n",
      "Iteration 150, loss = 0.34686718\n",
      "Iteration 151, loss = 0.34795336\n",
      "Iteration 152, loss = 0.34723933\n",
      "Iteration 153, loss = 0.34723959\n",
      "Iteration 154, loss = 0.34793003\n",
      "Iteration 155, loss = 0.34613171\n",
      "Iteration 156, loss = 0.34741205\n",
      "Iteration 157, loss = 0.34591446\n",
      "Iteration 158, loss = 0.34766572\n",
      "Iteration 159, loss = 0.34680916\n",
      "Iteration 160, loss = 0.34583703\n",
      "Iteration 161, loss = 0.34584586\n",
      "Iteration 162, loss = 0.34545822\n",
      "Iteration 163, loss = 0.34547592\n",
      "Iteration 164, loss = 0.34551397\n",
      "Iteration 165, loss = 0.34538317\n",
      "Iteration 166, loss = 0.34561810\n",
      "Iteration 167, loss = 0.34468744\n",
      "Iteration 168, loss = 0.34529063\n",
      "Iteration 169, loss = 0.34519232\n",
      "Iteration 170, loss = 0.34407091\n",
      "Iteration 171, loss = 0.34380263\n",
      "Iteration 172, loss = 0.34320212\n",
      "Iteration 173, loss = 0.34314449\n",
      "Iteration 174, loss = 0.34304635\n",
      "Iteration 175, loss = 0.34309881\n",
      "Iteration 176, loss = 0.34349002\n",
      "Iteration 177, loss = 0.34367803\n",
      "Iteration 178, loss = 0.34283954\n",
      "Iteration 179, loss = 0.34251461\n",
      "Iteration 180, loss = 0.34220322\n",
      "Iteration 181, loss = 0.34216068\n",
      "Iteration 182, loss = 0.34216046\n",
      "Iteration 183, loss = 0.34314491\n",
      "Iteration 184, loss = 0.34149694\n",
      "Iteration 185, loss = 0.34278620\n",
      "Iteration 186, loss = 0.34074998\n",
      "Iteration 187, loss = 0.34099984\n",
      "Iteration 188, loss = 0.34034941\n",
      "Iteration 189, loss = 0.34140412\n",
      "Iteration 190, loss = 0.34060470\n",
      "Iteration 191, loss = 0.34057234\n",
      "Iteration 192, loss = 0.34093204\n",
      "Iteration 193, loss = 0.34095298\n",
      "Iteration 194, loss = 0.34112182\n",
      "Iteration 195, loss = 0.33942841\n",
      "Iteration 196, loss = 0.33914446\n",
      "Iteration 197, loss = 0.33981021\n",
      "Iteration 198, loss = 0.34026619\n",
      "Iteration 199, loss = 0.34016203\n",
      "Iteration 200, loss = 0.33967399\n",
      "Iteration 201, loss = 0.33884027\n",
      "Iteration 202, loss = 0.33900014\n",
      "Iteration 203, loss = 0.34013079\n",
      "Iteration 204, loss = 0.33902123\n",
      "Iteration 205, loss = 0.33948815\n",
      "Iteration 206, loss = 0.33751607\n",
      "Iteration 207, loss = 0.33835998\n",
      "Iteration 208, loss = 0.33782629\n",
      "Iteration 209, loss = 0.33747724\n",
      "Iteration 210, loss = 0.33847195\n",
      "Iteration 211, loss = 0.33815893\n",
      "Iteration 212, loss = 0.33812081\n",
      "Iteration 213, loss = 0.33732989\n",
      "Iteration 214, loss = 0.33670015\n",
      "Iteration 215, loss = 0.33655730\n",
      "Iteration 216, loss = 0.33689614\n",
      "Iteration 217, loss = 0.33625415\n",
      "Iteration 218, loss = 0.33680021\n",
      "Iteration 219, loss = 0.33701853\n",
      "Iteration 220, loss = 0.33707945\n",
      "Iteration 221, loss = 0.33645559\n",
      "Iteration 222, loss = 0.33565159\n",
      "Iteration 223, loss = 0.33630113\n",
      "Iteration 224, loss = 0.33539297\n",
      "Iteration 225, loss = 0.33597851\n",
      "Iteration 226, loss = 0.33585786\n",
      "Iteration 227, loss = 0.33442392\n",
      "Iteration 228, loss = 0.33631590\n",
      "Iteration 229, loss = 0.33541160\n",
      "Iteration 230, loss = 0.33613253\n",
      "Iteration 231, loss = 0.33441864\n",
      "Iteration 232, loss = 0.33452379\n",
      "Iteration 233, loss = 0.33413351\n",
      "Iteration 234, loss = 0.33507166\n",
      "Iteration 235, loss = 0.33448310\n",
      "Iteration 236, loss = 0.33469285\n",
      "Iteration 237, loss = 0.33480042\n",
      "Iteration 238, loss = 0.33422881\n",
      "Iteration 239, loss = 0.33403276\n",
      "Iteration 240, loss = 0.33319853\n",
      "Iteration 241, loss = 0.33446409\n",
      "Iteration 242, loss = 0.33395588\n",
      "Iteration 243, loss = 0.33325147\n",
      "Iteration 244, loss = 0.33300262\n",
      "Iteration 245, loss = 0.33309159\n",
      "Iteration 246, loss = 0.33326196\n",
      "Iteration 247, loss = 0.33337567\n",
      "Iteration 248, loss = 0.33265672\n",
      "Iteration 249, loss = 0.33355917\n",
      "Iteration 250, loss = 0.33408294\n",
      "Iteration 251, loss = 0.33296468\n",
      "Iteration 252, loss = 0.33233929\n",
      "Iteration 253, loss = 0.33128711\n",
      "Iteration 254, loss = 0.33172106\n",
      "Iteration 255, loss = 0.33271956\n",
      "Iteration 256, loss = 0.33220854\n",
      "Iteration 257, loss = 0.33128167\n",
      "Iteration 258, loss = 0.33224740\n",
      "Iteration 259, loss = 0.33179316\n",
      "Iteration 260, loss = 0.33212189\n",
      "Iteration 261, loss = 0.33158688\n",
      "Iteration 262, loss = 0.33085088\n",
      "Iteration 263, loss = 0.33184543\n",
      "Iteration 264, loss = 0.33217880\n",
      "Iteration 265, loss = 0.33112930\n",
      "Iteration 266, loss = 0.33163500\n",
      "Iteration 267, loss = 0.32978667\n",
      "Iteration 268, loss = 0.33076222\n",
      "Iteration 269, loss = 0.33095463\n",
      "Iteration 270, loss = 0.33095914\n",
      "Iteration 271, loss = 0.33057258\n",
      "Iteration 272, loss = 0.33060478\n",
      "Iteration 273, loss = 0.33045953\n",
      "Iteration 274, loss = 0.32971120\n",
      "Iteration 275, loss = 0.33129615\n",
      "Iteration 276, loss = 0.33096356\n",
      "Iteration 277, loss = 0.32992704\n",
      "Iteration 278, loss = 0.32937462\n",
      "Iteration 279, loss = 0.32951239\n",
      "Iteration 280, loss = 0.33022238\n",
      "Iteration 281, loss = 0.32991390\n",
      "Iteration 282, loss = 0.32944387\n",
      "Iteration 283, loss = 0.32933132\n",
      "Iteration 284, loss = 0.32964775\n",
      "Iteration 285, loss = 0.32981430\n",
      "Iteration 286, loss = 0.32960468\n",
      "Iteration 287, loss = 0.32847997\n",
      "Iteration 288, loss = 0.32890307\n",
      "Iteration 289, loss = 0.32909607\n",
      "Iteration 290, loss = 0.32909752\n",
      "Iteration 291, loss = 0.32865609\n",
      "Iteration 292, loss = 0.32768262\n",
      "Iteration 293, loss = 0.32836587\n",
      "Iteration 294, loss = 0.32880553\n",
      "Iteration 295, loss = 0.32914321\n",
      "Iteration 296, loss = 0.32907758\n",
      "Iteration 297, loss = 0.33011250\n",
      "Iteration 298, loss = 0.32876558\n",
      "Iteration 299, loss = 0.32809543\n",
      "Iteration 300, loss = 0.32779339\n",
      "Iteration 301, loss = 0.32705026\n",
      "Iteration 302, loss = 0.32774503\n",
      "Iteration 303, loss = 0.32818721\n",
      "Iteration 304, loss = 0.32722474\n",
      "Iteration 305, loss = 0.32770256\n",
      "Iteration 306, loss = 0.32726949\n",
      "Iteration 307, loss = 0.32835785\n",
      "Iteration 308, loss = 0.32806981\n",
      "Iteration 309, loss = 0.32730686\n",
      "Iteration 310, loss = 0.32809789\n",
      "Iteration 311, loss = 0.32702973\n",
      "Iteration 312, loss = 0.32659818\n",
      "Iteration 313, loss = 0.32753970\n",
      "Iteration 314, loss = 0.32731748\n",
      "Iteration 315, loss = 0.32743204\n",
      "Iteration 316, loss = 0.32662208\n",
      "Iteration 317, loss = 0.32679285\n",
      "Iteration 318, loss = 0.32671802\n",
      "Iteration 319, loss = 0.32635586\n",
      "Iteration 320, loss = 0.32559618\n",
      "Iteration 321, loss = 0.32780640\n",
      "Iteration 322, loss = 0.32578170\n",
      "Iteration 323, loss = 0.32574355\n",
      "Iteration 324, loss = 0.32573806\n",
      "Iteration 325, loss = 0.32577876\n",
      "Iteration 326, loss = 0.32688553\n",
      "Iteration 327, loss = 0.32570281\n",
      "Iteration 328, loss = 0.32497607\n",
      "Iteration 329, loss = 0.32507583\n",
      "Iteration 330, loss = 0.32508555\n",
      "Iteration 331, loss = 0.32613273\n",
      "Iteration 332, loss = 0.32501230\n",
      "Iteration 333, loss = 0.32563286\n",
      "Iteration 334, loss = 0.32480739\n",
      "Iteration 335, loss = 0.32518942\n",
      "Iteration 336, loss = 0.32564833\n",
      "Iteration 337, loss = 0.32523760\n",
      "Iteration 338, loss = 0.32560773\n",
      "Iteration 339, loss = 0.32571565\n",
      "Iteration 340, loss = 0.32549974\n",
      "Iteration 341, loss = 0.32423893\n",
      "Iteration 342, loss = 0.32531759\n",
      "Iteration 343, loss = 0.32406632\n",
      "Iteration 344, loss = 0.32461399\n",
      "Iteration 345, loss = 0.32415166\n",
      "Iteration 346, loss = 0.32357372\n",
      "Iteration 347, loss = 0.32450098\n",
      "Iteration 348, loss = 0.32338753\n",
      "Iteration 349, loss = 0.32394244\n",
      "Iteration 350, loss = 0.32349461\n",
      "Iteration 351, loss = 0.32446701\n",
      "Iteration 352, loss = 0.32505486\n",
      "Iteration 353, loss = 0.32420850\n",
      "Iteration 354, loss = 0.32384741\n",
      "Iteration 355, loss = 0.32303464\n",
      "Iteration 356, loss = 0.32410985\n",
      "Iteration 357, loss = 0.32390420\n",
      "Iteration 358, loss = 0.32386990\n",
      "Iteration 359, loss = 0.32338485\n",
      "Iteration 360, loss = 0.32329961\n",
      "Iteration 361, loss = 0.32295744\n",
      "Iteration 362, loss = 0.32429808\n",
      "Iteration 363, loss = 0.32446432\n",
      "Iteration 364, loss = 0.32377602\n",
      "Iteration 365, loss = 0.32239605\n",
      "Iteration 366, loss = 0.32298754\n",
      "Iteration 367, loss = 0.32246735\n",
      "Iteration 368, loss = 0.32209984\n",
      "Iteration 369, loss = 0.32251281\n",
      "Iteration 370, loss = 0.32311998\n",
      "Iteration 371, loss = 0.32243309\n",
      "Iteration 372, loss = 0.32192135\n",
      "Iteration 373, loss = 0.32219927\n",
      "Iteration 374, loss = 0.32233635\n",
      "Iteration 375, loss = 0.32231830\n",
      "Iteration 376, loss = 0.32435427\n",
      "Iteration 377, loss = 0.32187112\n",
      "Iteration 378, loss = 0.32152033\n",
      "Iteration 379, loss = 0.32252074\n",
      "Iteration 380, loss = 0.32220900\n",
      "Iteration 381, loss = 0.32234026\n",
      "Iteration 382, loss = 0.32232255\n",
      "Iteration 383, loss = 0.32193140\n",
      "Iteration 384, loss = 0.32193077\n",
      "Iteration 385, loss = 0.32114074\n",
      "Iteration 386, loss = 0.32364282\n",
      "Iteration 387, loss = 0.32153737\n",
      "Iteration 388, loss = 0.32160828\n",
      "Iteration 389, loss = 0.32036823\n",
      "Iteration 390, loss = 0.32207348\n",
      "Iteration 391, loss = 0.32149358\n",
      "Iteration 392, loss = 0.32046791\n",
      "Iteration 393, loss = 0.32106302\n",
      "Iteration 394, loss = 0.32222780\n",
      "Iteration 395, loss = 0.32206329\n",
      "Iteration 396, loss = 0.32145346\n",
      "Iteration 397, loss = 0.32129836\n",
      "Iteration 398, loss = 0.32091833\n",
      "Iteration 399, loss = 0.32212549\n",
      "Iteration 400, loss = 0.32039710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51041516\n",
      "Iteration 2, loss = 0.46376359\n",
      "Iteration 3, loss = 0.45509299\n",
      "Iteration 4, loss = 0.45008995\n",
      "Iteration 5, loss = 0.44551389\n",
      "Iteration 6, loss = 0.44192278\n",
      "Iteration 7, loss = 0.43828645\n",
      "Iteration 8, loss = 0.43498174\n",
      "Iteration 9, loss = 0.43193655\n",
      "Iteration 10, loss = 0.42937321\n",
      "Iteration 11, loss = 0.42693684\n",
      "Iteration 12, loss = 0.42500602\n",
      "Iteration 13, loss = 0.42269091\n",
      "Iteration 14, loss = 0.42088916\n",
      "Iteration 15, loss = 0.41830077\n",
      "Iteration 16, loss = 0.41672605\n",
      "Iteration 17, loss = 0.41474026\n",
      "Iteration 18, loss = 0.41297708\n",
      "Iteration 19, loss = 0.41143894\n",
      "Iteration 20, loss = 0.41001449\n",
      "Iteration 21, loss = 0.40904080\n",
      "Iteration 22, loss = 0.40749440\n",
      "Iteration 23, loss = 0.40602028\n",
      "Iteration 24, loss = 0.40510506\n",
      "Iteration 25, loss = 0.40349643\n",
      "Iteration 26, loss = 0.40247201\n",
      "Iteration 27, loss = 0.40117573\n",
      "Iteration 28, loss = 0.40032926\n",
      "Iteration 29, loss = 0.39936719\n",
      "Iteration 30, loss = 0.39887747\n",
      "Iteration 31, loss = 0.39725388\n",
      "Iteration 32, loss = 0.39645001\n",
      "Iteration 33, loss = 0.39532596\n",
      "Iteration 34, loss = 0.39420628\n",
      "Iteration 35, loss = 0.39359685\n",
      "Iteration 36, loss = 0.39371064\n",
      "Iteration 37, loss = 0.39214545\n",
      "Iteration 38, loss = 0.39129906\n",
      "Iteration 39, loss = 0.38990852\n",
      "Iteration 40, loss = 0.38967191\n",
      "Iteration 41, loss = 0.38871560\n",
      "Iteration 42, loss = 0.38729095\n",
      "Iteration 43, loss = 0.38699108\n",
      "Iteration 44, loss = 0.38606767\n",
      "Iteration 45, loss = 0.38494568\n",
      "Iteration 46, loss = 0.38469187\n",
      "Iteration 47, loss = 0.38362697\n",
      "Iteration 48, loss = 0.38286805\n",
      "Iteration 49, loss = 0.38210720\n",
      "Iteration 50, loss = 0.38198670\n",
      "Iteration 51, loss = 0.38014979\n",
      "Iteration 52, loss = 0.38020520\n",
      "Iteration 53, loss = 0.37847634\n",
      "Iteration 54, loss = 0.37832415\n",
      "Iteration 55, loss = 0.37810091\n",
      "Iteration 56, loss = 0.37686759\n",
      "Iteration 57, loss = 0.37644002\n",
      "Iteration 58, loss = 0.37570815\n",
      "Iteration 59, loss = 0.37513642\n",
      "Iteration 60, loss = 0.37446600\n",
      "Iteration 61, loss = 0.37392439\n",
      "Iteration 62, loss = 0.37364784\n",
      "Iteration 63, loss = 0.37227932\n",
      "Iteration 64, loss = 0.37228043\n",
      "Iteration 65, loss = 0.37180782\n",
      "Iteration 66, loss = 0.37065678\n",
      "Iteration 67, loss = 0.37060130\n",
      "Iteration 68, loss = 0.37057091\n",
      "Iteration 69, loss = 0.36973730\n",
      "Iteration 70, loss = 0.36889004\n",
      "Iteration 71, loss = 0.36825718\n",
      "Iteration 72, loss = 0.36763479\n",
      "Iteration 73, loss = 0.36730529\n",
      "Iteration 74, loss = 0.36667481\n",
      "Iteration 75, loss = 0.36610612\n",
      "Iteration 76, loss = 0.36642660\n",
      "Iteration 77, loss = 0.36616324\n",
      "Iteration 78, loss = 0.36548553\n",
      "Iteration 79, loss = 0.36447248\n",
      "Iteration 80, loss = 0.36392277\n",
      "Iteration 81, loss = 0.36395378\n",
      "Iteration 82, loss = 0.36322348\n",
      "Iteration 83, loss = 0.36267044\n",
      "Iteration 84, loss = 0.36353076\n",
      "Iteration 85, loss = 0.36279014\n",
      "Iteration 86, loss = 0.36197394\n",
      "Iteration 87, loss = 0.36121085\n",
      "Iteration 88, loss = 0.36078405\n",
      "Iteration 89, loss = 0.36062225\n",
      "Iteration 90, loss = 0.35995405\n",
      "Iteration 91, loss = 0.35934009\n",
      "Iteration 92, loss = 0.35841173\n",
      "Iteration 93, loss = 0.35977660\n",
      "Iteration 94, loss = 0.35917462\n",
      "Iteration 95, loss = 0.35855617\n",
      "Iteration 96, loss = 0.35797804\n",
      "Iteration 97, loss = 0.35800194\n",
      "Iteration 98, loss = 0.35718972\n",
      "Iteration 99, loss = 0.35698017\n",
      "Iteration 100, loss = 0.35687126\n",
      "Iteration 101, loss = 0.35560025\n",
      "Iteration 102, loss = 0.35639533\n",
      "Iteration 103, loss = 0.35619364\n",
      "Iteration 104, loss = 0.35512324\n",
      "Iteration 105, loss = 0.35481945\n",
      "Iteration 106, loss = 0.35500142\n",
      "Iteration 107, loss = 0.35424741\n",
      "Iteration 108, loss = 0.35489766\n",
      "Iteration 109, loss = 0.35337972\n",
      "Iteration 110, loss = 0.35262882\n",
      "Iteration 111, loss = 0.35290239\n",
      "Iteration 112, loss = 0.35297787\n",
      "Iteration 113, loss = 0.35262381\n",
      "Iteration 114, loss = 0.35237812\n",
      "Iteration 115, loss = 0.35241167\n",
      "Iteration 116, loss = 0.35232528\n",
      "Iteration 117, loss = 0.35072998\n",
      "Iteration 118, loss = 0.35113071\n",
      "Iteration 119, loss = 0.35095854\n",
      "Iteration 120, loss = 0.35058175\n",
      "Iteration 121, loss = 0.34998995\n",
      "Iteration 122, loss = 0.34963759\n",
      "Iteration 123, loss = 0.34990922\n",
      "Iteration 124, loss = 0.35070760\n",
      "Iteration 125, loss = 0.34864148\n",
      "Iteration 126, loss = 0.35050988\n",
      "Iteration 127, loss = 0.34818523\n",
      "Iteration 128, loss = 0.34756647\n",
      "Iteration 129, loss = 0.34808735\n",
      "Iteration 130, loss = 0.34788454\n",
      "Iteration 131, loss = 0.34665864\n",
      "Iteration 132, loss = 0.34634675\n",
      "Iteration 133, loss = 0.34678764\n",
      "Iteration 134, loss = 0.34671029\n",
      "Iteration 135, loss = 0.34725155\n",
      "Iteration 136, loss = 0.34626226\n",
      "Iteration 137, loss = 0.34530013\n",
      "Iteration 138, loss = 0.34678885\n",
      "Iteration 139, loss = 0.34569040\n",
      "Iteration 140, loss = 0.34518619\n",
      "Iteration 141, loss = 0.34465269\n",
      "Iteration 142, loss = 0.34409323\n",
      "Iteration 143, loss = 0.34376537\n",
      "Iteration 144, loss = 0.34443947\n",
      "Iteration 145, loss = 0.34514617\n",
      "Iteration 146, loss = 0.34405802\n",
      "Iteration 147, loss = 0.34304362\n",
      "Iteration 148, loss = 0.34354042\n",
      "Iteration 149, loss = 0.34303265\n",
      "Iteration 150, loss = 0.34256709\n",
      "Iteration 151, loss = 0.34342261\n",
      "Iteration 152, loss = 0.34177227\n",
      "Iteration 153, loss = 0.34153239\n",
      "Iteration 154, loss = 0.34222192\n",
      "Iteration 155, loss = 0.34153872\n",
      "Iteration 156, loss = 0.34125062\n",
      "Iteration 157, loss = 0.34034826\n",
      "Iteration 158, loss = 0.34050661\n",
      "Iteration 159, loss = 0.34139980\n",
      "Iteration 160, loss = 0.34074691\n",
      "Iteration 161, loss = 0.34029920\n",
      "Iteration 162, loss = 0.34009153\n",
      "Iteration 163, loss = 0.33950075\n",
      "Iteration 164, loss = 0.34013804\n",
      "Iteration 165, loss = 0.33979310\n",
      "Iteration 166, loss = 0.33872464\n",
      "Iteration 167, loss = 0.34010973\n",
      "Iteration 168, loss = 0.33893491\n",
      "Iteration 169, loss = 0.33877212\n",
      "Iteration 170, loss = 0.33900772\n",
      "Iteration 171, loss = 0.33802219\n",
      "Iteration 172, loss = 0.33782986\n",
      "Iteration 173, loss = 0.33777929\n",
      "Iteration 174, loss = 0.33758021\n",
      "Iteration 175, loss = 0.33734881\n",
      "Iteration 176, loss = 0.33820454\n",
      "Iteration 177, loss = 0.33639332\n",
      "Iteration 178, loss = 0.33697109\n",
      "Iteration 179, loss = 0.33779224\n",
      "Iteration 180, loss = 0.33629719\n",
      "Iteration 181, loss = 0.33637547\n",
      "Iteration 182, loss = 0.33580502\n",
      "Iteration 183, loss = 0.33639517\n",
      "Iteration 184, loss = 0.33557720\n",
      "Iteration 185, loss = 0.33574621\n",
      "Iteration 186, loss = 0.33585229\n",
      "Iteration 187, loss = 0.33617077\n",
      "Iteration 188, loss = 0.33543235\n",
      "Iteration 189, loss = 0.33578627\n",
      "Iteration 190, loss = 0.33504535\n",
      "Iteration 191, loss = 0.33561063\n",
      "Iteration 192, loss = 0.33412034\n",
      "Iteration 193, loss = 0.33413709\n",
      "Iteration 194, loss = 0.33365542\n",
      "Iteration 195, loss = 0.33447169\n",
      "Iteration 196, loss = 0.33455872\n",
      "Iteration 197, loss = 0.33354289\n",
      "Iteration 198, loss = 0.33325140\n",
      "Iteration 199, loss = 0.33327932\n",
      "Iteration 200, loss = 0.33307064\n",
      "Iteration 201, loss = 0.33374204\n",
      "Iteration 202, loss = 0.33355828\n",
      "Iteration 203, loss = 0.33321151\n",
      "Iteration 204, loss = 0.33158729\n",
      "Iteration 205, loss = 0.33260296\n",
      "Iteration 206, loss = 0.33216079\n",
      "Iteration 207, loss = 0.33313228\n",
      "Iteration 208, loss = 0.33279065\n",
      "Iteration 209, loss = 0.33224542\n",
      "Iteration 210, loss = 0.33103797\n",
      "Iteration 211, loss = 0.33176457\n",
      "Iteration 212, loss = 0.33200734\n",
      "Iteration 213, loss = 0.33122563\n",
      "Iteration 214, loss = 0.33136881\n",
      "Iteration 215, loss = 0.33075377\n",
      "Iteration 216, loss = 0.33084060\n",
      "Iteration 217, loss = 0.33116166\n",
      "Iteration 218, loss = 0.33092789\n",
      "Iteration 219, loss = 0.33000787\n",
      "Iteration 220, loss = 0.33064521\n",
      "Iteration 221, loss = 0.33098917\n",
      "Iteration 222, loss = 0.32938110\n",
      "Iteration 223, loss = 0.33028584\n",
      "Iteration 224, loss = 0.32986331\n",
      "Iteration 225, loss = 0.32970838\n",
      "Iteration 226, loss = 0.33020323\n",
      "Iteration 227, loss = 0.32957725\n",
      "Iteration 228, loss = 0.32961707\n",
      "Iteration 229, loss = 0.32953963\n",
      "Iteration 230, loss = 0.32949633\n",
      "Iteration 231, loss = 0.32810805\n",
      "Iteration 232, loss = 0.33009049\n",
      "Iteration 233, loss = 0.32895184\n",
      "Iteration 234, loss = 0.32753360\n",
      "Iteration 235, loss = 0.32808634\n",
      "Iteration 236, loss = 0.32856077\n",
      "Iteration 237, loss = 0.32801262\n",
      "Iteration 238, loss = 0.32723007\n",
      "Iteration 239, loss = 0.32775329\n",
      "Iteration 240, loss = 0.32761662\n",
      "Iteration 241, loss = 0.32867778\n",
      "Iteration 242, loss = 0.32761104\n",
      "Iteration 243, loss = 0.32753112\n",
      "Iteration 244, loss = 0.32749661\n",
      "Iteration 245, loss = 0.32756384\n",
      "Iteration 246, loss = 0.32741985\n",
      "Iteration 247, loss = 0.32685819\n",
      "Iteration 248, loss = 0.32693115\n",
      "Iteration 249, loss = 0.32665063\n",
      "Iteration 250, loss = 0.32587751\n",
      "Iteration 251, loss = 0.32635442\n",
      "Iteration 252, loss = 0.32652848\n",
      "Iteration 253, loss = 0.32676240\n",
      "Iteration 254, loss = 0.32670641\n",
      "Iteration 255, loss = 0.32685741\n",
      "Iteration 256, loss = 0.32519410\n",
      "Iteration 257, loss = 0.32634818\n",
      "Iteration 258, loss = 0.32621406\n",
      "Iteration 259, loss = 0.32488610\n",
      "Iteration 260, loss = 0.32477703\n",
      "Iteration 261, loss = 0.32561145\n",
      "Iteration 262, loss = 0.32518515\n",
      "Iteration 263, loss = 0.32656368\n",
      "Iteration 264, loss = 0.32536210\n",
      "Iteration 265, loss = 0.32478694\n",
      "Iteration 266, loss = 0.32413921\n",
      "Iteration 267, loss = 0.32390078\n",
      "Iteration 268, loss = 0.32459937\n",
      "Iteration 269, loss = 0.32361496\n",
      "Iteration 270, loss = 0.32450210\n",
      "Iteration 271, loss = 0.32379601\n",
      "Iteration 272, loss = 0.32445526\n",
      "Iteration 273, loss = 0.32458923\n",
      "Iteration 274, loss = 0.32393964\n",
      "Iteration 275, loss = 0.32337990\n",
      "Iteration 276, loss = 0.32558917\n",
      "Iteration 277, loss = 0.32319436\n",
      "Iteration 278, loss = 0.32483188\n",
      "Iteration 279, loss = 0.32401974\n",
      "Iteration 280, loss = 0.32247979\n",
      "Iteration 281, loss = 0.32336814\n",
      "Iteration 282, loss = 0.32277029\n",
      "Iteration 283, loss = 0.32292285\n",
      "Iteration 284, loss = 0.32319705\n",
      "Iteration 285, loss = 0.32227777\n",
      "Iteration 286, loss = 0.32350817\n",
      "Iteration 287, loss = 0.32189142\n",
      "Iteration 288, loss = 0.32262430\n",
      "Iteration 289, loss = 0.32302884\n",
      "Iteration 290, loss = 0.32157572\n",
      "Iteration 291, loss = 0.32194507\n",
      "Iteration 292, loss = 0.32234253\n",
      "Iteration 293, loss = 0.32175839\n",
      "Iteration 294, loss = 0.32282852\n",
      "Iteration 295, loss = 0.32125307\n",
      "Iteration 296, loss = 0.32122912\n",
      "Iteration 297, loss = 0.32198606\n",
      "Iteration 298, loss = 0.32108302\n",
      "Iteration 299, loss = 0.32120669\n",
      "Iteration 300, loss = 0.32134612\n",
      "Iteration 301, loss = 0.32190154\n",
      "Iteration 302, loss = 0.32099991\n",
      "Iteration 303, loss = 0.32075492\n",
      "Iteration 304, loss = 0.32143539\n",
      "Iteration 305, loss = 0.32099401\n",
      "Iteration 306, loss = 0.32072835\n",
      "Iteration 307, loss = 0.31994025\n",
      "Iteration 308, loss = 0.32054710\n",
      "Iteration 309, loss = 0.32161111\n",
      "Iteration 310, loss = 0.31970755\n",
      "Iteration 311, loss = 0.31965877\n",
      "Iteration 312, loss = 0.32032638\n",
      "Iteration 313, loss = 0.31939521\n",
      "Iteration 314, loss = 0.31995631\n",
      "Iteration 315, loss = 0.31969634\n",
      "Iteration 316, loss = 0.32127538\n",
      "Iteration 317, loss = 0.31964161\n",
      "Iteration 318, loss = 0.32027983\n",
      "Iteration 319, loss = 0.31934995\n",
      "Iteration 320, loss = 0.31975683\n",
      "Iteration 321, loss = 0.31984622\n",
      "Iteration 322, loss = 0.31914168\n",
      "Iteration 323, loss = 0.31935330\n",
      "Iteration 324, loss = 0.31926275\n",
      "Iteration 325, loss = 0.31941723\n",
      "Iteration 326, loss = 0.31864811\n",
      "Iteration 327, loss = 0.31833577\n",
      "Iteration 328, loss = 0.31826862\n",
      "Iteration 329, loss = 0.31850992\n",
      "Iteration 330, loss = 0.31846881\n",
      "Iteration 331, loss = 0.31827806\n",
      "Iteration 332, loss = 0.31899909\n",
      "Iteration 333, loss = 0.31876667\n",
      "Iteration 334, loss = 0.31806209\n",
      "Iteration 335, loss = 0.31851377\n",
      "Iteration 336, loss = 0.31832674\n",
      "Iteration 337, loss = 0.31731182\n",
      "Iteration 338, loss = 0.31843593\n",
      "Iteration 339, loss = 0.31785846\n",
      "Iteration 340, loss = 0.31744754\n",
      "Iteration 341, loss = 0.31727375\n",
      "Iteration 342, loss = 0.31684643\n",
      "Iteration 343, loss = 0.31793897\n",
      "Iteration 344, loss = 0.31667351\n",
      "Iteration 345, loss = 0.31742384\n",
      "Iteration 346, loss = 0.31848954\n",
      "Iteration 347, loss = 0.31681993\n",
      "Iteration 348, loss = 0.31655637\n",
      "Iteration 349, loss = 0.31698979\n",
      "Iteration 350, loss = 0.31617497\n",
      "Iteration 351, loss = 0.31656981\n",
      "Iteration 352, loss = 0.31682782\n",
      "Iteration 353, loss = 0.31683238\n",
      "Iteration 354, loss = 0.31714707\n",
      "Iteration 355, loss = 0.31562771\n",
      "Iteration 356, loss = 0.31646003\n",
      "Iteration 357, loss = 0.31607539\n",
      "Iteration 358, loss = 0.31718555\n",
      "Iteration 359, loss = 0.31606633\n",
      "Iteration 360, loss = 0.31679972\n",
      "Iteration 361, loss = 0.31634741\n",
      "Iteration 362, loss = 0.31538179\n",
      "Iteration 363, loss = 0.31625729\n",
      "Iteration 364, loss = 0.31653057\n",
      "Iteration 365, loss = 0.31595717\n",
      "Iteration 366, loss = 0.31765721\n",
      "Iteration 367, loss = 0.31611094\n",
      "Iteration 368, loss = 0.31590025\n",
      "Iteration 369, loss = 0.31579171\n",
      "Iteration 370, loss = 0.31507972\n",
      "Iteration 371, loss = 0.31539159\n",
      "Iteration 372, loss = 0.31477246\n",
      "Iteration 373, loss = 0.31473379\n",
      "Iteration 374, loss = 0.31527180\n",
      "Iteration 375, loss = 0.31493046\n",
      "Iteration 376, loss = 0.31559021\n",
      "Iteration 377, loss = 0.31405978\n",
      "Iteration 378, loss = 0.31463854\n",
      "Iteration 379, loss = 0.31477109\n",
      "Iteration 380, loss = 0.31483491\n",
      "Iteration 381, loss = 0.31629785\n",
      "Iteration 382, loss = 0.31480201\n",
      "Iteration 383, loss = 0.31469878\n",
      "Iteration 384, loss = 0.31370733\n",
      "Iteration 385, loss = 0.31463260\n",
      "Iteration 386, loss = 0.31410742\n",
      "Iteration 387, loss = 0.31489100\n",
      "Iteration 388, loss = 0.31370404\n",
      "Iteration 389, loss = 0.31370339\n",
      "Iteration 390, loss = 0.31381003\n",
      "Iteration 391, loss = 0.31397519\n",
      "Iteration 392, loss = 0.31355744\n",
      "Iteration 393, loss = 0.31428458\n",
      "Iteration 394, loss = 0.31484563\n",
      "Iteration 395, loss = 0.31299002\n",
      "Iteration 396, loss = 0.31369120\n",
      "Iteration 397, loss = 0.31427706\n",
      "Iteration 398, loss = 0.31383902\n",
      "Iteration 399, loss = 0.31336587\n",
      "Iteration 400, loss = 0.31339619\n",
      "Iteration 401, loss = 0.31221909\n",
      "Iteration 402, loss = 0.31358268\n",
      "Iteration 403, loss = 0.31305303\n",
      "Iteration 404, loss = 0.31191706\n",
      "Iteration 405, loss = 0.31265054\n",
      "Iteration 406, loss = 0.31190452\n",
      "Iteration 407, loss = 0.31259403\n",
      "Iteration 408, loss = 0.31354844\n",
      "Iteration 409, loss = 0.31368198\n",
      "Iteration 410, loss = 0.31262133\n",
      "Iteration 411, loss = 0.31264860\n",
      "Iteration 412, loss = 0.31339623\n",
      "Iteration 413, loss = 0.31401782\n",
      "Iteration 414, loss = 0.31082042\n",
      "Iteration 415, loss = 0.31205960\n",
      "Iteration 416, loss = 0.31232059\n",
      "Iteration 417, loss = 0.31249911\n",
      "Iteration 418, loss = 0.31198153\n",
      "Iteration 419, loss = 0.31146207\n",
      "Iteration 420, loss = 0.31288781\n",
      "Iteration 421, loss = 0.31235082\n",
      "Iteration 422, loss = 0.31160934\n",
      "Iteration 423, loss = 0.31195835\n",
      "Iteration 424, loss = 0.31122318\n",
      "Iteration 425, loss = 0.31182651\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.50565847\n",
      "Iteration 2, loss = 0.45788914\n",
      "Iteration 3, loss = 0.44865424\n",
      "Iteration 4, loss = 0.44330481\n",
      "Iteration 5, loss = 0.43908319\n",
      "Iteration 6, loss = 0.43512415\n",
      "Iteration 7, loss = 0.43120964\n",
      "Iteration 8, loss = 0.42791978\n",
      "Iteration 9, loss = 0.42425333\n",
      "Iteration 10, loss = 0.42159709\n",
      "Iteration 11, loss = 0.41974457\n",
      "Iteration 12, loss = 0.41626359\n",
      "Iteration 13, loss = 0.41409759\n",
      "Iteration 14, loss = 0.41160826\n",
      "Iteration 15, loss = 0.40991910\n",
      "Iteration 16, loss = 0.40743357\n",
      "Iteration 17, loss = 0.40600459\n",
      "Iteration 18, loss = 0.40407377\n",
      "Iteration 19, loss = 0.40213050\n",
      "Iteration 20, loss = 0.40083060\n",
      "Iteration 21, loss = 0.40016257\n",
      "Iteration 22, loss = 0.39853658\n",
      "Iteration 23, loss = 0.39675329\n",
      "Iteration 24, loss = 0.39536753\n",
      "Iteration 25, loss = 0.39409137\n",
      "Iteration 26, loss = 0.39279578\n",
      "Iteration 27, loss = 0.39184532\n",
      "Iteration 28, loss = 0.39091498\n",
      "Iteration 29, loss = 0.38962444\n",
      "Iteration 30, loss = 0.38951215\n",
      "Iteration 31, loss = 0.38855372\n",
      "Iteration 32, loss = 0.38678163\n",
      "Iteration 33, loss = 0.38634482\n",
      "Iteration 34, loss = 0.38492379\n",
      "Iteration 35, loss = 0.38439714\n",
      "Iteration 36, loss = 0.38310926\n",
      "Iteration 37, loss = 0.38268494\n",
      "Iteration 38, loss = 0.38164894\n",
      "Iteration 39, loss = 0.38115064\n",
      "Iteration 40, loss = 0.37976029\n",
      "Iteration 41, loss = 0.37899351\n",
      "Iteration 42, loss = 0.37831869\n",
      "Iteration 43, loss = 0.37738497\n",
      "Iteration 44, loss = 0.37697075\n",
      "Iteration 45, loss = 0.37567737\n",
      "Iteration 46, loss = 0.37497640\n",
      "Iteration 47, loss = 0.37451891\n",
      "Iteration 48, loss = 0.37321448\n",
      "Iteration 49, loss = 0.37299838\n",
      "Iteration 50, loss = 0.37232629\n",
      "Iteration 51, loss = 0.37185283\n",
      "Iteration 52, loss = 0.37053264\n",
      "Iteration 53, loss = 0.36994930\n",
      "Iteration 54, loss = 0.36931593\n",
      "Iteration 55, loss = 0.36952850\n",
      "Iteration 56, loss = 0.36832877\n",
      "Iteration 57, loss = 0.36723400\n",
      "Iteration 58, loss = 0.36653345\n",
      "Iteration 59, loss = 0.36597982\n",
      "Iteration 60, loss = 0.36609648\n",
      "Iteration 61, loss = 0.36486706\n",
      "Iteration 62, loss = 0.36426202\n",
      "Iteration 63, loss = 0.36332187\n",
      "Iteration 64, loss = 0.36326307\n",
      "Iteration 65, loss = 0.36254200\n",
      "Iteration 66, loss = 0.36185934\n",
      "Iteration 67, loss = 0.36166631\n",
      "Iteration 68, loss = 0.36077190\n",
      "Iteration 69, loss = 0.36036100\n",
      "Iteration 70, loss = 0.36003272\n",
      "Iteration 71, loss = 0.35894361\n",
      "Iteration 72, loss = 0.35943600\n",
      "Iteration 73, loss = 0.35842894\n",
      "Iteration 74, loss = 0.35787683\n",
      "Iteration 75, loss = 0.35708204\n",
      "Iteration 76, loss = 0.35736366\n",
      "Iteration 77, loss = 0.35662481\n",
      "Iteration 78, loss = 0.35623366\n",
      "Iteration 79, loss = 0.35501922\n",
      "Iteration 80, loss = 0.35477705\n",
      "Iteration 81, loss = 0.35446262\n",
      "Iteration 82, loss = 0.35336038\n",
      "Iteration 83, loss = 0.35391103\n",
      "Iteration 84, loss = 0.35257281\n",
      "Iteration 85, loss = 0.35169622\n",
      "Iteration 86, loss = 0.35185886\n",
      "Iteration 87, loss = 0.35193891\n",
      "Iteration 88, loss = 0.35107431\n",
      "Iteration 89, loss = 0.35130720\n",
      "Iteration 90, loss = 0.35046577\n",
      "Iteration 91, loss = 0.34948247\n",
      "Iteration 92, loss = 0.34928597\n",
      "Iteration 93, loss = 0.34965122\n",
      "Iteration 94, loss = 0.34878241\n",
      "Iteration 95, loss = 0.34845085\n",
      "Iteration 96, loss = 0.34899237\n",
      "Iteration 97, loss = 0.34775918\n",
      "Iteration 98, loss = 0.34713202\n",
      "Iteration 99, loss = 0.34714217\n",
      "Iteration 100, loss = 0.34699988\n",
      "Iteration 101, loss = 0.34581296\n",
      "Iteration 102, loss = 0.34671150\n",
      "Iteration 103, loss = 0.34592919\n",
      "Iteration 104, loss = 0.34546035\n",
      "Iteration 105, loss = 0.34553908\n",
      "Iteration 106, loss = 0.34448837\n",
      "Iteration 107, loss = 0.34482948\n",
      "Iteration 108, loss = 0.34384070\n",
      "Iteration 109, loss = 0.34403896\n",
      "Iteration 110, loss = 0.34361063\n",
      "Iteration 111, loss = 0.34393085\n",
      "Iteration 112, loss = 0.34391002\n",
      "Iteration 113, loss = 0.34257250\n",
      "Iteration 114, loss = 0.34306719\n",
      "Iteration 115, loss = 0.34224255\n",
      "Iteration 116, loss = 0.34264993\n",
      "Iteration 117, loss = 0.34159122\n",
      "Iteration 118, loss = 0.34111745\n",
      "Iteration 119, loss = 0.34067129\n",
      "Iteration 120, loss = 0.34140837\n",
      "Iteration 121, loss = 0.34081871\n",
      "Iteration 122, loss = 0.33995614\n",
      "Iteration 123, loss = 0.34046401\n",
      "Iteration 124, loss = 0.33968584\n",
      "Iteration 125, loss = 0.33955846\n",
      "Iteration 126, loss = 0.33985813\n",
      "Iteration 127, loss = 0.33845794\n",
      "Iteration 128, loss = 0.33852108\n",
      "Iteration 129, loss = 0.33825461\n",
      "Iteration 130, loss = 0.33779590\n",
      "Iteration 131, loss = 0.33795888\n",
      "Iteration 132, loss = 0.33686977\n",
      "Iteration 133, loss = 0.33825971\n",
      "Iteration 134, loss = 0.33686049\n",
      "Iteration 135, loss = 0.33717305\n",
      "Iteration 136, loss = 0.33751209\n",
      "Iteration 137, loss = 0.33544718\n",
      "Iteration 138, loss = 0.33544337\n",
      "Iteration 139, loss = 0.33555922\n",
      "Iteration 140, loss = 0.33578797\n",
      "Iteration 141, loss = 0.33558534\n",
      "Iteration 142, loss = 0.33428253\n",
      "Iteration 143, loss = 0.33492419\n",
      "Iteration 144, loss = 0.33373703\n",
      "Iteration 145, loss = 0.33459576\n",
      "Iteration 146, loss = 0.33388429\n",
      "Iteration 147, loss = 0.33452233\n",
      "Iteration 148, loss = 0.33255616\n",
      "Iteration 149, loss = 0.33301322\n",
      "Iteration 150, loss = 0.33313409\n",
      "Iteration 151, loss = 0.33242809\n",
      "Iteration 152, loss = 0.33298295\n",
      "Iteration 153, loss = 0.33245346\n",
      "Iteration 154, loss = 0.33208447\n",
      "Iteration 155, loss = 0.33215498\n",
      "Iteration 156, loss = 0.33224054\n",
      "Iteration 157, loss = 0.33207629\n",
      "Iteration 158, loss = 0.33183375\n",
      "Iteration 159, loss = 0.33069470\n",
      "Iteration 160, loss = 0.33064198\n",
      "Iteration 161, loss = 0.33019125\n",
      "Iteration 162, loss = 0.33055757\n",
      "Iteration 163, loss = 0.33162669\n",
      "Iteration 164, loss = 0.33080538\n",
      "Iteration 165, loss = 0.32965265\n",
      "Iteration 166, loss = 0.32975372\n",
      "Iteration 167, loss = 0.33008077\n",
      "Iteration 168, loss = 0.32906401\n",
      "Iteration 169, loss = 0.33016940\n",
      "Iteration 170, loss = 0.32895928\n",
      "Iteration 171, loss = 0.32957316\n",
      "Iteration 172, loss = 0.32884059\n",
      "Iteration 173, loss = 0.32928240\n",
      "Iteration 174, loss = 0.32891735\n",
      "Iteration 175, loss = 0.32798210\n",
      "Iteration 176, loss = 0.32779296\n",
      "Iteration 177, loss = 0.32899410\n",
      "Iteration 178, loss = 0.32832566\n",
      "Iteration 179, loss = 0.32707516\n",
      "Iteration 180, loss = 0.32754981\n",
      "Iteration 181, loss = 0.32771099\n",
      "Iteration 182, loss = 0.32681352\n",
      "Iteration 183, loss = 0.32659092\n",
      "Iteration 184, loss = 0.32654671\n",
      "Iteration 185, loss = 0.32695432\n",
      "Iteration 186, loss = 0.32623136\n",
      "Iteration 187, loss = 0.32635187\n",
      "Iteration 188, loss = 0.32572877\n",
      "Iteration 189, loss = 0.32547626\n",
      "Iteration 190, loss = 0.32647664\n",
      "Iteration 191, loss = 0.32578182\n",
      "Iteration 192, loss = 0.32578661\n",
      "Iteration 193, loss = 0.32529548\n",
      "Iteration 194, loss = 0.32489076\n",
      "Iteration 195, loss = 0.32517132\n",
      "Iteration 196, loss = 0.32443069\n",
      "Iteration 197, loss = 0.32510417\n",
      "Iteration 198, loss = 0.32438488\n",
      "Iteration 199, loss = 0.32409711\n",
      "Iteration 200, loss = 0.32416546\n",
      "Iteration 201, loss = 0.32351555\n",
      "Iteration 202, loss = 0.32406886\n",
      "Iteration 203, loss = 0.32347087\n",
      "Iteration 204, loss = 0.32446424\n",
      "Iteration 205, loss = 0.32361619\n",
      "Iteration 206, loss = 0.32289297\n",
      "Iteration 207, loss = 0.32386164\n",
      "Iteration 208, loss = 0.32341122\n",
      "Iteration 209, loss = 0.32266412\n",
      "Iteration 210, loss = 0.32261487\n",
      "Iteration 211, loss = 0.32330963\n",
      "Iteration 212, loss = 0.32255464\n",
      "Iteration 213, loss = 0.32264720\n",
      "Iteration 214, loss = 0.32299443\n",
      "Iteration 215, loss = 0.32322424\n",
      "Iteration 216, loss = 0.32152460\n",
      "Iteration 217, loss = 0.32162956\n",
      "Iteration 218, loss = 0.32199154\n",
      "Iteration 219, loss = 0.32173882\n",
      "Iteration 220, loss = 0.32178258\n",
      "Iteration 221, loss = 0.32142925\n",
      "Iteration 222, loss = 0.32170015\n",
      "Iteration 223, loss = 0.32076708\n",
      "Iteration 224, loss = 0.32129413\n",
      "Iteration 225, loss = 0.32063243\n",
      "Iteration 226, loss = 0.32090065\n",
      "Iteration 227, loss = 0.32088536\n",
      "Iteration 228, loss = 0.32127756\n",
      "Iteration 229, loss = 0.32088978\n",
      "Iteration 230, loss = 0.32139810\n",
      "Iteration 231, loss = 0.32009844\n",
      "Iteration 232, loss = 0.32079033\n",
      "Iteration 233, loss = 0.32047613\n",
      "Iteration 234, loss = 0.32122197\n",
      "Iteration 235, loss = 0.31956938\n",
      "Iteration 236, loss = 0.31990705\n",
      "Iteration 237, loss = 0.31913759\n",
      "Iteration 238, loss = 0.31954126\n",
      "Iteration 239, loss = 0.32014194\n",
      "Iteration 240, loss = 0.31924958\n",
      "Iteration 241, loss = 0.31931064\n",
      "Iteration 242, loss = 0.31862225\n",
      "Iteration 243, loss = 0.31895072\n",
      "Iteration 244, loss = 0.31908841\n",
      "Iteration 245, loss = 0.31860499\n",
      "Iteration 246, loss = 0.31845537\n",
      "Iteration 247, loss = 0.31793261\n",
      "Iteration 248, loss = 0.31816925\n",
      "Iteration 249, loss = 0.31962718\n",
      "Iteration 250, loss = 0.31822384\n",
      "Iteration 251, loss = 0.31778491\n",
      "Iteration 252, loss = 0.31800725\n",
      "Iteration 253, loss = 0.31836173\n",
      "Iteration 254, loss = 0.31772067\n",
      "Iteration 255, loss = 0.31741939\n",
      "Iteration 256, loss = 0.31813138\n",
      "Iteration 257, loss = 0.31696837\n",
      "Iteration 258, loss = 0.31738473\n",
      "Iteration 259, loss = 0.31719285\n",
      "Iteration 260, loss = 0.31635865\n",
      "Iteration 261, loss = 0.31726744\n",
      "Iteration 262, loss = 0.31590666\n",
      "Iteration 263, loss = 0.31649268\n",
      "Iteration 264, loss = 0.31665875\n",
      "Iteration 265, loss = 0.31619681\n",
      "Iteration 266, loss = 0.31546068\n",
      "Iteration 267, loss = 0.31576658\n",
      "Iteration 268, loss = 0.31560790\n",
      "Iteration 269, loss = 0.31604735\n",
      "Iteration 270, loss = 0.31560247\n",
      "Iteration 271, loss = 0.31619412\n",
      "Iteration 272, loss = 0.31721573\n",
      "Iteration 273, loss = 0.31462058\n",
      "Iteration 274, loss = 0.31537851\n",
      "Iteration 275, loss = 0.31471010\n",
      "Iteration 276, loss = 0.31522927\n",
      "Iteration 277, loss = 0.31486221\n",
      "Iteration 278, loss = 0.31588838\n",
      "Iteration 279, loss = 0.31510946\n",
      "Iteration 280, loss = 0.31598101\n",
      "Iteration 281, loss = 0.31527126\n",
      "Iteration 282, loss = 0.31478115\n",
      "Iteration 283, loss = 0.31531512\n",
      "Iteration 284, loss = 0.31422032\n",
      "Iteration 285, loss = 0.31439899\n",
      "Iteration 286, loss = 0.31434221\n",
      "Iteration 287, loss = 0.31553340\n",
      "Iteration 288, loss = 0.31323906\n",
      "Iteration 289, loss = 0.31396556\n",
      "Iteration 290, loss = 0.31466291\n",
      "Iteration 291, loss = 0.31500293\n",
      "Iteration 292, loss = 0.31361795\n",
      "Iteration 293, loss = 0.31249969\n",
      "Iteration 294, loss = 0.31341942\n",
      "Iteration 295, loss = 0.31410949\n",
      "Iteration 296, loss = 0.31384623\n",
      "Iteration 297, loss = 0.31404823\n",
      "Iteration 298, loss = 0.31245986\n",
      "Iteration 299, loss = 0.31375507\n",
      "Iteration 300, loss = 0.31271030\n",
      "Iteration 301, loss = 0.31274537\n",
      "Iteration 302, loss = 0.31292083\n",
      "Iteration 303, loss = 0.31287304\n",
      "Iteration 304, loss = 0.31342418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51800258\n",
      "Iteration 2, loss = 0.46913489\n",
      "Iteration 3, loss = 0.46061221\n",
      "Iteration 4, loss = 0.45581784\n",
      "Iteration 5, loss = 0.45189743\n",
      "Iteration 6, loss = 0.44814268\n",
      "Iteration 7, loss = 0.44454358\n",
      "Iteration 8, loss = 0.44192210\n",
      "Iteration 9, loss = 0.43885141\n",
      "Iteration 10, loss = 0.43602606\n",
      "Iteration 11, loss = 0.43379450\n",
      "Iteration 12, loss = 0.43139050\n",
      "Iteration 13, loss = 0.42910292\n",
      "Iteration 14, loss = 0.42704230\n",
      "Iteration 15, loss = 0.42432411\n",
      "Iteration 16, loss = 0.42239439\n",
      "Iteration 17, loss = 0.42057495\n",
      "Iteration 18, loss = 0.41882435\n",
      "Iteration 19, loss = 0.41680252\n",
      "Iteration 20, loss = 0.41490069\n",
      "Iteration 21, loss = 0.41303416\n",
      "Iteration 22, loss = 0.41169349\n",
      "Iteration 23, loss = 0.40967735\n",
      "Iteration 24, loss = 0.40865983\n",
      "Iteration 25, loss = 0.40691568\n",
      "Iteration 26, loss = 0.40577037\n",
      "Iteration 27, loss = 0.40504665\n",
      "Iteration 28, loss = 0.40315120\n",
      "Iteration 29, loss = 0.40209151\n",
      "Iteration 30, loss = 0.40096335\n",
      "Iteration 31, loss = 0.40100797\n",
      "Iteration 32, loss = 0.39903527\n",
      "Iteration 33, loss = 0.39833332\n",
      "Iteration 34, loss = 0.39709754\n",
      "Iteration 35, loss = 0.39630596\n",
      "Iteration 36, loss = 0.39460727\n",
      "Iteration 37, loss = 0.39396240\n",
      "Iteration 38, loss = 0.39311216\n",
      "Iteration 39, loss = 0.39189833\n",
      "Iteration 40, loss = 0.39080385\n",
      "Iteration 41, loss = 0.39052150\n",
      "Iteration 42, loss = 0.38940296\n",
      "Iteration 43, loss = 0.38863962\n",
      "Iteration 44, loss = 0.38756995\n",
      "Iteration 45, loss = 0.38654173\n",
      "Iteration 46, loss = 0.38650393\n",
      "Iteration 47, loss = 0.38570130\n",
      "Iteration 48, loss = 0.38450707\n",
      "Iteration 49, loss = 0.38432022\n",
      "Iteration 50, loss = 0.38354680\n",
      "Iteration 51, loss = 0.38284118\n",
      "Iteration 52, loss = 0.38188698\n",
      "Iteration 53, loss = 0.38130035\n",
      "Iteration 54, loss = 0.38059768\n",
      "Iteration 55, loss = 0.37979180\n",
      "Iteration 56, loss = 0.37999374\n",
      "Iteration 57, loss = 0.37884865\n",
      "Iteration 58, loss = 0.37846133\n",
      "Iteration 59, loss = 0.37850715\n",
      "Iteration 60, loss = 0.37642900\n",
      "Iteration 61, loss = 0.37615577\n",
      "Iteration 62, loss = 0.37635297\n",
      "Iteration 63, loss = 0.37584868\n",
      "Iteration 64, loss = 0.37453654\n",
      "Iteration 65, loss = 0.37468605\n",
      "Iteration 66, loss = 0.37468400\n",
      "Iteration 67, loss = 0.37334028\n",
      "Iteration 68, loss = 0.37291868\n",
      "Iteration 69, loss = 0.37194502\n",
      "Iteration 70, loss = 0.37172134\n",
      "Iteration 71, loss = 0.37078516\n",
      "Iteration 72, loss = 0.37064145\n",
      "Iteration 73, loss = 0.36996563\n",
      "Iteration 74, loss = 0.37008516\n",
      "Iteration 75, loss = 0.37001137\n",
      "Iteration 76, loss = 0.36921663\n",
      "Iteration 77, loss = 0.36889565\n",
      "Iteration 78, loss = 0.36811018\n",
      "Iteration 79, loss = 0.36689157\n",
      "Iteration 80, loss = 0.36632496\n",
      "Iteration 81, loss = 0.36643170\n",
      "Iteration 82, loss = 0.36613737\n",
      "Iteration 83, loss = 0.36457395\n",
      "Iteration 84, loss = 0.36534350\n",
      "Iteration 85, loss = 0.36532179\n",
      "Iteration 86, loss = 0.36451172\n",
      "Iteration 87, loss = 0.36417934\n",
      "Iteration 88, loss = 0.36279952\n",
      "Iteration 89, loss = 0.36276318\n",
      "Iteration 90, loss = 0.36263310\n",
      "Iteration 91, loss = 0.36186716\n",
      "Iteration 92, loss = 0.36241953\n",
      "Iteration 93, loss = 0.36159630\n",
      "Iteration 94, loss = 0.36101724\n",
      "Iteration 95, loss = 0.36054774\n",
      "Iteration 96, loss = 0.36003663\n",
      "Iteration 97, loss = 0.35977391\n",
      "Iteration 98, loss = 0.35896631\n",
      "Iteration 99, loss = 0.35890647\n",
      "Iteration 100, loss = 0.35923112\n",
      "Iteration 101, loss = 0.35809198\n",
      "Iteration 102, loss = 0.35718293\n",
      "Iteration 103, loss = 0.35742719\n",
      "Iteration 104, loss = 0.35684585\n",
      "Iteration 105, loss = 0.35747093\n",
      "Iteration 106, loss = 0.35684193\n",
      "Iteration 107, loss = 0.35685450\n",
      "Iteration 108, loss = 0.35625623\n",
      "Iteration 109, loss = 0.35558153\n",
      "Iteration 110, loss = 0.35489075\n",
      "Iteration 111, loss = 0.35458200\n",
      "Iteration 112, loss = 0.35435503\n",
      "Iteration 113, loss = 0.35387525\n",
      "Iteration 114, loss = 0.35428851\n",
      "Iteration 115, loss = 0.35370232\n",
      "Iteration 116, loss = 0.35224930\n",
      "Iteration 117, loss = 0.35281621\n",
      "Iteration 118, loss = 0.35226001\n",
      "Iteration 119, loss = 0.35246929\n",
      "Iteration 120, loss = 0.35166057\n",
      "Iteration 121, loss = 0.35157298\n",
      "Iteration 122, loss = 0.35204745\n",
      "Iteration 123, loss = 0.35136148\n",
      "Iteration 124, loss = 0.35099696\n",
      "Iteration 125, loss = 0.35073106\n",
      "Iteration 126, loss = 0.34982381\n",
      "Iteration 127, loss = 0.34998059\n",
      "Iteration 128, loss = 0.34974892\n",
      "Iteration 129, loss = 0.34942029\n",
      "Iteration 130, loss = 0.34886227\n",
      "Iteration 131, loss = 0.34912686\n",
      "Iteration 132, loss = 0.34890194\n",
      "Iteration 133, loss = 0.34882998\n",
      "Iteration 134, loss = 0.34811620\n",
      "Iteration 135, loss = 0.34713353\n",
      "Iteration 136, loss = 0.34849555\n",
      "Iteration 137, loss = 0.34726547\n",
      "Iteration 138, loss = 0.34747163\n",
      "Iteration 139, loss = 0.34689041\n",
      "Iteration 140, loss = 0.34650529\n",
      "Iteration 141, loss = 0.34695577\n",
      "Iteration 142, loss = 0.34576972\n",
      "Iteration 143, loss = 0.34593084\n",
      "Iteration 144, loss = 0.34578801\n",
      "Iteration 145, loss = 0.34506440\n",
      "Iteration 146, loss = 0.34464366\n",
      "Iteration 147, loss = 0.34484551\n",
      "Iteration 148, loss = 0.34468171\n",
      "Iteration 149, loss = 0.34499250\n",
      "Iteration 150, loss = 0.34403023\n",
      "Iteration 151, loss = 0.34420926\n",
      "Iteration 152, loss = 0.34391761\n",
      "Iteration 153, loss = 0.34387682\n",
      "Iteration 154, loss = 0.34276150\n",
      "Iteration 155, loss = 0.34389922\n",
      "Iteration 156, loss = 0.34224149\n",
      "Iteration 157, loss = 0.34300273\n",
      "Iteration 158, loss = 0.34259608\n",
      "Iteration 159, loss = 0.34248039\n",
      "Iteration 160, loss = 0.34236806\n",
      "Iteration 161, loss = 0.34334261\n",
      "Iteration 162, loss = 0.34234017\n",
      "Iteration 163, loss = 0.34138988\n",
      "Iteration 164, loss = 0.34179287\n",
      "Iteration 165, loss = 0.34120911\n",
      "Iteration 166, loss = 0.34039840\n",
      "Iteration 167, loss = 0.34073507\n",
      "Iteration 168, loss = 0.34012574\n",
      "Iteration 169, loss = 0.34028061\n",
      "Iteration 170, loss = 0.34093570\n",
      "Iteration 171, loss = 0.33963051\n",
      "Iteration 172, loss = 0.33977331\n",
      "Iteration 173, loss = 0.33929015\n",
      "Iteration 174, loss = 0.33928150\n",
      "Iteration 175, loss = 0.33974397\n",
      "Iteration 176, loss = 0.34024667\n",
      "Iteration 177, loss = 0.34080778\n",
      "Iteration 178, loss = 0.33909733\n",
      "Iteration 179, loss = 0.33856191\n",
      "Iteration 180, loss = 0.33950705\n",
      "Iteration 181, loss = 0.33892695\n",
      "Iteration 182, loss = 0.33768840\n",
      "Iteration 183, loss = 0.33727643\n",
      "Iteration 184, loss = 0.33791757\n",
      "Iteration 185, loss = 0.33741742\n",
      "Iteration 186, loss = 0.33772017\n",
      "Iteration 187, loss = 0.33703731\n",
      "Iteration 188, loss = 0.33695583\n",
      "Iteration 189, loss = 0.33738895\n",
      "Iteration 190, loss = 0.33706691\n",
      "Iteration 191, loss = 0.33696461\n",
      "Iteration 192, loss = 0.33590813\n",
      "Iteration 193, loss = 0.33669179\n",
      "Iteration 194, loss = 0.33537218\n",
      "Iteration 195, loss = 0.33636820\n",
      "Iteration 196, loss = 0.33550519\n",
      "Iteration 197, loss = 0.33575350\n",
      "Iteration 198, loss = 0.33463882\n",
      "Iteration 199, loss = 0.33461685\n",
      "Iteration 200, loss = 0.33410985\n",
      "Iteration 201, loss = 0.33459283\n",
      "Iteration 202, loss = 0.33463859\n",
      "Iteration 203, loss = 0.33410633\n",
      "Iteration 204, loss = 0.33531182\n",
      "Iteration 205, loss = 0.33450295\n",
      "Iteration 206, loss = 0.33380647\n",
      "Iteration 207, loss = 0.33285622\n",
      "Iteration 208, loss = 0.33314131\n",
      "Iteration 209, loss = 0.33389162\n",
      "Iteration 210, loss = 0.33337578\n",
      "Iteration 211, loss = 0.33417998\n",
      "Iteration 212, loss = 0.33324194\n",
      "Iteration 213, loss = 0.33270328\n",
      "Iteration 214, loss = 0.33254445\n",
      "Iteration 215, loss = 0.33301710\n",
      "Iteration 216, loss = 0.33293602\n",
      "Iteration 217, loss = 0.33200559\n",
      "Iteration 218, loss = 0.33176548\n",
      "Iteration 219, loss = 0.33280512\n",
      "Iteration 220, loss = 0.33142333\n",
      "Iteration 221, loss = 0.33117884\n",
      "Iteration 222, loss = 0.33111607\n",
      "Iteration 223, loss = 0.33112384\n",
      "Iteration 224, loss = 0.33132501\n",
      "Iteration 225, loss = 0.33144953\n",
      "Iteration 226, loss = 0.33103714\n",
      "Iteration 227, loss = 0.33021859\n",
      "Iteration 228, loss = 0.33086363\n",
      "Iteration 229, loss = 0.32993254\n",
      "Iteration 230, loss = 0.33062784\n",
      "Iteration 231, loss = 0.33065804\n",
      "Iteration 232, loss = 0.33095886\n",
      "Iteration 233, loss = 0.32986972\n",
      "Iteration 234, loss = 0.32941315\n",
      "Iteration 235, loss = 0.32997357\n",
      "Iteration 236, loss = 0.32971177\n",
      "Iteration 237, loss = 0.32921999\n",
      "Iteration 238, loss = 0.32861207\n",
      "Iteration 239, loss = 0.32891222\n",
      "Iteration 240, loss = 0.32891395\n",
      "Iteration 241, loss = 0.32899835\n",
      "Iteration 242, loss = 0.32944419\n",
      "Iteration 243, loss = 0.32909666\n",
      "Iteration 244, loss = 0.32770602\n",
      "Iteration 245, loss = 0.32843745\n",
      "Iteration 246, loss = 0.32719877\n",
      "Iteration 247, loss = 0.32795091\n",
      "Iteration 248, loss = 0.32829313\n",
      "Iteration 249, loss = 0.32819122\n",
      "Iteration 250, loss = 0.32763938\n",
      "Iteration 251, loss = 0.32903972\n",
      "Iteration 252, loss = 0.32761440\n",
      "Iteration 253, loss = 0.32697810\n",
      "Iteration 254, loss = 0.32784622\n",
      "Iteration 255, loss = 0.32730591\n",
      "Iteration 256, loss = 0.32738912\n",
      "Iteration 257, loss = 0.32689363\n",
      "Iteration 258, loss = 0.32755232\n",
      "Iteration 259, loss = 0.32727829\n",
      "Iteration 260, loss = 0.32722440\n",
      "Iteration 261, loss = 0.32593493\n",
      "Iteration 262, loss = 0.32648168\n",
      "Iteration 263, loss = 0.32623239\n",
      "Iteration 264, loss = 0.32621853\n",
      "Iteration 265, loss = 0.32590703\n",
      "Iteration 266, loss = 0.32680312\n",
      "Iteration 267, loss = 0.32530366\n",
      "Iteration 268, loss = 0.32659711\n",
      "Iteration 269, loss = 0.32588578\n",
      "Iteration 270, loss = 0.32566537\n",
      "Iteration 271, loss = 0.32499020\n",
      "Iteration 272, loss = 0.32603974\n",
      "Iteration 273, loss = 0.32582661\n",
      "Iteration 274, loss = 0.32611097\n",
      "Iteration 275, loss = 0.32585553\n",
      "Iteration 276, loss = 0.32547501\n",
      "Iteration 277, loss = 0.32449780\n",
      "Iteration 278, loss = 0.32479387\n",
      "Iteration 279, loss = 0.32363134\n",
      "Iteration 280, loss = 0.32700249\n",
      "Iteration 281, loss = 0.32441058\n",
      "Iteration 282, loss = 0.32439458\n",
      "Iteration 283, loss = 0.32486067\n",
      "Iteration 284, loss = 0.32537405\n",
      "Iteration 285, loss = 0.32412236\n",
      "Iteration 286, loss = 0.32457590\n",
      "Iteration 287, loss = 0.32482823\n",
      "Iteration 288, loss = 0.32377000\n",
      "Iteration 289, loss = 0.32466957\n",
      "Iteration 290, loss = 0.32345195\n",
      "Iteration 291, loss = 0.32399879\n",
      "Iteration 292, loss = 0.32382383\n",
      "Iteration 293, loss = 0.32421330\n",
      "Iteration 294, loss = 0.32339838\n",
      "Iteration 295, loss = 0.32310593\n",
      "Iteration 296, loss = 0.32356223\n",
      "Iteration 297, loss = 0.32344791\n",
      "Iteration 298, loss = 0.32286710\n",
      "Iteration 299, loss = 0.32318961\n",
      "Iteration 300, loss = 0.32223404\n",
      "Iteration 301, loss = 0.32281962\n",
      "Iteration 302, loss = 0.32300824\n",
      "Iteration 303, loss = 0.32279025\n",
      "Iteration 304, loss = 0.32355623\n",
      "Iteration 305, loss = 0.32230283\n",
      "Iteration 306, loss = 0.32167469\n",
      "Iteration 307, loss = 0.32185000\n",
      "Iteration 308, loss = 0.32136351\n",
      "Iteration 309, loss = 0.32172742\n",
      "Iteration 310, loss = 0.32234214\n",
      "Iteration 311, loss = 0.32210105\n",
      "Iteration 312, loss = 0.32123711\n",
      "Iteration 313, loss = 0.32128540\n",
      "Iteration 314, loss = 0.32161612\n",
      "Iteration 315, loss = 0.32098530\n",
      "Iteration 316, loss = 0.32112889\n",
      "Iteration 317, loss = 0.32037477\n",
      "Iteration 318, loss = 0.32101047\n",
      "Iteration 319, loss = 0.32085012\n",
      "Iteration 320, loss = 0.32168443\n",
      "Iteration 321, loss = 0.32122669\n",
      "Iteration 322, loss = 0.32043681\n",
      "Iteration 323, loss = 0.32142131\n",
      "Iteration 324, loss = 0.32126583\n",
      "Iteration 325, loss = 0.32056086\n",
      "Iteration 326, loss = 0.32086226\n",
      "Iteration 327, loss = 0.32095261\n",
      "Iteration 328, loss = 0.32061115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51737615\n",
      "Iteration 2, loss = 0.47048914\n",
      "Iteration 3, loss = 0.46169104\n",
      "Iteration 4, loss = 0.45657830\n",
      "Iteration 5, loss = 0.45251779\n",
      "Iteration 6, loss = 0.44857934\n",
      "Iteration 7, loss = 0.44585675\n",
      "Iteration 8, loss = 0.44195729\n",
      "Iteration 9, loss = 0.43910821\n",
      "Iteration 10, loss = 0.43624791\n",
      "Iteration 11, loss = 0.43390343\n",
      "Iteration 12, loss = 0.43100987\n",
      "Iteration 13, loss = 0.42903661\n",
      "Iteration 14, loss = 0.42700523\n",
      "Iteration 15, loss = 0.42461494\n",
      "Iteration 16, loss = 0.42298718\n",
      "Iteration 17, loss = 0.42141002\n",
      "Iteration 18, loss = 0.41986094\n",
      "Iteration 19, loss = 0.41783057\n",
      "Iteration 20, loss = 0.41658847\n",
      "Iteration 21, loss = 0.41484497\n",
      "Iteration 22, loss = 0.41372383\n",
      "Iteration 23, loss = 0.41253740\n",
      "Iteration 24, loss = 0.41184676\n",
      "Iteration 25, loss = 0.41005614\n",
      "Iteration 26, loss = 0.40892994\n",
      "Iteration 27, loss = 0.40734733\n",
      "Iteration 28, loss = 0.40694772\n",
      "Iteration 29, loss = 0.40533227\n",
      "Iteration 30, loss = 0.40454379\n",
      "Iteration 31, loss = 0.40335003\n",
      "Iteration 32, loss = 0.40221983\n",
      "Iteration 33, loss = 0.40100630\n",
      "Iteration 34, loss = 0.40089561\n",
      "Iteration 35, loss = 0.39901745\n",
      "Iteration 36, loss = 0.39829733\n",
      "Iteration 37, loss = 0.39739668\n",
      "Iteration 38, loss = 0.39641211\n",
      "Iteration 39, loss = 0.39551219\n",
      "Iteration 40, loss = 0.39491718\n",
      "Iteration 41, loss = 0.39373347\n",
      "Iteration 42, loss = 0.39340641\n",
      "Iteration 43, loss = 0.39281258\n",
      "Iteration 44, loss = 0.39151099\n",
      "Iteration 45, loss = 0.39095400\n",
      "Iteration 46, loss = 0.38983549\n",
      "Iteration 47, loss = 0.38971833\n",
      "Iteration 48, loss = 0.38837110\n",
      "Iteration 49, loss = 0.38742138\n",
      "Iteration 50, loss = 0.38715078\n",
      "Iteration 51, loss = 0.38638239\n",
      "Iteration 52, loss = 0.38566195\n",
      "Iteration 53, loss = 0.38515393\n",
      "Iteration 54, loss = 0.38456146\n",
      "Iteration 55, loss = 0.38393389\n",
      "Iteration 56, loss = 0.38350762\n",
      "Iteration 57, loss = 0.38316891\n",
      "Iteration 58, loss = 0.38191296\n",
      "Iteration 59, loss = 0.38170582\n",
      "Iteration 60, loss = 0.38109951\n",
      "Iteration 61, loss = 0.37990923\n",
      "Iteration 62, loss = 0.38137175\n",
      "Iteration 63, loss = 0.37911429\n",
      "Iteration 64, loss = 0.37916641\n",
      "Iteration 65, loss = 0.37846900\n",
      "Iteration 66, loss = 0.37747885\n",
      "Iteration 67, loss = 0.37695200\n",
      "Iteration 68, loss = 0.37661677\n",
      "Iteration 69, loss = 0.37628551\n",
      "Iteration 70, loss = 0.37582466\n",
      "Iteration 71, loss = 0.37493200\n",
      "Iteration 72, loss = 0.37439191\n",
      "Iteration 73, loss = 0.37361114\n",
      "Iteration 74, loss = 0.37350357\n",
      "Iteration 75, loss = 0.37298268\n",
      "Iteration 76, loss = 0.37200994\n",
      "Iteration 77, loss = 0.37328594\n",
      "Iteration 78, loss = 0.37197460\n",
      "Iteration 79, loss = 0.37192981\n",
      "Iteration 80, loss = 0.37077521\n",
      "Iteration 81, loss = 0.37010067\n",
      "Iteration 82, loss = 0.36967120\n",
      "Iteration 83, loss = 0.36851190\n",
      "Iteration 84, loss = 0.36910673\n",
      "Iteration 85, loss = 0.36793877\n",
      "Iteration 86, loss = 0.36821732\n",
      "Iteration 87, loss = 0.36824863\n",
      "Iteration 88, loss = 0.36700764\n",
      "Iteration 89, loss = 0.36690986\n",
      "Iteration 90, loss = 0.36679373\n",
      "Iteration 91, loss = 0.36604303\n",
      "Iteration 92, loss = 0.36587262\n",
      "Iteration 93, loss = 0.36524149\n",
      "Iteration 94, loss = 0.36490144\n",
      "Iteration 95, loss = 0.36443269\n",
      "Iteration 96, loss = 0.36349801\n",
      "Iteration 97, loss = 0.36375142\n",
      "Iteration 98, loss = 0.36314714\n",
      "Iteration 99, loss = 0.36332990\n",
      "Iteration 100, loss = 0.36325270\n",
      "Iteration 101, loss = 0.36282067\n",
      "Iteration 102, loss = 0.36098419\n",
      "Iteration 103, loss = 0.36166059\n",
      "Iteration 104, loss = 0.36148795\n",
      "Iteration 105, loss = 0.36049373\n",
      "Iteration 106, loss = 0.36004954\n",
      "Iteration 107, loss = 0.35963258\n",
      "Iteration 108, loss = 0.35915761\n",
      "Iteration 109, loss = 0.35917279\n",
      "Iteration 110, loss = 0.35889136\n",
      "Iteration 111, loss = 0.35855094\n",
      "Iteration 112, loss = 0.35803257\n",
      "Iteration 113, loss = 0.35788380\n",
      "Iteration 114, loss = 0.35804237\n",
      "Iteration 115, loss = 0.35838572\n",
      "Iteration 116, loss = 0.35729813\n",
      "Iteration 117, loss = 0.35769714\n",
      "Iteration 118, loss = 0.35685960\n",
      "Iteration 119, loss = 0.35678164\n",
      "Iteration 120, loss = 0.35534957\n",
      "Iteration 121, loss = 0.35646567\n",
      "Iteration 122, loss = 0.35546804\n",
      "Iteration 123, loss = 0.35626151\n",
      "Iteration 124, loss = 0.35576019\n",
      "Iteration 125, loss = 0.35533779\n",
      "Iteration 126, loss = 0.35427009\n",
      "Iteration 127, loss = 0.35394843\n",
      "Iteration 128, loss = 0.35374599\n",
      "Iteration 129, loss = 0.35414537\n",
      "Iteration 130, loss = 0.35358423\n",
      "Iteration 131, loss = 0.35351917\n",
      "Iteration 132, loss = 0.35198987\n",
      "Iteration 133, loss = 0.35293700\n",
      "Iteration 134, loss = 0.35232932\n",
      "Iteration 135, loss = 0.35184512\n",
      "Iteration 136, loss = 0.35173889\n",
      "Iteration 137, loss = 0.35147374\n",
      "Iteration 138, loss = 0.35178265\n",
      "Iteration 139, loss = 0.35123385\n",
      "Iteration 140, loss = 0.35134436\n",
      "Iteration 141, loss = 0.35033574\n",
      "Iteration 142, loss = 0.34991932\n",
      "Iteration 143, loss = 0.34933688\n",
      "Iteration 144, loss = 0.34937627\n",
      "Iteration 145, loss = 0.35039066\n",
      "Iteration 146, loss = 0.34977605\n",
      "Iteration 147, loss = 0.34992771\n",
      "Iteration 148, loss = 0.34890030\n",
      "Iteration 149, loss = 0.34914269\n",
      "Iteration 150, loss = 0.34816649\n",
      "Iteration 151, loss = 0.34892751\n",
      "Iteration 152, loss = 0.34833897\n",
      "Iteration 153, loss = 0.34894569\n",
      "Iteration 154, loss = 0.34726473\n",
      "Iteration 155, loss = 0.34837692\n",
      "Iteration 156, loss = 0.34705396\n",
      "Iteration 157, loss = 0.34723332\n",
      "Iteration 158, loss = 0.34736083\n",
      "Iteration 159, loss = 0.34585889\n",
      "Iteration 160, loss = 0.34734120\n",
      "Iteration 161, loss = 0.34623987\n",
      "Iteration 162, loss = 0.34605268\n",
      "Iteration 163, loss = 0.34627347\n",
      "Iteration 164, loss = 0.34588159\n",
      "Iteration 165, loss = 0.34539023\n",
      "Iteration 166, loss = 0.34514246\n",
      "Iteration 167, loss = 0.34538684\n",
      "Iteration 168, loss = 0.34465031\n",
      "Iteration 169, loss = 0.34512066\n",
      "Iteration 170, loss = 0.34362548\n",
      "Iteration 171, loss = 0.34362469\n",
      "Iteration 172, loss = 0.34348317\n",
      "Iteration 173, loss = 0.34378568\n",
      "Iteration 174, loss = 0.34324182\n",
      "Iteration 175, loss = 0.34347282\n",
      "Iteration 176, loss = 0.34302538\n",
      "Iteration 177, loss = 0.34300682\n",
      "Iteration 178, loss = 0.34187332\n",
      "Iteration 179, loss = 0.34234762\n",
      "Iteration 180, loss = 0.34226780\n",
      "Iteration 181, loss = 0.34223613\n",
      "Iteration 182, loss = 0.34229011\n",
      "Iteration 183, loss = 0.34139251\n",
      "Iteration 184, loss = 0.34191262\n",
      "Iteration 185, loss = 0.34145285\n",
      "Iteration 186, loss = 0.34122032\n",
      "Iteration 187, loss = 0.34089410\n",
      "Iteration 188, loss = 0.34092877\n",
      "Iteration 189, loss = 0.34055034\n",
      "Iteration 190, loss = 0.33983202\n",
      "Iteration 191, loss = 0.33957857\n",
      "Iteration 192, loss = 0.33951408\n",
      "Iteration 193, loss = 0.33913687\n",
      "Iteration 194, loss = 0.33995557\n",
      "Iteration 195, loss = 0.33989847\n",
      "Iteration 196, loss = 0.33997520\n",
      "Iteration 197, loss = 0.33878071\n",
      "Iteration 198, loss = 0.33918237\n",
      "Iteration 199, loss = 0.33915994\n",
      "Iteration 200, loss = 0.33929192\n",
      "Iteration 201, loss = 0.33834245\n",
      "Iteration 202, loss = 0.33795156\n",
      "Iteration 203, loss = 0.33817391\n",
      "Iteration 204, loss = 0.33812400\n",
      "Iteration 205, loss = 0.33912157\n",
      "Iteration 206, loss = 0.33791596\n",
      "Iteration 207, loss = 0.33820146\n",
      "Iteration 208, loss = 0.33724768\n",
      "Iteration 209, loss = 0.33834734\n",
      "Iteration 210, loss = 0.33766435\n",
      "Iteration 211, loss = 0.33667476\n",
      "Iteration 212, loss = 0.33740240\n",
      "Iteration 213, loss = 0.33800522\n",
      "Iteration 214, loss = 0.33646889\n",
      "Iteration 215, loss = 0.33664687\n",
      "Iteration 216, loss = 0.33600002\n",
      "Iteration 217, loss = 0.33616031\n",
      "Iteration 218, loss = 0.33648494\n",
      "Iteration 219, loss = 0.33604366\n",
      "Iteration 220, loss = 0.33564101\n",
      "Iteration 221, loss = 0.33518248\n",
      "Iteration 222, loss = 0.33534401\n",
      "Iteration 223, loss = 0.33546360\n",
      "Iteration 224, loss = 0.33606162\n",
      "Iteration 225, loss = 0.33550645\n",
      "Iteration 226, loss = 0.33529831\n",
      "Iteration 227, loss = 0.33511919\n",
      "Iteration 228, loss = 0.33480503\n",
      "Iteration 229, loss = 0.33538900\n",
      "Iteration 230, loss = 0.33454413\n",
      "Iteration 231, loss = 0.33413067\n",
      "Iteration 232, loss = 0.33444942\n",
      "Iteration 233, loss = 0.33488228\n",
      "Iteration 234, loss = 0.33487336\n",
      "Iteration 235, loss = 0.33440692\n",
      "Iteration 236, loss = 0.33502552\n",
      "Iteration 237, loss = 0.33316621\n",
      "Iteration 238, loss = 0.33301026\n",
      "Iteration 239, loss = 0.33391100\n",
      "Iteration 240, loss = 0.33338648\n",
      "Iteration 241, loss = 0.33273544\n",
      "Iteration 242, loss = 0.33303502\n",
      "Iteration 243, loss = 0.33304296\n",
      "Iteration 244, loss = 0.33318174\n",
      "Iteration 245, loss = 0.33333734\n",
      "Iteration 246, loss = 0.33219125\n",
      "Iteration 247, loss = 0.33258787\n",
      "Iteration 248, loss = 0.33312477\n",
      "Iteration 249, loss = 0.33230028\n",
      "Iteration 250, loss = 0.33150569\n",
      "Iteration 251, loss = 0.33272695\n",
      "Iteration 252, loss = 0.33330060\n",
      "Iteration 253, loss = 0.33142848\n",
      "Iteration 254, loss = 0.33140713\n",
      "Iteration 255, loss = 0.33192886\n",
      "Iteration 256, loss = 0.33288793\n",
      "Iteration 257, loss = 0.33199419\n",
      "Iteration 258, loss = 0.33055611\n",
      "Iteration 259, loss = 0.33145035\n",
      "Iteration 260, loss = 0.33005780\n",
      "Iteration 261, loss = 0.33098816\n",
      "Iteration 262, loss = 0.33062991\n",
      "Iteration 263, loss = 0.33166777\n",
      "Iteration 264, loss = 0.33095783\n",
      "Iteration 265, loss = 0.33026713\n",
      "Iteration 266, loss = 0.33192304\n",
      "Iteration 267, loss = 0.33014117\n",
      "Iteration 268, loss = 0.33044747\n",
      "Iteration 269, loss = 0.33082017\n",
      "Iteration 270, loss = 0.33037097\n",
      "Iteration 271, loss = 0.32984141\n",
      "Iteration 272, loss = 0.33053092\n",
      "Iteration 273, loss = 0.32956856\n",
      "Iteration 274, loss = 0.33036904\n",
      "Iteration 275, loss = 0.32977702\n",
      "Iteration 276, loss = 0.32914741\n",
      "Iteration 277, loss = 0.32983172\n",
      "Iteration 278, loss = 0.33023101\n",
      "Iteration 279, loss = 0.32996587\n",
      "Iteration 280, loss = 0.32912140\n",
      "Iteration 281, loss = 0.33066591\n",
      "Iteration 282, loss = 0.32856402\n",
      "Iteration 283, loss = 0.32813539\n",
      "Iteration 284, loss = 0.32884631\n",
      "Iteration 285, loss = 0.32936805\n",
      "Iteration 286, loss = 0.32950524\n",
      "Iteration 287, loss = 0.32850211\n",
      "Iteration 288, loss = 0.32759193\n",
      "Iteration 289, loss = 0.32897491\n",
      "Iteration 290, loss = 0.32754709\n",
      "Iteration 291, loss = 0.32920365\n",
      "Iteration 292, loss = 0.32838307\n",
      "Iteration 293, loss = 0.32819448\n",
      "Iteration 294, loss = 0.32783577\n",
      "Iteration 295, loss = 0.32817892\n",
      "Iteration 296, loss = 0.32900805\n",
      "Iteration 297, loss = 0.32851774\n",
      "Iteration 298, loss = 0.32730858\n",
      "Iteration 299, loss = 0.32647529\n",
      "Iteration 300, loss = 0.32744903\n",
      "Iteration 301, loss = 0.32771916\n",
      "Iteration 302, loss = 0.32730481\n",
      "Iteration 303, loss = 0.32917205\n",
      "Iteration 304, loss = 0.32779843\n",
      "Iteration 305, loss = 0.32673252\n",
      "Iteration 306, loss = 0.32673446\n",
      "Iteration 307, loss = 0.32635139\n",
      "Iteration 308, loss = 0.32707879\n",
      "Iteration 309, loss = 0.32656672\n",
      "Iteration 310, loss = 0.32712146\n",
      "Iteration 311, loss = 0.32637238\n",
      "Iteration 312, loss = 0.32590104\n",
      "Iteration 313, loss = 0.32661869\n",
      "Iteration 314, loss = 0.32584246\n",
      "Iteration 315, loss = 0.32709949\n",
      "Iteration 316, loss = 0.32528799\n",
      "Iteration 317, loss = 0.32687498\n",
      "Iteration 318, loss = 0.32604757\n",
      "Iteration 319, loss = 0.32537142\n",
      "Iteration 320, loss = 0.32579181\n",
      "Iteration 321, loss = 0.32612864\n",
      "Iteration 322, loss = 0.32549016\n",
      "Iteration 323, loss = 0.32555291\n",
      "Iteration 324, loss = 0.32605652\n",
      "Iteration 325, loss = 0.32592133\n",
      "Iteration 326, loss = 0.32471451\n",
      "Iteration 327, loss = 0.32539039\n",
      "Iteration 328, loss = 0.32549640\n",
      "Iteration 329, loss = 0.32635859\n",
      "Iteration 330, loss = 0.32454194\n",
      "Iteration 331, loss = 0.32540484\n",
      "Iteration 332, loss = 0.32421970\n",
      "Iteration 333, loss = 0.32496995\n",
      "Iteration 334, loss = 0.32450886\n",
      "Iteration 335, loss = 0.32452127\n",
      "Iteration 336, loss = 0.32488692\n",
      "Iteration 337, loss = 0.32388974\n",
      "Iteration 338, loss = 0.32493215\n",
      "Iteration 339, loss = 0.32435684\n",
      "Iteration 340, loss = 0.32507646\n",
      "Iteration 341, loss = 0.32378364\n",
      "Iteration 342, loss = 0.32367422\n",
      "Iteration 343, loss = 0.32365615\n",
      "Iteration 344, loss = 0.32430800\n",
      "Iteration 345, loss = 0.32483240\n",
      "Iteration 346, loss = 0.32333097\n",
      "Iteration 347, loss = 0.32393272\n",
      "Iteration 348, loss = 0.32303966\n",
      "Iteration 349, loss = 0.32344746\n",
      "Iteration 350, loss = 0.32269588\n",
      "Iteration 351, loss = 0.32367472\n",
      "Iteration 352, loss = 0.32307133\n",
      "Iteration 353, loss = 0.32318768\n",
      "Iteration 354, loss = 0.32292458\n",
      "Iteration 355, loss = 0.32369182\n",
      "Iteration 356, loss = 0.32278722\n",
      "Iteration 357, loss = 0.32271850\n",
      "Iteration 358, loss = 0.32281618\n",
      "Iteration 359, loss = 0.32261553\n",
      "Iteration 360, loss = 0.32276781\n",
      "Iteration 361, loss = 0.32385058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.51174679\n",
      "Iteration 2, loss = 0.46263560\n",
      "Iteration 3, loss = 0.45425339\n",
      "Iteration 4, loss = 0.44931178\n",
      "Iteration 5, loss = 0.44489601\n",
      "Iteration 6, loss = 0.44161584\n",
      "Iteration 7, loss = 0.43856211\n",
      "Iteration 8, loss = 0.43522341\n",
      "Iteration 9, loss = 0.43239915\n",
      "Iteration 10, loss = 0.43008092\n",
      "Iteration 11, loss = 0.42759710\n",
      "Iteration 12, loss = 0.42534092\n",
      "Iteration 13, loss = 0.42321887\n",
      "Iteration 14, loss = 0.42126737\n",
      "Iteration 15, loss = 0.41904546\n",
      "Iteration 16, loss = 0.41705692\n",
      "Iteration 17, loss = 0.41526488\n",
      "Iteration 18, loss = 0.41362336\n",
      "Iteration 19, loss = 0.41203441\n",
      "Iteration 20, loss = 0.41033988\n",
      "Iteration 21, loss = 0.40866556\n",
      "Iteration 22, loss = 0.40777376\n",
      "Iteration 23, loss = 0.40647493\n",
      "Iteration 24, loss = 0.40537236\n",
      "Iteration 25, loss = 0.40425386\n",
      "Iteration 26, loss = 0.40277586\n",
      "Iteration 27, loss = 0.40187396\n",
      "Iteration 28, loss = 0.40055366\n",
      "Iteration 29, loss = 0.39947172\n",
      "Iteration 30, loss = 0.39873547\n",
      "Iteration 31, loss = 0.39771608\n",
      "Iteration 32, loss = 0.39723752\n",
      "Iteration 33, loss = 0.39523576\n",
      "Iteration 34, loss = 0.39540230\n",
      "Iteration 35, loss = 0.39460798\n",
      "Iteration 36, loss = 0.39398075\n",
      "Iteration 37, loss = 0.39187386\n",
      "Iteration 38, loss = 0.39130571\n",
      "Iteration 39, loss = 0.39057598\n",
      "Iteration 40, loss = 0.39025483\n",
      "Iteration 41, loss = 0.38827271\n",
      "Iteration 42, loss = 0.38800479\n",
      "Iteration 43, loss = 0.38802222\n",
      "Iteration 44, loss = 0.38594051\n",
      "Iteration 45, loss = 0.38598725\n",
      "Iteration 46, loss = 0.38508258\n",
      "Iteration 47, loss = 0.38475141\n",
      "Iteration 48, loss = 0.38444522\n",
      "Iteration 49, loss = 0.38300883\n",
      "Iteration 50, loss = 0.38251991\n",
      "Iteration 51, loss = 0.38161281\n",
      "Iteration 52, loss = 0.38036518\n",
      "Iteration 53, loss = 0.38028486\n",
      "Iteration 54, loss = 0.38019614\n",
      "Iteration 55, loss = 0.37901460\n",
      "Iteration 56, loss = 0.37844771\n",
      "Iteration 57, loss = 0.37795184\n",
      "Iteration 58, loss = 0.37785340\n",
      "Iteration 59, loss = 0.37688230\n",
      "Iteration 60, loss = 0.37651261\n",
      "Iteration 61, loss = 0.37559664\n",
      "Iteration 62, loss = 0.37406058\n",
      "Iteration 63, loss = 0.37538732\n",
      "Iteration 64, loss = 0.37388238\n",
      "Iteration 65, loss = 0.37347895\n",
      "Iteration 66, loss = 0.37256097\n",
      "Iteration 67, loss = 0.37256051\n",
      "Iteration 68, loss = 0.37147901\n",
      "Iteration 69, loss = 0.37176996\n",
      "Iteration 70, loss = 0.37068945\n",
      "Iteration 71, loss = 0.37060240\n",
      "Iteration 72, loss = 0.36986984\n",
      "Iteration 73, loss = 0.36952954\n",
      "Iteration 74, loss = 0.36847171\n",
      "Iteration 75, loss = 0.36839831\n",
      "Iteration 76, loss = 0.36777022\n",
      "Iteration 77, loss = 0.36769520\n",
      "Iteration 78, loss = 0.36700252\n",
      "Iteration 79, loss = 0.36690145\n",
      "Iteration 80, loss = 0.36592234\n",
      "Iteration 81, loss = 0.36576328\n",
      "Iteration 82, loss = 0.36552844\n",
      "Iteration 83, loss = 0.36432973\n",
      "Iteration 84, loss = 0.36453672\n",
      "Iteration 85, loss = 0.36400172\n",
      "Iteration 86, loss = 0.36400131\n",
      "Iteration 87, loss = 0.36301513\n",
      "Iteration 88, loss = 0.36266580\n",
      "Iteration 89, loss = 0.36262002\n",
      "Iteration 90, loss = 0.36239057\n",
      "Iteration 91, loss = 0.36139426\n",
      "Iteration 92, loss = 0.36087895\n",
      "Iteration 93, loss = 0.36110345\n",
      "Iteration 94, loss = 0.36033382\n",
      "Iteration 95, loss = 0.36052013\n",
      "Iteration 96, loss = 0.35971086\n",
      "Iteration 97, loss = 0.35933486\n",
      "Iteration 98, loss = 0.35858991\n",
      "Iteration 99, loss = 0.35855705\n",
      "Iteration 100, loss = 0.35949766\n",
      "Iteration 101, loss = 0.35804982\n",
      "Iteration 102, loss = 0.35816771\n",
      "Iteration 103, loss = 0.35798159\n",
      "Iteration 104, loss = 0.35671394\n",
      "Iteration 105, loss = 0.35635827\n",
      "Iteration 106, loss = 0.35663828\n",
      "Iteration 107, loss = 0.35595767\n",
      "Iteration 108, loss = 0.35548745\n",
      "Iteration 109, loss = 0.35500985\n",
      "Iteration 110, loss = 0.35578531\n",
      "Iteration 111, loss = 0.35484999\n",
      "Iteration 112, loss = 0.35462332\n",
      "Iteration 113, loss = 0.35420356\n",
      "Iteration 114, loss = 0.35666609\n",
      "Iteration 115, loss = 0.35355482\n",
      "Iteration 116, loss = 0.35421400\n",
      "Iteration 117, loss = 0.35366131\n",
      "Iteration 118, loss = 0.35414698\n",
      "Iteration 119, loss = 0.35258633\n",
      "Iteration 120, loss = 0.35153312\n",
      "Iteration 121, loss = 0.35191818\n",
      "Iteration 122, loss = 0.35202030\n",
      "Iteration 123, loss = 0.35106188\n",
      "Iteration 124, loss = 0.35207058\n",
      "Iteration 125, loss = 0.35148735\n",
      "Iteration 126, loss = 0.35083350\n",
      "Iteration 127, loss = 0.34984010\n",
      "Iteration 128, loss = 0.34992667\n",
      "Iteration 129, loss = 0.35087331\n",
      "Iteration 130, loss = 0.34902744\n",
      "Iteration 131, loss = 0.34908899\n",
      "Iteration 132, loss = 0.34869005\n",
      "Iteration 133, loss = 0.34897003\n",
      "Iteration 134, loss = 0.34804263\n",
      "Iteration 135, loss = 0.34789883\n",
      "Iteration 136, loss = 0.34860108\n",
      "Iteration 137, loss = 0.34822957\n",
      "Iteration 138, loss = 0.34778878\n",
      "Iteration 139, loss = 0.34795027\n",
      "Iteration 140, loss = 0.34752720\n",
      "Iteration 141, loss = 0.34721809\n",
      "Iteration 142, loss = 0.34669570\n",
      "Iteration 143, loss = 0.34655938\n",
      "Iteration 144, loss = 0.34567257\n",
      "Iteration 145, loss = 0.34606069\n",
      "Iteration 146, loss = 0.34660078\n",
      "Iteration 147, loss = 0.34566186\n",
      "Iteration 148, loss = 0.34526974\n",
      "Iteration 149, loss = 0.34442921\n",
      "Iteration 150, loss = 0.34472586\n",
      "Iteration 151, loss = 0.34496042\n",
      "Iteration 152, loss = 0.34415576\n",
      "Iteration 153, loss = 0.34494033\n",
      "Iteration 154, loss = 0.34354055\n",
      "Iteration 155, loss = 0.34471782\n",
      "Iteration 156, loss = 0.34365289\n",
      "Iteration 157, loss = 0.34443397\n",
      "Iteration 158, loss = 0.34328366\n",
      "Iteration 159, loss = 0.34396851\n",
      "Iteration 160, loss = 0.34227413\n",
      "Iteration 161, loss = 0.34203393\n",
      "Iteration 162, loss = 0.34325826\n",
      "Iteration 163, loss = 0.34248293\n",
      "Iteration 164, loss = 0.34183119\n",
      "Iteration 165, loss = 0.34097049\n",
      "Iteration 166, loss = 0.34182713\n",
      "Iteration 167, loss = 0.34066878\n",
      "Iteration 168, loss = 0.34150034\n",
      "Iteration 169, loss = 0.34082173\n",
      "Iteration 170, loss = 0.34037269\n",
      "Iteration 171, loss = 0.33950123\n",
      "Iteration 172, loss = 0.33969163\n",
      "Iteration 173, loss = 0.34031339\n",
      "Iteration 174, loss = 0.33941353\n",
      "Iteration 175, loss = 0.33962758\n",
      "Iteration 176, loss = 0.33916440\n",
      "Iteration 177, loss = 0.33941435\n",
      "Iteration 178, loss = 0.33876316\n",
      "Iteration 179, loss = 0.33912932\n",
      "Iteration 180, loss = 0.33861588\n",
      "Iteration 181, loss = 0.33812050\n",
      "Iteration 182, loss = 0.33875166\n",
      "Iteration 183, loss = 0.33878133\n",
      "Iteration 184, loss = 0.33785627\n",
      "Iteration 185, loss = 0.33709578\n",
      "Iteration 186, loss = 0.33840521\n",
      "Iteration 187, loss = 0.33889614\n",
      "Iteration 188, loss = 0.33709746\n",
      "Iteration 189, loss = 0.33765434\n",
      "Iteration 190, loss = 0.33685088\n",
      "Iteration 191, loss = 0.33649738\n",
      "Iteration 192, loss = 0.33645220\n",
      "Iteration 193, loss = 0.33635725\n",
      "Iteration 194, loss = 0.33663626\n",
      "Iteration 195, loss = 0.33647786\n",
      "Iteration 196, loss = 0.33567153\n",
      "Iteration 197, loss = 0.33470613\n",
      "Iteration 198, loss = 0.33567426\n",
      "Iteration 199, loss = 0.33585716\n",
      "Iteration 200, loss = 0.33490944\n",
      "Iteration 201, loss = 0.33421228\n",
      "Iteration 202, loss = 0.33572666\n",
      "Iteration 203, loss = 0.33474870\n",
      "Iteration 204, loss = 0.33540682\n",
      "Iteration 205, loss = 0.33466337\n",
      "Iteration 206, loss = 0.33365697\n",
      "Iteration 207, loss = 0.33378251\n",
      "Iteration 208, loss = 0.33374159\n",
      "Iteration 209, loss = 0.33364652\n",
      "Iteration 210, loss = 0.33336902\n",
      "Iteration 211, loss = 0.33295650\n",
      "Iteration 212, loss = 0.33350363\n",
      "Iteration 213, loss = 0.33407865\n",
      "Iteration 214, loss = 0.33208358\n",
      "Iteration 215, loss = 0.33236664\n",
      "Iteration 216, loss = 0.33231235\n",
      "Iteration 217, loss = 0.33324062\n",
      "Iteration 218, loss = 0.33342635\n",
      "Iteration 219, loss = 0.33239692\n",
      "Iteration 220, loss = 0.33253855\n",
      "Iteration 221, loss = 0.33205856\n",
      "Iteration 222, loss = 0.33174423\n",
      "Iteration 223, loss = 0.33181407\n",
      "Iteration 224, loss = 0.33095035\n",
      "Iteration 225, loss = 0.33192146\n",
      "Iteration 226, loss = 0.33181867\n",
      "Iteration 227, loss = 0.33125306\n",
      "Iteration 228, loss = 0.33074491\n",
      "Iteration 229, loss = 0.33207710\n",
      "Iteration 230, loss = 0.33058302\n",
      "Iteration 231, loss = 0.33086308\n",
      "Iteration 232, loss = 0.33062933\n",
      "Iteration 233, loss = 0.33042324\n",
      "Iteration 234, loss = 0.33050693\n",
      "Iteration 235, loss = 0.33023653\n",
      "Iteration 236, loss = 0.33007354\n",
      "Iteration 237, loss = 0.32941415\n",
      "Iteration 238, loss = 0.32916061\n",
      "Iteration 239, loss = 0.33012118\n",
      "Iteration 240, loss = 0.32927891\n",
      "Iteration 241, loss = 0.32876276\n",
      "Iteration 242, loss = 0.32901352\n",
      "Iteration 243, loss = 0.32948484\n",
      "Iteration 244, loss = 0.32862896\n",
      "Iteration 245, loss = 0.33045241\n",
      "Iteration 246, loss = 0.33013182\n",
      "Iteration 247, loss = 0.32823891\n",
      "Iteration 248, loss = 0.32773674\n",
      "Iteration 249, loss = 0.32777183\n",
      "Iteration 250, loss = 0.32779741\n",
      "Iteration 251, loss = 0.32898747\n",
      "Iteration 252, loss = 0.32879853\n",
      "Iteration 253, loss = 0.32776991\n",
      "Iteration 254, loss = 0.32713551\n",
      "Iteration 255, loss = 0.32797561\n",
      "Iteration 256, loss = 0.32659553\n",
      "Iteration 257, loss = 0.32691110\n",
      "Iteration 258, loss = 0.32627051\n",
      "Iteration 259, loss = 0.32687127\n",
      "Iteration 260, loss = 0.32670220\n",
      "Iteration 261, loss = 0.32602979\n",
      "Iteration 262, loss = 0.32630574\n",
      "Iteration 263, loss = 0.32682179\n",
      "Iteration 264, loss = 0.32645611\n",
      "Iteration 265, loss = 0.32678174\n",
      "Iteration 266, loss = 0.32625957\n",
      "Iteration 267, loss = 0.32489772\n",
      "Iteration 268, loss = 0.32619538\n",
      "Iteration 269, loss = 0.32625658\n",
      "Iteration 270, loss = 0.32636010\n",
      "Iteration 271, loss = 0.32565553\n",
      "Iteration 272, loss = 0.32516293\n",
      "Iteration 273, loss = 0.32569480\n",
      "Iteration 274, loss = 0.32557369\n",
      "Iteration 275, loss = 0.32552626\n",
      "Iteration 276, loss = 0.32538390\n",
      "Iteration 277, loss = 0.32495223\n",
      "Iteration 278, loss = 0.32416025\n",
      "Iteration 279, loss = 0.32429458\n",
      "Iteration 280, loss = 0.32484023\n",
      "Iteration 281, loss = 0.32486930\n",
      "Iteration 282, loss = 0.32526704\n",
      "Iteration 283, loss = 0.32406925\n",
      "Iteration 284, loss = 0.32422438\n",
      "Iteration 285, loss = 0.32379220\n",
      "Iteration 286, loss = 0.32399829\n",
      "Iteration 287, loss = 0.32345880\n",
      "Iteration 288, loss = 0.32496608\n",
      "Iteration 289, loss = 0.32461454\n",
      "Iteration 290, loss = 0.32290878\n",
      "Iteration 291, loss = 0.32398553\n",
      "Iteration 292, loss = 0.32374021\n",
      "Iteration 293, loss = 0.32343681\n",
      "Iteration 294, loss = 0.32412635\n",
      "Iteration 295, loss = 0.32255260\n",
      "Iteration 296, loss = 0.32382703\n",
      "Iteration 297, loss = 0.32249143\n",
      "Iteration 298, loss = 0.32302221\n",
      "Iteration 299, loss = 0.32210196\n",
      "Iteration 300, loss = 0.32239507\n",
      "Iteration 301, loss = 0.32201462\n",
      "Iteration 302, loss = 0.32301379\n",
      "Iteration 303, loss = 0.32358035\n",
      "Iteration 304, loss = 0.32244260\n",
      "Iteration 305, loss = 0.32187231\n",
      "Iteration 306, loss = 0.32316195\n",
      "Iteration 307, loss = 0.32214280\n",
      "Iteration 308, loss = 0.32155882\n",
      "Iteration 309, loss = 0.32218045\n",
      "Iteration 310, loss = 0.32166955\n",
      "Iteration 311, loss = 0.32229883\n",
      "Iteration 312, loss = 0.32247020\n",
      "Iteration 313, loss = 0.32085409\n",
      "Iteration 314, loss = 0.32233841\n",
      "Iteration 315, loss = 0.32085784\n",
      "Iteration 316, loss = 0.32147207\n",
      "Iteration 317, loss = 0.32036539\n",
      "Iteration 318, loss = 0.32150228\n",
      "Iteration 319, loss = 0.32163365\n",
      "Iteration 320, loss = 0.32142394\n",
      "Iteration 321, loss = 0.32066177\n",
      "Iteration 322, loss = 0.32020717\n",
      "Iteration 323, loss = 0.32151259\n",
      "Iteration 324, loss = 0.32146103\n",
      "Iteration 325, loss = 0.32094865\n",
      "Iteration 326, loss = 0.32037727\n",
      "Iteration 327, loss = 0.32004588\n",
      "Iteration 328, loss = 0.32034225\n",
      "Iteration 329, loss = 0.32061357\n",
      "Iteration 330, loss = 0.31957353\n",
      "Iteration 331, loss = 0.31982661\n",
      "Iteration 332, loss = 0.31980392\n",
      "Iteration 333, loss = 0.32042365\n",
      "Iteration 334, loss = 0.32103052\n",
      "Iteration 335, loss = 0.32021893\n",
      "Iteration 336, loss = 0.31996926\n",
      "Iteration 337, loss = 0.32048698\n",
      "Iteration 338, loss = 0.31947346\n",
      "Iteration 339, loss = 0.31885303\n",
      "Iteration 340, loss = 0.31932676\n",
      "Iteration 341, loss = 0.31920265\n",
      "Iteration 342, loss = 0.32023888\n",
      "Iteration 343, loss = 0.31846623\n",
      "Iteration 344, loss = 0.31840352\n",
      "Iteration 345, loss = 0.32025386\n",
      "Iteration 346, loss = 0.31897896\n",
      "Iteration 347, loss = 0.31869777\n",
      "Iteration 348, loss = 0.31841065\n",
      "Iteration 349, loss = 0.31902834\n",
      "Iteration 350, loss = 0.31884427\n",
      "Iteration 351, loss = 0.31858038\n",
      "Iteration 352, loss = 0.31959533\n",
      "Iteration 353, loss = 0.31981131\n",
      "Iteration 354, loss = 0.31888050\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.48751875\n",
      "Iteration 2, loss = 0.44500666\n",
      "Iteration 3, loss = 0.43650221\n",
      "Iteration 4, loss = 0.43170359\n",
      "Iteration 5, loss = 0.42759677\n",
      "Iteration 6, loss = 0.42452981\n",
      "Iteration 7, loss = 0.42137678\n",
      "Iteration 8, loss = 0.41848405\n",
      "Iteration 9, loss = 0.41548446\n",
      "Iteration 10, loss = 0.41308375\n",
      "Iteration 11, loss = 0.41074812\n",
      "Iteration 12, loss = 0.40857772\n",
      "Iteration 13, loss = 0.40653622\n",
      "Iteration 14, loss = 0.40413289\n",
      "Iteration 15, loss = 0.40187276\n",
      "Iteration 16, loss = 0.40007610\n",
      "Iteration 17, loss = 0.39835333\n",
      "Iteration 18, loss = 0.39657614\n",
      "Iteration 19, loss = 0.39511916\n",
      "Iteration 20, loss = 0.39386730\n",
      "Iteration 21, loss = 0.39232042\n",
      "Iteration 22, loss = 0.39125522\n",
      "Iteration 23, loss = 0.38945121\n",
      "Iteration 24, loss = 0.38854538\n",
      "Iteration 25, loss = 0.38704663\n",
      "Iteration 26, loss = 0.38674056\n",
      "Iteration 27, loss = 0.38544392\n",
      "Iteration 28, loss = 0.38400799\n",
      "Iteration 29, loss = 0.38301490\n",
      "Iteration 30, loss = 0.38203564\n",
      "Iteration 31, loss = 0.38120833\n",
      "Iteration 32, loss = 0.38058150\n",
      "Iteration 33, loss = 0.37998165\n",
      "Iteration 34, loss = 0.37839395\n",
      "Iteration 35, loss = 0.37745792\n",
      "Iteration 36, loss = 0.37655047\n",
      "Iteration 37, loss = 0.37613155\n",
      "Iteration 38, loss = 0.37493370\n",
      "Iteration 39, loss = 0.37425298\n",
      "Iteration 40, loss = 0.37344825\n",
      "Iteration 41, loss = 0.37257292\n",
      "Iteration 42, loss = 0.37198243\n",
      "Iteration 43, loss = 0.37044675\n",
      "Iteration 44, loss = 0.36945149\n",
      "Iteration 45, loss = 0.36900801\n",
      "Iteration 46, loss = 0.36794326\n",
      "Iteration 47, loss = 0.36745958\n",
      "Iteration 48, loss = 0.36776496\n",
      "Iteration 49, loss = 0.36586942\n",
      "Iteration 50, loss = 0.36514491\n",
      "Iteration 51, loss = 0.36494576\n",
      "Iteration 52, loss = 0.36403628\n",
      "Iteration 53, loss = 0.36367785\n",
      "Iteration 54, loss = 0.36267980\n",
      "Iteration 55, loss = 0.36165202\n",
      "Iteration 56, loss = 0.36059902\n",
      "Iteration 57, loss = 0.36086622\n",
      "Iteration 58, loss = 0.35930774\n",
      "Iteration 59, loss = 0.35880680\n",
      "Iteration 60, loss = 0.35819864\n",
      "Iteration 61, loss = 0.35847430\n",
      "Iteration 62, loss = 0.35818884\n",
      "Iteration 63, loss = 0.35608046\n",
      "Iteration 64, loss = 0.35576714\n",
      "Iteration 65, loss = 0.35526418\n",
      "Iteration 66, loss = 0.35524692\n",
      "Iteration 67, loss = 0.35506654\n",
      "Iteration 68, loss = 0.35377879\n",
      "Iteration 69, loss = 0.35320314\n",
      "Iteration 70, loss = 0.35294112\n",
      "Iteration 71, loss = 0.35242513\n",
      "Iteration 72, loss = 0.35138244\n",
      "Iteration 73, loss = 0.35142491\n",
      "Iteration 74, loss = 0.35188109\n",
      "Iteration 75, loss = 0.35073002\n",
      "Iteration 76, loss = 0.34976008\n",
      "Iteration 77, loss = 0.34961832\n",
      "Iteration 78, loss = 0.34844487\n",
      "Iteration 79, loss = 0.34786595\n",
      "Iteration 80, loss = 0.34872307\n",
      "Iteration 81, loss = 0.34767188\n",
      "Iteration 82, loss = 0.34681022\n",
      "Iteration 83, loss = 0.34678601\n",
      "Iteration 84, loss = 0.34570226\n",
      "Iteration 85, loss = 0.34520978\n",
      "Iteration 86, loss = 0.34552643\n",
      "Iteration 87, loss = 0.34394798\n",
      "Iteration 88, loss = 0.34520750\n",
      "Iteration 89, loss = 0.34411467\n",
      "Iteration 90, loss = 0.34329472\n",
      "Iteration 91, loss = 0.34283107\n",
      "Iteration 92, loss = 0.34283477\n",
      "Iteration 93, loss = 0.34349524\n",
      "Iteration 94, loss = 0.34215616\n",
      "Iteration 95, loss = 0.34113167\n",
      "Iteration 96, loss = 0.34136072\n",
      "Iteration 97, loss = 0.34121869\n",
      "Iteration 98, loss = 0.34115784\n",
      "Iteration 99, loss = 0.34107284\n",
      "Iteration 100, loss = 0.34014615\n",
      "Iteration 101, loss = 0.33904897\n",
      "Iteration 102, loss = 0.33959916\n",
      "Iteration 103, loss = 0.33899252\n",
      "Iteration 104, loss = 0.33894406\n",
      "Iteration 105, loss = 0.33820105\n",
      "Iteration 106, loss = 0.33882813\n",
      "Iteration 107, loss = 0.33791938\n",
      "Iteration 108, loss = 0.33804192\n",
      "Iteration 109, loss = 0.33763253\n",
      "Iteration 110, loss = 0.33726975\n",
      "Iteration 111, loss = 0.33652805\n",
      "Iteration 112, loss = 0.33666779\n",
      "Iteration 113, loss = 0.33575533\n",
      "Iteration 114, loss = 0.33652124\n",
      "Iteration 115, loss = 0.33598638\n",
      "Iteration 116, loss = 0.33469960\n",
      "Iteration 117, loss = 0.33512633\n",
      "Iteration 118, loss = 0.33525900\n",
      "Iteration 119, loss = 0.33462864\n",
      "Iteration 120, loss = 0.33407925\n",
      "Iteration 121, loss = 0.33363037\n",
      "Iteration 122, loss = 0.33400240\n",
      "Iteration 123, loss = 0.33407480\n",
      "Iteration 124, loss = 0.33342486\n",
      "Iteration 125, loss = 0.33333475\n",
      "Iteration 126, loss = 0.33299982\n",
      "Iteration 127, loss = 0.33216231\n",
      "Iteration 128, loss = 0.33223771\n",
      "Iteration 129, loss = 0.33241497\n",
      "Iteration 130, loss = 0.33165603\n",
      "Iteration 131, loss = 0.33179469\n",
      "Iteration 132, loss = 0.33105483\n",
      "Iteration 133, loss = 0.33105402\n",
      "Iteration 134, loss = 0.33169374\n",
      "Iteration 135, loss = 0.32998082\n",
      "Iteration 136, loss = 0.33021499\n",
      "Iteration 137, loss = 0.33076444\n",
      "Iteration 138, loss = 0.33029624\n",
      "Iteration 139, loss = 0.33048752\n",
      "Iteration 140, loss = 0.32883492\n",
      "Iteration 141, loss = 0.32925210\n",
      "Iteration 142, loss = 0.32848766\n",
      "Iteration 143, loss = 0.32878648\n",
      "Iteration 144, loss = 0.32828881\n",
      "Iteration 145, loss = 0.32823479\n",
      "Iteration 146, loss = 0.32919288\n",
      "Iteration 147, loss = 0.32749589\n",
      "Iteration 148, loss = 0.32777279\n",
      "Iteration 149, loss = 0.32769958\n",
      "Iteration 150, loss = 0.32662175\n",
      "Iteration 151, loss = 0.32756159\n",
      "Iteration 152, loss = 0.32660442\n",
      "Iteration 153, loss = 0.32642548\n",
      "Iteration 154, loss = 0.32694469\n",
      "Iteration 155, loss = 0.32614621\n",
      "Iteration 156, loss = 0.32696950\n",
      "Iteration 157, loss = 0.32456742\n",
      "Iteration 158, loss = 0.32549801\n",
      "Iteration 159, loss = 0.32632379\n",
      "Iteration 160, loss = 0.32543915\n",
      "Iteration 161, loss = 0.32429492\n",
      "Iteration 162, loss = 0.32430616\n",
      "Iteration 163, loss = 0.32477093\n",
      "Iteration 164, loss = 0.32453226\n",
      "Iteration 165, loss = 0.32409936\n",
      "Iteration 166, loss = 0.32387664\n",
      "Iteration 167, loss = 0.32374245\n",
      "Iteration 168, loss = 0.32391698\n",
      "Iteration 169, loss = 0.32363338\n",
      "Iteration 170, loss = 0.32452281\n",
      "Iteration 171, loss = 0.32280096\n",
      "Iteration 172, loss = 0.32372072\n",
      "Iteration 173, loss = 0.32277762\n",
      "Iteration 174, loss = 0.32286992\n",
      "Iteration 175, loss = 0.32223386\n",
      "Iteration 176, loss = 0.32181234\n",
      "Iteration 177, loss = 0.32222333\n",
      "Iteration 178, loss = 0.32243394\n",
      "Iteration 179, loss = 0.32283952\n",
      "Iteration 180, loss = 0.32169003\n",
      "Iteration 181, loss = 0.32118541\n",
      "Iteration 182, loss = 0.32184335\n",
      "Iteration 183, loss = 0.32115660\n",
      "Iteration 184, loss = 0.32198766\n",
      "Iteration 185, loss = 0.32083194\n",
      "Iteration 186, loss = 0.32159779\n",
      "Iteration 187, loss = 0.32048927\n",
      "Iteration 188, loss = 0.32046103\n",
      "Iteration 189, loss = 0.31971070\n",
      "Iteration 190, loss = 0.32014663\n",
      "Iteration 191, loss = 0.32007099\n",
      "Iteration 192, loss = 0.32010006\n",
      "Iteration 193, loss = 0.32053600\n",
      "Iteration 194, loss = 0.32008176\n",
      "Iteration 195, loss = 0.32018002\n",
      "Iteration 196, loss = 0.31944621\n",
      "Iteration 197, loss = 0.31827842\n",
      "Iteration 198, loss = 0.31932928\n",
      "Iteration 199, loss = 0.31927384\n",
      "Iteration 200, loss = 0.31904115\n",
      "Iteration 201, loss = 0.31859220\n",
      "Iteration 202, loss = 0.31831943\n",
      "Iteration 203, loss = 0.31876498\n",
      "Iteration 204, loss = 0.31795179\n",
      "Iteration 205, loss = 0.31794567\n",
      "Iteration 206, loss = 0.31858353\n",
      "Iteration 207, loss = 0.31682995\n",
      "Iteration 208, loss = 0.31872278\n",
      "Iteration 209, loss = 0.31759182\n",
      "Iteration 210, loss = 0.31852409\n",
      "Iteration 211, loss = 0.31689126\n",
      "Iteration 212, loss = 0.31721288\n",
      "Iteration 213, loss = 0.31696714\n",
      "Iteration 214, loss = 0.31648931\n",
      "Iteration 215, loss = 0.31577255\n",
      "Iteration 216, loss = 0.31686123\n",
      "Iteration 217, loss = 0.31790883\n",
      "Iteration 218, loss = 0.31650100\n",
      "Iteration 219, loss = 0.31622617\n",
      "Iteration 220, loss = 0.31692401\n",
      "Iteration 221, loss = 0.31638969\n",
      "Iteration 222, loss = 0.31582024\n",
      "Iteration 223, loss = 0.31559464\n",
      "Iteration 224, loss = 0.31615155\n",
      "Iteration 225, loss = 0.31648470\n",
      "Iteration 226, loss = 0.31603696\n",
      "Iteration 227, loss = 0.31484935\n",
      "Iteration 228, loss = 0.31559978\n",
      "Iteration 229, loss = 0.31404720\n",
      "Iteration 230, loss = 0.31478411\n",
      "Iteration 231, loss = 0.31624124\n",
      "Iteration 232, loss = 0.31547666\n",
      "Iteration 233, loss = 0.31348502\n",
      "Iteration 234, loss = 0.31381482\n",
      "Iteration 235, loss = 0.31480884\n",
      "Iteration 236, loss = 0.31475075\n",
      "Iteration 237, loss = 0.31461210\n",
      "Iteration 238, loss = 0.31396871\n",
      "Iteration 239, loss = 0.31443189\n",
      "Iteration 240, loss = 0.31298731\n",
      "Iteration 241, loss = 0.31391425\n",
      "Iteration 242, loss = 0.31256856\n",
      "Iteration 243, loss = 0.31274598\n",
      "Iteration 244, loss = 0.31278865\n",
      "Iteration 245, loss = 0.31316225\n",
      "Iteration 246, loss = 0.31411289\n",
      "Iteration 247, loss = 0.31191152\n",
      "Iteration 248, loss = 0.31385757\n",
      "Iteration 249, loss = 0.31191756\n",
      "Iteration 250, loss = 0.31356285\n",
      "Iteration 251, loss = 0.31161958\n",
      "Iteration 252, loss = 0.31195664\n",
      "Iteration 253, loss = 0.31143205\n",
      "Iteration 254, loss = 0.31199799\n",
      "Iteration 255, loss = 0.31148837\n",
      "Iteration 256, loss = 0.31075377\n",
      "Iteration 257, loss = 0.31149914\n",
      "Iteration 258, loss = 0.31213834\n",
      "Iteration 259, loss = 0.31114131\n",
      "Iteration 260, loss = 0.31052959\n",
      "Iteration 261, loss = 0.31111607\n",
      "Iteration 262, loss = 0.31034145\n",
      "Iteration 263, loss = 0.31056190\n",
      "Iteration 264, loss = 0.30979588\n",
      "Iteration 265, loss = 0.31052665\n",
      "Iteration 266, loss = 0.31058229\n",
      "Iteration 267, loss = 0.31070357\n",
      "Iteration 268, loss = 0.31063701\n",
      "Iteration 269, loss = 0.31118538\n",
      "Iteration 270, loss = 0.31022963\n",
      "Iteration 271, loss = 0.31028227\n",
      "Iteration 272, loss = 0.30926801\n",
      "Iteration 273, loss = 0.30909945\n",
      "Iteration 274, loss = 0.30939619\n",
      "Iteration 275, loss = 0.30933800\n",
      "Iteration 276, loss = 0.30808236\n",
      "Iteration 277, loss = 0.30860749\n",
      "Iteration 278, loss = 0.30956474\n",
      "Iteration 279, loss = 0.30988044\n",
      "Iteration 280, loss = 0.30941441\n",
      "Iteration 281, loss = 0.30796550\n",
      "Iteration 282, loss = 0.30831334\n",
      "Iteration 283, loss = 0.30924019\n",
      "Iteration 284, loss = 0.30890463\n",
      "Iteration 285, loss = 0.30898659\n",
      "Iteration 286, loss = 0.30830724\n",
      "Iteration 287, loss = 0.30840700\n",
      "Iteration 288, loss = 0.30835309\n",
      "Iteration 289, loss = 0.30998938\n",
      "Iteration 290, loss = 0.30666224\n",
      "Iteration 291, loss = 0.30815032\n",
      "Iteration 292, loss = 0.30711597\n",
      "Iteration 293, loss = 0.30769967\n",
      "Iteration 294, loss = 0.30824036\n",
      "Iteration 295, loss = 0.30697741\n",
      "Iteration 296, loss = 0.30736293\n",
      "Iteration 297, loss = 0.30657700\n",
      "Iteration 298, loss = 0.30649283\n",
      "Iteration 299, loss = 0.30819940\n",
      "Iteration 300, loss = 0.30732002\n",
      "Iteration 301, loss = 0.30701976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.45639818\n",
      "Iteration 2, loss = 0.40307287\n",
      "Iteration 3, loss = 0.39586659\n",
      "Iteration 4, loss = 0.39043487\n",
      "Iteration 5, loss = 0.38687479\n",
      "Iteration 6, loss = 0.38264723\n",
      "Iteration 7, loss = 0.37984821\n",
      "Iteration 8, loss = 0.37652138\n",
      "Iteration 9, loss = 0.37378977\n",
      "Iteration 10, loss = 0.37188701\n",
      "Iteration 11, loss = 0.36888696\n",
      "Iteration 12, loss = 0.36682244\n",
      "Iteration 13, loss = 0.36422174\n",
      "Iteration 14, loss = 0.36280515\n",
      "Iteration 15, loss = 0.36020714\n",
      "Iteration 16, loss = 0.35891642\n",
      "Iteration 17, loss = 0.35724664\n",
      "Iteration 18, loss = 0.35512987\n",
      "Iteration 19, loss = 0.35376894\n",
      "Iteration 20, loss = 0.35299172\n",
      "Iteration 21, loss = 0.35111211\n",
      "Iteration 22, loss = 0.35021995\n",
      "Iteration 23, loss = 0.34854042\n",
      "Iteration 24, loss = 0.34767575\n",
      "Iteration 25, loss = 0.34637082\n",
      "Iteration 26, loss = 0.34588417\n",
      "Iteration 27, loss = 0.34450466\n",
      "Iteration 28, loss = 0.34355951\n",
      "Iteration 29, loss = 0.34266155\n",
      "Iteration 30, loss = 0.34115428\n",
      "Iteration 31, loss = 0.34048619\n",
      "Iteration 32, loss = 0.33999912\n",
      "Iteration 33, loss = 0.33925907\n",
      "Iteration 34, loss = 0.33807496\n",
      "Iteration 35, loss = 0.33756360\n",
      "Iteration 36, loss = 0.33605321\n",
      "Iteration 37, loss = 0.33556878\n",
      "Iteration 38, loss = 0.33448825\n",
      "Iteration 39, loss = 0.33388892\n",
      "Iteration 40, loss = 0.33289635\n",
      "Iteration 41, loss = 0.33189638\n",
      "Iteration 42, loss = 0.33155788\n",
      "Iteration 43, loss = 0.33006864\n",
      "Iteration 44, loss = 0.33052618\n",
      "Iteration 45, loss = 0.32861844\n",
      "Iteration 46, loss = 0.32856856\n",
      "Iteration 47, loss = 0.32747641\n",
      "Iteration 48, loss = 0.32660603\n",
      "Iteration 49, loss = 0.32592420\n",
      "Iteration 50, loss = 0.32520595\n",
      "Iteration 51, loss = 0.32447303\n",
      "Iteration 52, loss = 0.32399554\n",
      "Iteration 53, loss = 0.32330893\n",
      "Iteration 54, loss = 0.32268874\n",
      "Iteration 55, loss = 0.32172167\n",
      "Iteration 56, loss = 0.32165667\n",
      "Iteration 57, loss = 0.32097354\n",
      "Iteration 58, loss = 0.32032118\n",
      "Iteration 59, loss = 0.31975634\n",
      "Iteration 60, loss = 0.31877808\n",
      "Iteration 61, loss = 0.31914067\n",
      "Iteration 62, loss = 0.31795231\n",
      "Iteration 63, loss = 0.31717560\n",
      "Iteration 64, loss = 0.31655189\n",
      "Iteration 65, loss = 0.31593009\n",
      "Iteration 66, loss = 0.31590308\n",
      "Iteration 67, loss = 0.31583182\n",
      "Iteration 68, loss = 0.31494421\n",
      "Iteration 69, loss = 0.31449064\n",
      "Iteration 70, loss = 0.31379444\n",
      "Iteration 71, loss = 0.31278878\n",
      "Iteration 72, loss = 0.31285799\n",
      "Iteration 73, loss = 0.31180528\n",
      "Iteration 74, loss = 0.31140591\n",
      "Iteration 75, loss = 0.31116484\n",
      "Iteration 76, loss = 0.31097647\n",
      "Iteration 77, loss = 0.31031518\n",
      "Iteration 78, loss = 0.30931991\n",
      "Iteration 79, loss = 0.30959048\n",
      "Iteration 80, loss = 0.30877260\n",
      "Iteration 81, loss = 0.30765667\n",
      "Iteration 82, loss = 0.30740658\n",
      "Iteration 83, loss = 0.30744183\n",
      "Iteration 84, loss = 0.30672399\n",
      "Iteration 85, loss = 0.30644762\n",
      "Iteration 86, loss = 0.30580503\n",
      "Iteration 87, loss = 0.30529324\n",
      "Iteration 88, loss = 0.30490362\n",
      "Iteration 89, loss = 0.30474434\n",
      "Iteration 90, loss = 0.30375957\n",
      "Iteration 91, loss = 0.30401016\n",
      "Iteration 92, loss = 0.30347938\n",
      "Iteration 93, loss = 0.30296793\n",
      "Iteration 94, loss = 0.30300104\n",
      "Iteration 95, loss = 0.30144746\n",
      "Iteration 96, loss = 0.30164925\n",
      "Iteration 97, loss = 0.30187109\n",
      "Iteration 98, loss = 0.30104025\n",
      "Iteration 99, loss = 0.30095823\n",
      "Iteration 100, loss = 0.30094052\n",
      "Iteration 101, loss = 0.29981203\n",
      "Iteration 102, loss = 0.29935459\n",
      "Iteration 103, loss = 0.30033436\n",
      "Iteration 104, loss = 0.29840224\n",
      "Iteration 105, loss = 0.29888308\n",
      "Iteration 106, loss = 0.29924459\n",
      "Iteration 107, loss = 0.29838747\n",
      "Iteration 108, loss = 0.29716918\n",
      "Iteration 109, loss = 0.29725753\n",
      "Iteration 110, loss = 0.29729797\n",
      "Iteration 111, loss = 0.29657039\n",
      "Iteration 112, loss = 0.29640385\n",
      "Iteration 113, loss = 0.29644857\n",
      "Iteration 114, loss = 0.29614502\n",
      "Iteration 115, loss = 0.29587945\n",
      "Iteration 116, loss = 0.29512851\n",
      "Iteration 117, loss = 0.29465531\n",
      "Iteration 118, loss = 0.29451149\n",
      "Iteration 119, loss = 0.29494290\n",
      "Iteration 120, loss = 0.29402951\n",
      "Iteration 121, loss = 0.29387235\n",
      "Iteration 122, loss = 0.29340897\n",
      "Iteration 123, loss = 0.29320364\n",
      "Iteration 124, loss = 0.29212002\n",
      "Iteration 125, loss = 0.29174822\n",
      "Iteration 126, loss = 0.29242043\n",
      "Iteration 127, loss = 0.29311686\n",
      "Iteration 128, loss = 0.29084657\n",
      "Iteration 129, loss = 0.29105528\n",
      "Iteration 130, loss = 0.29102711\n",
      "Iteration 131, loss = 0.29147652\n",
      "Iteration 132, loss = 0.29040318\n",
      "Iteration 133, loss = 0.29109853\n",
      "Iteration 134, loss = 0.28981378\n",
      "Iteration 135, loss = 0.29033207\n",
      "Iteration 136, loss = 0.28964069\n",
      "Iteration 137, loss = 0.28981752\n",
      "Iteration 138, loss = 0.28904468\n",
      "Iteration 139, loss = 0.28859256\n",
      "Iteration 140, loss = 0.28874362\n",
      "Iteration 141, loss = 0.28803565\n",
      "Iteration 142, loss = 0.28706079\n",
      "Iteration 143, loss = 0.28839462\n",
      "Iteration 144, loss = 0.28832257\n",
      "Iteration 145, loss = 0.28749304\n",
      "Iteration 146, loss = 0.28724510\n",
      "Iteration 147, loss = 0.28691229\n",
      "Iteration 148, loss = 0.28649684\n",
      "Iteration 149, loss = 0.28587257\n",
      "Iteration 150, loss = 0.28608573\n",
      "Iteration 151, loss = 0.28539002\n",
      "Iteration 152, loss = 0.28550231\n",
      "Iteration 153, loss = 0.28545567\n",
      "Iteration 154, loss = 0.28516835\n",
      "Iteration 155, loss = 0.28579120\n",
      "Iteration 156, loss = 0.28511847\n",
      "Iteration 157, loss = 0.28420808\n",
      "Iteration 158, loss = 0.28434439\n",
      "Iteration 159, loss = 0.28398116\n",
      "Iteration 160, loss = 0.28428649\n",
      "Iteration 161, loss = 0.28317384\n",
      "Iteration 162, loss = 0.28294664\n",
      "Iteration 163, loss = 0.28337699\n",
      "Iteration 164, loss = 0.28285233\n",
      "Iteration 165, loss = 0.28333899\n",
      "Iteration 166, loss = 0.28372573\n",
      "Iteration 167, loss = 0.28250270\n",
      "Iteration 168, loss = 0.28194939\n",
      "Iteration 169, loss = 0.28274656\n",
      "Iteration 170, loss = 0.28185700\n",
      "Iteration 171, loss = 0.28134529\n",
      "Iteration 172, loss = 0.28209424\n",
      "Iteration 173, loss = 0.28033995\n",
      "Iteration 174, loss = 0.28184154\n",
      "Iteration 175, loss = 0.28093765\n",
      "Iteration 176, loss = 0.28055038\n",
      "Iteration 177, loss = 0.28065974\n",
      "Iteration 178, loss = 0.28104603\n",
      "Iteration 179, loss = 0.28107045\n",
      "Iteration 180, loss = 0.27985996\n",
      "Iteration 181, loss = 0.27875160\n",
      "Iteration 182, loss = 0.28006990\n",
      "Iteration 183, loss = 0.27882797\n",
      "Iteration 184, loss = 0.27936385\n",
      "Iteration 185, loss = 0.28057650\n",
      "Iteration 186, loss = 0.27876573\n",
      "Iteration 187, loss = 0.27906091\n",
      "Iteration 188, loss = 0.27870097\n",
      "Iteration 189, loss = 0.27803498\n",
      "Iteration 190, loss = 0.27802400\n",
      "Iteration 191, loss = 0.27811640\n",
      "Iteration 192, loss = 0.27848069\n",
      "Iteration 193, loss = 0.27848764\n",
      "Iteration 194, loss = 0.27667919\n",
      "Iteration 195, loss = 0.27740009\n",
      "Iteration 196, loss = 0.27792199\n",
      "Iteration 197, loss = 0.27717249\n",
      "Iteration 198, loss = 0.27671673\n",
      "Iteration 199, loss = 0.27693299\n",
      "Iteration 200, loss = 0.27590110\n",
      "Iteration 201, loss = 0.27615379\n",
      "Iteration 202, loss = 0.27702546\n",
      "Iteration 203, loss = 0.27585473\n",
      "Iteration 204, loss = 0.27619935\n",
      "Iteration 205, loss = 0.27644999\n",
      "Iteration 206, loss = 0.27590116\n",
      "Iteration 207, loss = 0.27517264\n",
      "Iteration 208, loss = 0.27555719\n",
      "Iteration 209, loss = 0.27549199\n",
      "Iteration 210, loss = 0.27423227\n",
      "Iteration 211, loss = 0.27526906\n",
      "Iteration 212, loss = 0.27425446\n",
      "Iteration 213, loss = 0.27528374\n",
      "Iteration 214, loss = 0.27413102\n",
      "Iteration 215, loss = 0.27464012\n",
      "Iteration 216, loss = 0.27443602\n",
      "Iteration 217, loss = 0.27325490\n",
      "Iteration 218, loss = 0.27411492\n",
      "Iteration 219, loss = 0.27416524\n",
      "Iteration 220, loss = 0.27386593\n",
      "Iteration 221, loss = 0.27302332\n",
      "Iteration 222, loss = 0.27287177\n",
      "Iteration 223, loss = 0.27323658\n",
      "Iteration 224, loss = 0.27277815\n",
      "Iteration 225, loss = 0.27248392\n",
      "Iteration 226, loss = 0.27257867\n",
      "Iteration 227, loss = 0.27168526\n",
      "Iteration 228, loss = 0.27210477\n",
      "Iteration 229, loss = 0.27338168\n",
      "Iteration 230, loss = 0.27167326\n",
      "Iteration 231, loss = 0.27246051\n",
      "Iteration 232, loss = 0.27183227\n",
      "Iteration 233, loss = 0.27154827\n",
      "Iteration 234, loss = 0.27035403\n",
      "Iteration 235, loss = 0.27169855\n",
      "Iteration 236, loss = 0.27112824\n",
      "Iteration 237, loss = 0.27112800\n",
      "Iteration 238, loss = 0.27103337\n",
      "Iteration 239, loss = 0.27017139\n",
      "Iteration 240, loss = 0.27026817\n",
      "Iteration 241, loss = 0.27097918\n",
      "Iteration 242, loss = 0.27054886\n",
      "Iteration 243, loss = 0.26997591\n",
      "Iteration 244, loss = 0.27070304\n",
      "Iteration 245, loss = 0.27100236\n",
      "Iteration 246, loss = 0.26983129\n",
      "Iteration 247, loss = 0.26918119\n",
      "Iteration 248, loss = 0.26978782\n",
      "Iteration 249, loss = 0.26884680\n",
      "Iteration 250, loss = 0.26889263\n",
      "Iteration 251, loss = 0.26987977\n",
      "Iteration 252, loss = 0.26902972\n",
      "Iteration 253, loss = 0.26829434\n",
      "Iteration 254, loss = 0.26960157\n",
      "Iteration 255, loss = 0.26801939\n",
      "Iteration 256, loss = 0.26875088\n",
      "Iteration 257, loss = 0.26892063\n",
      "Iteration 258, loss = 0.26817316\n",
      "Iteration 259, loss = 0.26723643\n",
      "Iteration 260, loss = 0.26728791\n",
      "Iteration 261, loss = 0.26829353\n",
      "Iteration 262, loss = 0.26698913\n",
      "Iteration 263, loss = 0.26701896\n",
      "Iteration 264, loss = 0.26696957\n",
      "Iteration 265, loss = 0.26724309\n",
      "Iteration 266, loss = 0.26823940\n",
      "Iteration 267, loss = 0.26768543\n",
      "Iteration 268, loss = 0.26781685\n",
      "Iteration 269, loss = 0.26638299\n",
      "Iteration 270, loss = 0.26632472\n",
      "Iteration 271, loss = 0.26632574\n",
      "Iteration 272, loss = 0.26611566\n",
      "Iteration 273, loss = 0.26620978\n",
      "Iteration 274, loss = 0.26567738\n",
      "Iteration 275, loss = 0.26688609\n",
      "Iteration 276, loss = 0.26692884\n",
      "Iteration 277, loss = 0.26593124\n",
      "Iteration 278, loss = 0.26517429\n",
      "Iteration 279, loss = 0.26565519\n",
      "Iteration 280, loss = 0.26474744\n",
      "Iteration 281, loss = 0.26545673\n",
      "Iteration 282, loss = 0.26549072\n",
      "Iteration 283, loss = 0.26551905\n",
      "Iteration 284, loss = 0.26556281\n",
      "Iteration 285, loss = 0.26587998\n",
      "Iteration 286, loss = 0.26611051\n",
      "Iteration 287, loss = 0.26459547\n",
      "Iteration 288, loss = 0.26436434\n",
      "Iteration 289, loss = 0.26552163\n",
      "Iteration 290, loss = 0.26449887\n",
      "Iteration 291, loss = 0.26435239\n",
      "Iteration 292, loss = 0.26512809\n",
      "Iteration 293, loss = 0.26436638\n",
      "Iteration 294, loss = 0.26419756\n",
      "Iteration 295, loss = 0.26358090\n",
      "Iteration 296, loss = 0.26401541\n",
      "Iteration 297, loss = 0.26346675\n",
      "Iteration 298, loss = 0.26387099\n",
      "Iteration 299, loss = 0.26353843\n",
      "Iteration 300, loss = 0.26527569\n",
      "Iteration 301, loss = 0.26424350\n",
      "Iteration 302, loss = 0.26395720\n",
      "Iteration 303, loss = 0.26299168\n",
      "Iteration 304, loss = 0.26296784\n",
      "Iteration 305, loss = 0.26303488\n",
      "Iteration 306, loss = 0.26304215\n",
      "Iteration 307, loss = 0.26406584\n",
      "Iteration 308, loss = 0.26307709\n",
      "Iteration 309, loss = 0.26292335\n",
      "Iteration 310, loss = 0.26260097\n",
      "Iteration 311, loss = 0.26407707\n",
      "Iteration 312, loss = 0.26276598\n",
      "Iteration 313, loss = 0.26156317\n",
      "Iteration 314, loss = 0.26210145\n",
      "Iteration 315, loss = 0.26270710\n",
      "Iteration 316, loss = 0.26280315\n",
      "Iteration 317, loss = 0.26207599\n",
      "Iteration 318, loss = 0.26165305\n",
      "Iteration 319, loss = 0.26121198\n",
      "Iteration 320, loss = 0.26177382\n",
      "Iteration 321, loss = 0.26121981\n",
      "Iteration 322, loss = 0.26127018\n",
      "Iteration 323, loss = 0.26225305\n",
      "Iteration 324, loss = 0.26097727\n",
      "Iteration 325, loss = 0.26105455\n",
      "Iteration 326, loss = 0.26069236\n",
      "Iteration 327, loss = 0.26071124\n",
      "Iteration 328, loss = 0.26068593\n",
      "Iteration 329, loss = 0.26150690\n",
      "Iteration 330, loss = 0.26208945\n",
      "Iteration 331, loss = 0.26111063\n",
      "Iteration 332, loss = 0.25970879\n",
      "Iteration 333, loss = 0.26029823\n",
      "Iteration 334, loss = 0.26017151\n",
      "Iteration 335, loss = 0.26021825\n",
      "Iteration 336, loss = 0.25974895\n",
      "Iteration 337, loss = 0.26084510\n",
      "Iteration 338, loss = 0.26048178\n",
      "Iteration 339, loss = 0.25976826\n",
      "Iteration 340, loss = 0.26051559\n",
      "Iteration 341, loss = 0.25989456\n",
      "Iteration 342, loss = 0.26003010\n",
      "Iteration 343, loss = 0.26028283\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExponentiatedGradient(constraints=&lt;fairlearn.reductions._moments.utility_parity.EqualizedOdds object at 0x31979fdc0&gt;,\n",
       "                      estimator=MLPClassifierFC(max_iter=500, verbose=True),\n",
       "                      max_iter=10, nu=0.0006424673718166643)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExponentiatedGradient</label><div class=\"sk-toggleable__content\"><pre>ExponentiatedGradient(constraints=&lt;fairlearn.reductions._moments.utility_parity.EqualizedOdds object at 0x31979fdc0&gt;,\n",
       "                      estimator=MLPClassifierFC(max_iter=500, verbose=True),\n",
       "                      max_iter=10, nu=0.0006424673718166643)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifierFC</label><div class=\"sk-toggleable__content\"><pre>MLPClassifierFC(max_iter=500, verbose=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifierFC</label><div class=\"sk-toggleable__content\"><pre>MLPClassifierFC(max_iter=500, verbose=True)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExponentiatedGradient(constraints=<fairlearn.reductions._moments.utility_parity.EqualizedOdds object at 0x31979fdc0>,\n",
       "                      estimator=MLPClassifierFC(max_iter=500, verbose=True),\n",
       "                      max_iter=10, nu=0.0006424673718166643)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf3 = MLPClassifierFC(max_iter=500, verbose=True)\n",
    "clf3.coefs_ = experiment.clf.coefs_\n",
    "clf3.intercepts_ = experiment.clf.intercepts_\n",
    "mitigator_db_p01 = ExponentiatedGradient(clf3, EqualizedOdds(), max_iter=10)\n",
    "mitigator_db_p01.fit(experiment.X_train, experiment.y_tr, sensitive_features=subgroup_vals_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_utils.save_model(mitigator_db_p01, f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_mitigator_eqodds_db_p01_test_{subgroup_col_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator_db_p01 = model_utils.load_model(f'<PATH_TO_MODEL>/{experiment.ds.ds.filenameroot}_mitigator_eqodds_db_p01_test_{subgroup_col_name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExponentiatedGradient(constraints=&lt;fairlearn.reductions._moments.utility_parity.DemographicParity object at 0x32458b8b0&gt;,\n",
       "                      estimator=MLPClassifierFC(max_iter=500),\n",
       "                      nu=0.0008548056047151522)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExponentiatedGradient</label><div class=\"sk-toggleable__content\"><pre>ExponentiatedGradient(constraints=&lt;fairlearn.reductions._moments.utility_parity.DemographicParity object at 0x32458b8b0&gt;,\n",
       "                      estimator=MLPClassifierFC(max_iter=500),\n",
       "                      nu=0.0008548056047151522)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifierFC</label><div class=\"sk-toggleable__content\"><pre>MLPClassifierFC(max_iter=500)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifierFC</label><div class=\"sk-toggleable__content\"><pre>MLPClassifierFC(max_iter=500)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExponentiatedGradient(constraints=<fairlearn.reductions._moments.utility_parity.DemographicParity object at 0x32458b8b0>,\n",
       "                      estimator=MLPClassifierFC(max_iter=500),\n",
       "                      nu=0.0008548056047151522)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf4 = MLPClassifierFC(max_iter=500)\n",
    "clf4.coefs_ = experiment.clf.coefs_\n",
    "clf4.intercepts_ = experiment.clf.intercepts_\n",
    "mitigator_dp_db_p01 = ExponentiatedGradient(clf4, DemographicParity(difference_bound=0.01))\n",
    "mitigator_dp_db_p01.fit(experiment.X_test, experiment.y_te, sensitive_features=experiment.X_test[f'{subgroup_col_name}_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Case 1</th>\n",
       "      <td>4145 (65.121)</td>\n",
       "      <td>5573 (71.0687)</td>\n",
       "      <td>68.5056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 2</th>\n",
       "      <td>17798 (55.1002)</td>\n",
       "      <td>16661 (66.8666)</td>\n",
       "      <td>60.8624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 3</th>\n",
       "      <td>3057 (56.3745)</td>\n",
       "      <td>2766 (56.0854)</td>\n",
       "      <td>56.2350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case All Cases</th>\n",
       "      <td>25000 (56.8906)</td>\n",
       "      <td>25000 (66.5318)</td>\n",
       "      <td>61.7476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1                0  Overall\n",
       "Case 1            4145 (65.121)   5573 (71.0687)  68.5056\n",
       "Case 2          17798 (55.1002)  16661 (66.8666)  60.8624\n",
       "Case 3           3057 (56.3745)   2766 (56.0854)  56.2350\n",
       "Case All Cases  25000 (56.8906)  25000 (66.5318)  61.7476"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_CSMIA_case_by_case_results(mitigator_db_p01, experiment.X_test, experiment.y_te, experiment.ds, subgroup_col_name='SEX', sensitive_col_name='MAR_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06296000000000002"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr_pred = mitigator_db_p01.predict(experiment.X_train)\n",
    "subgroup_vals_tr = experiment.X_train[f'SEX_0'].to_numpy().ravel()\n",
    "equalized_odds_difference(experiment.y_tr.ravel(), y_tr_pred, sensitive_features=subgroup_vals_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017199999999999993"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te_pred = mitigator_db_p01.predict(experiment.X_test)\n",
    "subgroup_vals_te = experiment.X_test[f'SEX_0'].to_numpy().ravel()\n",
    "equalized_odds_difference(experiment.y_te.ravel(), y_te_pred, sensitive_features=subgroup_vals_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014400000000000024"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te_pred = mitigator_db_p01.predict(experiment.X_test)\n",
    "subgroup_vals_te = experiment.X_test[f'SEX_0'].to_numpy().ravel()\n",
    "demographic_parity_difference(experiment.y_te.ravel(), y_te_pred, sensitive_features=subgroup_vals_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_vals_te = experiment.X_test[f'{subgroup_col_name}_1'].to_numpy().ravel()\n",
    "sens_pred_LOMIA = LOMIA_attack(experiment, mitigator_db_p01, experiment.X_test, experiment.y_te, experiment.ds.ds.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices_LOMIA = (sens_pred_LOMIA == experiment.X_test[[f'{experiment.ds.ds.meta[\"sensitive_column\"]}_1']].to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.895999999999995\n"
     ]
    }
   ],
   "source": [
    "print(100 * (correct_indices_LOMIA[np.where(subgroup_vals_te==0)[0]].mean()-correct_indices_LOMIA[np.where(subgroup_vals_te==1)[0]].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import predict_proba_for_mitiagtor\n",
    "subgroup_vals_tr = experiment.X_train[f'{subgroup_col_name}_1'].to_numpy().ravel()\n",
    "male_indices = np.where(subgroup_vals_tr==0)[0]\n",
    "female_indices = np.where(subgroup_vals_tr==1)[0]\n",
    "y_tr_pred = np.argmax(mitigator_db_p01._pmf_predict(experiment.X_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.724"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * accuracy_score(experiment.y_tr.ravel()[male_indices], y_tr_pred[male_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * accuracy_score(experiment.y_tr.ravel()[female_indices], y_tr_pred[male_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigator_db_p01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & 100 & 500 & 1000 & 5000 \\\\\n",
      "\\midrule\n",
      "0.100000 & 47.96 & 54.41 & 60.15 & 62.54 \\\\\n",
      "0.200000 & 57.79 & 68.95 & 66.93 & 67.68 \\\\\n",
      "0.300000 & 64.16 & 71.50 & 70.58 & 71.57 \\\\\n",
      "0.400000 & 71.47 & 71.98 & 72.89 & 73.31 \\\\\n",
      "0.500000 & 68.84 & 73.43 & 71.82 & 73.46 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame.from_dict(imputation_perf_dict, orient='index').T[[100, 500, 1000, 5000]].round(2).to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Case 1</th>\n",
       "      <td>3750 (65.9039)</td>\n",
       "      <td>5272 (69.5954)</td>\n",
       "      <td>68.1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 2</th>\n",
       "      <td>18077 (52.4326)</td>\n",
       "      <td>16885 (67.0515)</td>\n",
       "      <td>59.3924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 3</th>\n",
       "      <td>3173 (57.1429)</td>\n",
       "      <td>2843 (58.2051)</td>\n",
       "      <td>57.6786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case All Cases</th>\n",
       "      <td>25000 (55.0063)</td>\n",
       "      <td>25000 (66.4534)</td>\n",
       "      <td>60.7494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1                0  Overall\n",
       "Case 1           3750 (65.9039)   5272 (69.5954)  68.1096\n",
       "Case 2          18077 (52.4326)  16885 (67.0515)  59.3924\n",
       "Case 3           3173 (57.1429)   2843 (58.2051)  57.6786\n",
       "Case All Cases  25000 (55.0063)  25000 (66.4534)  60.7494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_CSMIA_case_by_case_results(mitigator, experiment.X_test, experiment.y_te, experiment.ds, subgroup_col_name='SEX', sensitive_col_name='MAR_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Case 1</th>\n",
       "      <td>1998 (63.3056)</td>\n",
       "      <td>4075 (63.8072)</td>\n",
       "      <td>63.6483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 2</th>\n",
       "      <td>19357 (55.7876)</td>\n",
       "      <td>17947 (74.5631)</td>\n",
       "      <td>65.0548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case 3</th>\n",
       "      <td>3645 (58.75)</td>\n",
       "      <td>2978 (55.9896)</td>\n",
       "      <td>57.3698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case All Cases</th>\n",
       "      <td>25000 (56.9703)</td>\n",
       "      <td>25000 (69.6637)</td>\n",
       "      <td>63.5705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1                0  Overall\n",
       "Case 1           1998 (63.3056)   4075 (63.8072)  63.6483\n",
       "Case 2          19357 (55.7876)  17947 (74.5631)  65.0548\n",
       "Case 3             3645 (58.75)   2978 (55.9896)  57.3698\n",
       "Case All Cases  25000 (56.9703)  25000 (69.6637)  63.5705"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_CSMIA_case_by_case_results(experiment.clf, experiment.X_test, experiment.y_te, experiment.ds, subgroup_col_name='SEX', sensitive_col_name='MAR_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
